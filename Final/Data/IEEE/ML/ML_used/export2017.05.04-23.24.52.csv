"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7732139,7727382,7727568,7726258,7732171,7732474,7729381,7727400,7729128,7726677,7726217,7730862,7727482,7724641,7724478,7724430,7724968,7725951,7724541,7724539,7458837,7549079,7695181,7723581,7684060,7603367,7604542,7604697,7604700,7603783,7450665,7600185,7590787,7591534,7591269,7591231,7592095,7591219,7591949,7591179,7591784,7589589,7585216,7585225,7585217,7589779,7586381,7586601,7584357,7581563,7582790,7581554,7583582,7575388,7533506,7579939,7575351,7578715,7577625,7578458,7578291,7577870,7575171,7574632,7576347,7574869,7559748,7575871,7574661,7571856,7573457,7571681,7573509,7573556,7573432,7543522,7569535,7564840,7566711,7568650,7289481,7566245,7566749,7563643,7564050,7563746,7562703,7562048,7562700,7561525,7561505,7558486,7559184,7559488,7558451,7559078,7557590,7556597,7556119,7555972",2017/05/04 23:24:52
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A comparison of machine learning algorithms for regional wheat yield prediction using NDVI time series of SPOT-VGT","M. Stas; J. Van Orshoven; Q. Dong; S. Heremans; B. Zhang","Department of Earth and Environmental Sciences, KU Leuven, Leuven, Belgium","2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)","20160929","2016","","","1","5","This study compares two machine learning algorithms to predict regional winter wheat yields. The models, based on Boosted Regression Trees (BRT) and Support Vector Machines (SVM), are constructed of Normalized Difference Vegetation Indices (NDVI) derived from low resolution SPOT VEGETATION imagery. Three types of NDVI-related predictors were used: Single NDVI, Incremental NDVI and Targeted NDVI. BRT and SVM were first used to select features with high relevance for predicting the yield. Periods of high influence spanning from March to June were detected by both machine learning methods. After feature selection, BRT and SVM models were applied to the subset of selected features for yield forecasting. BRT seems to consistently outperform SVM.","","","10.1109/Agro-Geoinformatics.2016.7577625","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577625","Machine Learning;Remote sensing;Wheat yield forecasting","Biological system modeling;Learning systems;Market research;Regression tree analysis;Support vector machines;Vegetation;Vegetation mapping","crops;learning (artificial intelligence);regression analysis;support vector machines;trees (mathematics)","BRT model;SPOT-VGT NDVI time series;SVM model;boosted regression tree;incremental NDVI;low resolution SPOT VEGETATION imagery;machine learning algorithm;normalized difference vegetation indices;regional wheat yield prediction;regional winter wheat yield prediction;single NDVI;support vector machine;targeted NDVI;yield forecasting","","","","","","","18-20 July 2016","","IEEE","IEEE Conference Publications"
"Analysis of Spectrum Occupancy Using Machine Learning Algorithms","F. Azmat; Y. Chen; N. Stocks","School of Engineering, University of Warwick, Coventry, U.K.","IEEE Transactions on Vehicular Technology","20160915","2016","65","9","6853","6860","In this paper, we analyze the spectrum occupancy in cognitive radio networks (CRNs) using different machine learning techniques. Both supervised techniques [naive Bayesian classifier (NBC), decision trees (DT), support vector machine (SVM), linear regression (LR)] and unsupervised algorithms [hidden Markov model (HMM)] are studied to find the best technique with the highest classification accuracy (CA). A detailed comparison of the supervised and unsupervised algorithms in terms of the computational time and the CA is performed. The classified occupancy status is further utilized to evaluate the blocking probability of secondary user for future time slots, which can be used by system designers to define spectrum-allocation and spectrum-sharing policies. Numerical results show that SVM is the best algorithm among all the supervised and unsupervised classifiers. Based on this, we proposed a new SVM algorithm by combining it with a firefly algorithm (FFA), which is shown to outperform all the other algorithms.","0018-9545;00189545","","10.1109/TVT.2015.2487047","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7289481","Firefly algorithm (FFA);hidden Markov model (HMM);spectrum occupancy, support vector machine (SVM)","Accuracy;Hidden Markov models;Machine learning algorithms;Silicon;Support vector machines;Time-frequency analysis","cognitive radio;decision trees;hidden Markov models;learning (artificial intelligence);probability;radio spectrum management;regression analysis;support vector machines;telecommunication computing","CRN;FFA;HMM;NBC;Naive-Bayesian classifier;SVM algorithm;blocking probability;classification accuracy;cognitive radio networks;decision trees;firefly algorithm;hidden Markov model;linear regression;machine learning algorithms;secondary user;spectrum occupancy;spectrum-allocation;spectrum-sharing policies;supervised techniques;support vector machine;time slots;unsupervised algorithms;unsupervised classifiers","","","","","","20151005","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Identification of pre-emergency states in the electric power system on the basis of machine learning technologies","V. Kurbatsky; N. Tomin","Electric Power System Department, Melentiev Energy Systems Institute, Irkutsk, 664033 Russia","2016 12th World Congress on Intelligent Control and Automation (WCICA)","20160929","2016","","","378","383","The paper proposes a concept of an intelligent system for early detection of pre-emergency state in electric power system as an option of preventive operation and emergency control. The main goal of such a system is to early warn and prevent dangerous states and emergency situations before they lead to a system blackout. Consideration is given to a construction principle of systems for security monitoring and assessment on the basis of machine learning algorithms. The feasibility of the approach in a proof-of-concept has been demonstrated on the modified 53-node reliability test system (IEEE RTS-96) and IEEE 118 power system.","","","10.1109/WCICA.2016.7578291","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578291","","Control systems;Generators;Monitoring;Power system stability;Security;Voltage control","emergency management;learning (artificial intelligence);power system identification;power system reliability;power system security","53-node reliability test system;IEEE 118 power system;IEEE RTS-96;electric power system;emergency control;intelligent system;machine learning technologies;preemergency state identification;security monitoring;system blackout","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Applied Difference Techniques of Machine Learning Algorithm and Web-Based Management System for Sickle Cell Disease","M. Khalaf; A. J. Hussain; D. Al-Jumeily; R. Keenan; R. Keight; P. Fergus; I. O. Idowu","Appl. Comput. Res. Group, Liverpool John Moores Univ., Liverpool, UK","2015 International Conference on Developments of E-Systems Engineering (DeSE)","20160912","2015","","","231","235","Machine learning algorithm and web-based application systems have played a major role in improving the healthcare organisation in terms of continuous tele-monitoring therapy and maintaining telemedicine management systems. Currently, no intelligent system has been used in terms of managing sickle cell disease. However, this paper presents a system that facilitates a shift from manual methods to automated approach that can offer fast data collection with a reduced error rate. The system will be able to examine patient data and provide a suitable amount of Hydroxycarbamide drugs/liquid for each patient. By using a web-based system, we tend to improve patient welfare and mitigate patient illness before it gets worse over time, particularly with elderly people. The system will forward any critical concerns from the patient, it generates an automatic message to the medical doctors based on web-based platform in order to assist them with optimal decision-making. The initial case study that has been addressed in this project is how to make predictions for sickle cell disease for the amount of dose based on different architectures of machine learning in order to obtain high accuracy and performance. The most significant key for making predictions of the amount of medication is to enable healthcare organisation to provide accurate therapy recommendations based on previous data. The results using ANN showed that the proposed network produces significant improvements using the different evaluation methods. In our experiments, The MLP algorithm produced the best result in terms of the lowest error rates compare with other techniques. The Mean Square Error, Root Mean Square Error, Mean Absolute Error, and Mean Absolute Percentage Error achieved 17887.55, 133.74, 90.20 and 0.1345, respectively.","","","10.1109/DeSE.2015.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563643","E-Health;Machine Learning Algorithm;Neural Network;Real-time data;Sickle Cell Disease;Web-based System","Artificial neural networks;Computer architecture;Diseases;Machine learning algorithms;Medical diagnostic imaging;Monitoring","Internet;data acquisition;diseases;health care;learning (artificial intelligence);medical information systems;neural nets;patient treatment","ANN;MLP algorithm;Web-based application systems;Web-based management system;artificial neural network;data collection;healthcare organisation;hydroxycarbamide drugs;hydroxycarbamide liquid;intelligent system;machine learning algorithm;medication;optimal decision-making;patient data;patient illness;patient welfare;sickle cell disease;telemedicine management systems;telemonitoring therapy;therapy recommendations","","","","","","","13-14 Dec. 2015","","IEEE","IEEE Conference Publications"
"Study on correction of daily precipitation data of the Qinghai-Tibetan plateau with machine learning models","C. Ning; Y. Wang; Z. Nan; H. Chen; C. Liu","Baoji University of Science and Art, 721013, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","517","520","The daily precipitation datasets of the Qinghai-Tibetan plateau (QTP) are mainly assimilated from remote sensing products and in-situ observations. The accuracy of those datasets needs further improvement with environmental and meteorological factors. This paper selected the related environmental and meteorological factors as input; k-Nearest Neighbor (KNN), Multivariate Adaptive Regression Splines (MARS), Support Vector Machine (SVM), Multinomial Log-linear Models (MLM) and Artificial Neural Networks (ANN) as correction models; 112 upscaled daily precipitation observations from the standard meteorological stations as ground truth to correct the commonly used ITPCAS and CMORPH daily precipitation of the QTP. Results show that the KNN model has the highest correction accuracy. The distribution of the corrected ITPCAS precipitation is nearer to the spatial pattern of the precipitation over the QTP than the corrected CMORPH precipitation. The correction accuracy is influenced by the precipitation distribution pattern of the original dataset.","","","10.1109/IGARSS.2016.7729128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729128","data correction;machine learning model;precipitation data;the Qinghai-Tibetan Plateau","Adaptation models;Data models;Distribution functions;Graphical models;Mars;Rain;Support vector machines","atmospheric precipitation;learning (artificial intelligence);neural nets;regression analysis;remote sensing;support vector machines","China;Qinghai-Tibetan plateau;artificial neural networks;climate prediction center morphing method;daily precipitation data correction;daily precipitation observation;in situ observations;k-nearest neighbor;machine learning model;multinomial log-linear model;multivariate adaptive regression splines;precipitation distribution pattern;remote sensing products;spatial precipitation pattern;support vector machine","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning approach for classification of sentence polarity","N. Akkarapatty; N. S. Raj","Department of Computer Science with Information Systems, SCMS School of Engineering & Technology, Ernakulam, India","2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN)","20160915","2016","","","316","321","Opinion Mining is the process used to determine the attitude/opinion/emotion expressed by a person about a particular topic. Analyzing opinions is an integral part for making decisions. In the era of web, if a person wants to buy a product, he will look into the reviews and comments given by the experienced users in web. But it seems to be a tedious task to read the entire reviews available in the web. Hence people are interested in checking whether the review recommends to buy a product or not. If lot of reviews recommends to buy the product, user reach at a conclusion to buy the product, otherwise not to buy the product. In this study, Machine Learning approach is applied to the TripAdvisor dataset in order to develop an efficient review classification. For this work to be carried out, style markers are applied to each of the reviews. In the next stage, significant style markers are recognized with the help of some suitable feature selection method. Thus the reviews can be identified by developing a classifier using the style markers that help to characterize nature of reviews as positive or negative.","","","10.1109/SPIN.2016.7566711","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566711","Aspects;Classification;Opinion Mining;PoS tags;Sentiment Classification;ensemble model;machine learning","Data mining;Feature extraction;Machine learning algorithms;Sentiment analysis;Signal processing;Syntactics;Tagging","Internet;Web sites;data mining;feature selection;learning (artificial intelligence);pattern classification;sentiment analysis","TripAdvisor dataset;feature selection;machine learning;opinion mining;review classification;sentence polarity classification","","","","","","","11-12 Feb. 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques with probability vector for cooperative spectrum sensing in cognitive radio networks","Y. Lu; P. Zhu; D. Wang; M. Fattouche","Department of Electrical and Computer Engineering, University of Calgary","2016 IEEE Wireless Communications and Networking Conference","20160915","2016","","","1","6","We study cooperative spectrum sensing in cognitive radio networks (CRN) using machine learning techniques in this paper. A low-dimensional probability vector is proposed as the feature vector for machine learning based classification, instead of the N-dimensional energy vector in a CRN with a single primary user (PU) and N secondary users (SUs). This proposed method down-converts a high-dimensional feature vector to a constant two-dimensional feature vector for machine learning techniques while keeping the same spectrum sensing performance if not better. Due to its lower dimension, the probability vector based classification is capable of having a smaller training duration and a shorter classification time for testing vectors.","","Electronic:978-1-4673-9814-5; POD:978-1-4673-9815-2","10.1109/WCNC.2016.7564840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564840","Cognitive radio network (CRN);Cooperative spectrum sensing;Energy vector;K-means clustering algorithm;Probability vector;Support vector machine (SVM)","Clustering algorithms;Cognitive radio;Machine learning algorithms;Sensors;Support vector machines;Testing;Training","cognitive radio;cooperative communication;learning (artificial intelligence);pattern classification;probability;radio spectrum management;telecommunication computing;vectors","CRN;N-dimensional energy vector;cognitive radio networks;constant 2D feature vector;cooperative spectrum sensing;machine learning based classification;machine learning techniques;primary user;probability vector based classification;secondary users","","","","","","","3-6 April 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques for effective text analysis of social network E-health data","S. Saini; S. Kohli","Birla Institute of Technology, NOIDA (U.P.), India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","3783","3788","The World Wide Web has evolved very drastically and the recent advent in social networking and media rich websites has necessitated analysis of the social networks and opinions expressed by various users in media like blogs, tweets, and website pages alike. While a lot of previous researches have been done on product / CRM domain, relatively few research has been done on the network analysis of social media with a focus on extraction and opinion of e-Health data i.e. data expressed in various blogs or website pages by users regarding various aspects of their health. While marketing and medical companies can leverage this data to augment customer reach and thus further their profits, sentiment analysis and network analysis on the data can help various organizations understand health patterns, address people's concerns or predict outbreak patterns in case of contagious diseases. This paper outlines the machine learning techniques which are helpful in the analysis of medical domain data from Social networks. In this paper we compare the existing techniques of machine learning, discuss the advantages and challenges encompassing the perspectives involving the use of text mining methods for applications in E-health and medicine. We evaluate medical domain data classification efficiency using various metrics like ROC, AUC implemented via R language packages. We need to infer which machine learning technique is more relevant for processing data from social networks having medical terms in it.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724968","Analytics;Data mining;E-health;Medical domain data;Social Networks","Decision support systems;Handheld computers","health care;learning (artificial intelligence);sentiment analysis;social networking (online)","World Wide Web;machine learning techniques;medical domain data classification efficiency;network analysis;sentiment analysis;social network e-health data;text analysis;text mining methods","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Proposed retinal abnormality detection and classification approach: Computer aided detection for diabetic retinopathy by machine learning approaches","V. Raman; P. Then; P. Sumari","Faculty of Engineering Computing and Science, Swinburne University of Technology Sarawak, Kuching, Malaysia","2016 8th IEEE International Conference on Communication Software and Networks (ICCSN)","20161010","2016","","","636","641","Diabetes occurs when the pancreas fails to secrete enough insulin, slowly affecting the retina of the human eye, leading to diabetic retinopathy. The blood vessels in the retina get altered and have abnormality. Exudates are secreted, micro-aneurysms and haemorrhages occur in the retina. The appearance of these features represents the degree of severity of the disease. Early detection of diabetic retinopathy plays a major role in the success of such disease treatment. The main challenge is to extract exudates which are similar in colour property and size of the optic disk, and then micro-aneurysms are similar in colour and proximity with blood vessels. The main objective of the paper is to develop a computer aided detection system to find the abnormality of retinal imaging and detects the presence of abnormality features from retinal fundus images. There is few existing research works have been undergone by applying machine learning techniques, but existing approaches have not achieved a good accuracy of detection and they have not yielded successful performance in different datasets. The proposed methodology is to enhance the image and filter the noise, detect blood vessel and identify the optic disc, extract the exudates and micro aneurysms, extract the features and classify different stages of diabetic retinopathy into mild, moderate, severe non-proliferative diabetic retinopathy (NPDR) and proliferative Diabetic retinopathy (PDR) by using proposed machine learning methods. The expected output of proposed work in this paper will be a preliminary design and pilot prototype development.","","","10.1109/ICCSN.2016.7586601","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586601","diabetes;exudates;microaneurysm;segmentation and classification","Biomedical imaging;Blood vessels;Diabetes;Feature extraction;Retina;Retinopathy","biomedical optical imaging;diseases;eye;feature extraction;image classification;image enhancement;learning (artificial intelligence);medical image processing","blood vessel;computer aided detection;diabetic retinopathy detection;disease treatment;feature extraction;human eye;image enhancement;machine learning technique;microaneurysm;optic disk colour property;proliferative diabetic retinopathy;retinal abnormality detection;retinal fundus images;retinal imaging","","","","","","","4-6 June 2016","","IEEE","IEEE Conference Publications"
"Evaluation of machine learning algorithms for image quality assessment","G. T. Tchendjou; R. Alhakim; E. Simeu; F. Lebowsky","Universit&#x00E9; Grenoble Alpes, TIMA, F-38000, Grenoble, France, CNRS, TIMA, F-38000, Grenoble, France","2016 IEEE 22nd International Symposium on On-Line Testing and Robust System Design (IOLTS)","20161024","2016","","","193","194","In this article, we apply different machine learning (ML) techniques for building objective models, that permit to automatically assess the image quality in agreement with human visual perception. The six ML methods proposed are discriminant analysis, k-nearest neighbors, artificial neural network, non-linear regression, decision tree and fuzzy logic. Both the stability and the robustness of designed models are evaluated by using Monte-Carlo cross-validation approach (MCCV). The simulation results demonstrate that fuzzy logic model provides the best prediction accuracy.","","","10.1109/IOLTS.2016.7604697","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604697","Image Quality Assessment (IQA);Machine Learning (ML);Monte-Carlo cross-validation approach (MCCV)","Correlation;Fuzzy logic;Image quality;Measurement;Neural networks;Predictive models;Training","Monte Carlo methods;decision trees;fuzzy logic;image processing;learning (artificial intelligence);neural nets;regression analysis","MCCV;ML methods;Monte-Carlo cross-validation approach;artificial neural network;decision tree;fuzzy logic model;human visual perception;image quality assessment;k-nearest neighbors;machine learning algorithms;non-linear regression","","","","","","","4-6 July 2016","","IEEE","IEEE Conference Publications"
"Mobile Multi-agent Systems for the Internet-of-Things and Clouds Using the JavaScript Agent Machine Platform and Machine Learning as a Service","S. Bosse","Dept. of Math. & Comput. Sci., Univ. of Bremen, Bremen, Germany","2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)","20160926","2016","","","244","253","The Internet-of-Things (IoT) gets real in today's life and is becoming part of pervasive and ubiquitous computing networks offering distributed and transparent services. A unified and common data processing and communication methodology is required to merge the IoT, sensor networks, and Cloud-based environments seamless, which can be fulfilled by the mobile agent-based computing paradigm, discussed in this work. Currently, portability, resource constraints, security, and scalability of Agent Processing Platforms (APP) are essential issues for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet, addressed in this work. To simplify the development and deployment of MAS it would be desirable to implement agents directly in JavaScript, which is a well known and public widespread used programming language, and JS VMs are available on all host platforms including WEB browsers. The novel proposed JS Agent Machine (JAM) is capable to execute AgentJS agents in a sandbox environment with full run-time protection and Machine learning as a service. Agents can migrate between different JAM nodes seamless preserving their data and control state by using a on-the-fly code-to-text transformation in an extended JSON+ format. A Distributed Organization System (DOS) layer provides JAM node connectivity and security in the Internet, completed by a Directory-Name Service offering an organizational graph structure. Agent authorization and platform security is ensured with capability-based access and different agent privilege levels.","","","10.1109/FiCloud.2016.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575871","Agent Platforms;Agents;Cloud Computing;IoT","Cloud computing;Computer architecture;Internet of things;Mobile communication;Security;Sensors","Internet of Things;Java;authorisation;cloud computing;data protection;graph theory;learning (artificial intelligence);mobile agents;mobile computing;multi-agent systems;online front-ends;virtual machines","APP;AgentJS agents;DOS layer;Internet-of-Things;IoT;JAM node connectivity;JAM node security;JS VM;JS agent machine;JavaScript agent machine platform;MAS;Web browsers;agent authorization;agent privilege levels;agent processing platforms;capability-based access;cloud-based environment;communication methodology;control state;data preservation;data processing;directory-name service;distributed organization system layer;extended JSON+ format;heterogeneous networks;host platforms;machine learning-as-a-service;mobile multiagent systems;on-the-fly code-to-text transformation;organizational graph structure;pervasive computing network;platform security;portability constraint;resource constraint;run-time protection;sandbox environment;scalability constraint;security constraint;sensor networks;ubiquitous computing network","","","","","","","22-24 Aug. 2016","","IEEE","IEEE Conference Publications"
"Recent machine learning advancements in sensor-based mobility analysis: Deep learning for Parkinson's disease assessment","B. M. Eskofier; S. I. Lee; J. F. Daneault; F. N. Golabchi; G. Ferreira-Carvalho; G. Vergara-Diaz; S. Sapienza; G. Costante; J. Klucken; T. Kautz; P. Bonato","Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg (FAU), Erlangen, Germany","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","655","658","The development of wearable sensors has opened the door for long-term assessment of movement disorders. However, there is still a need for developing methods suitable to monitor motor symptoms in and outside the clinic. The purpose of this paper was to investigate deep learning as a method for this monitoring. Deep learning recently broke records in speech and image classification, but it has not been fully investigated as a potential approach to analyze wearable sensor data. We collected data from ten patients with idiopathic Parkinson's disease using inertial measurement units. Several motor tasks were expert-labeled and used for classification. We specifically focused on the detection of bradykinesia. For this, we compared standard machine learning pipelines with deep learning based on convolutional neural networks. Our results showed that deep learning outperformed other state-of-the-art machine learning algorithms by at least 4.6 % in terms of classification rate. We contribute a discussion of the advantages and disadvantages of deep learning for sensor-based movement assessment and conclude that deep learning is a promising method for this field.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590787","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590787","","Biomedical monitoring;Convolution;Feature extraction;Machine learning;Pipelines;Standards;Training","biomechanics;biomedical measurement;body sensor networks;diseases;inertial systems;learning (artificial intelligence)","Parkinson's disease assessment;bradykinesia;deep learning;inertial measurement units;machine learning;sensor-based mobility analysis","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning-based proactive data retention error screening in 1Xnm TLC NAND flash","Y. Nakamura; T. Iwasaki; K. Takeuchi","Chuo University, Tokyo, Japan","2016 IEEE International Reliability Physics Symposium (IRPS)","20160926","2016","","","PR-3-1","PR-3-4","A screening method to proactively reduce data retention, as well as program disturb errors. Repeated program disturb (P.D.) measurement indicates that 25% of P.D. errors are concentrated in 3.5% of the memory cells, called PD-weak cells. PD-weak cells have 2.4Ã— worse data retention (D.R.) than non-PD-weak cells, therefore D.R. errors are reduced by PD-weak cell screening. Proactive D.R. detection is a new capability, because conventional retention testing time is too long for chip testing. In 1Xnm TLC NAND flash, removal of PD-weak cells with <;2% overhead extends D.R. by 20%. The measurement method is described, and machine learning is applied to detect PD-weak cells. Detection rate vs. cost is also compared for 3 learning algorithms.","","","10.1109/IRPS.2016.7574632","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574632","NAND flash;machine learning;reliability","Error correction codes;Flash memories;Machine learning algorithms;Measurement uncertainty;Prediction algorithms;Predictive models;Support vector machines","NAND circuits;flash memories;learning (artificial intelligence)","PD-weak cells;TLC NAND flash;machine learning;memory cells;proactive data retention error screening;program disturb errors;repeated program disturb measurement;retention testing time;triple-level cell;worse data retention","","","","","","","17-21 April 2016","","IEEE","IEEE Conference Publications"
"Hunting Killer Tasks for Cloud System through Machine Learning: A Google Cluster Case Study","H. Tang; Y. Li; T. Jia; Z. Wu","Sch. of Software & Microelectron., Peking Univ., Beijing, China","2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)","20161013","2016","","","1","12","Motivated by frequent failures in cloud computing systems, we analyze failure frequency and failure continuity of tasks from the Google cloud cluster, and find what we call killer tasks that suffer from frequent failures and repeated rescheduling. Killer tasks cause unnecessary resource wasting and significant increase of scheduling workloads, which can be a big concern in cloud systems. We aim to recognize killer tasks at the very early stage of their occurrence so that they can be addressed proactively instead of being rescheduled repeatedly, so as to promote reliability and save resources. To recognize killer tasks from a large amount of tasks in real time is really challenging. In this paper, we first investigate characteristics and behavior patterns of killer tasks and then develop two machine learning based methods, K-HUNTER and C-HUNTER, for online recognition of killer tasks. The empirical results show that our approach performs at 97% of precision in recognizing killer tasks with an 89% timing advance and 88% of resource saving for the cloud system on average.","","","10.1109/QRS.2016.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589779","behavior pattern;cloud computing system;killer tasks;online recognition;time series","Cloud computing;Google;Pattern recognition;Predictive models;Processor scheduling;Servers;Time series analysis","cloud computing;fault tolerant computing;learning (artificial intelligence);resource allocation","C-HUNTER method;Google cloud cluster;K-HUNTER method;cloud computing systems;failure continuity;failure frequency;hunting killer task;machine learning;resource saving;scheduling workload","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"Improving detection rate using misuse detection and machine learning","R. Rajpal; S. Kaur; R. Kaur","Computer Science and Engineering, M. Tech, Thapar University, Patiala, India","2016 SAI Computing Conference (SAI)","20160901","2016","","","1131","1135","Network security is the provision made in an underlying computer network or rules made by the administrator to protect the network and its resources from unauthorized access. Network Security is becoming a crucial issue for all the firms and companies and with the increase in knowledge of intruders and hackers they have made many prosperous attempts to bring down web services and high-profile company networks. Misuse detection detects intrusions by matching the network traffic with a database of stored signatures and anomaly detection looks for behavior deviating from normal or common behavior for detecting intrusions. The primary objective of this paper is to combine both these techniques. The KDD dataset is used for this purpose. Finally, the data is processed by classification algorithms to obtain the results. The results show a high percentage of correct classification and accuracy. Experimental evaluation shows that the combined approach of Machine learning and misuse detection gives better performance.","","Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2","10.1109/SAI.2016.7556119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556119","Machine Learning;Misuse Detection","Classification algorithms;Communication networks;Intrusion detection;Support vector machines;Testing;Training","computer network security;learning (artificial intelligence);pattern classification;security of data","KDD dataset;Web services;classification algorithms;computer network;detection rate improvement;hackers;high-profile company networks;intruders;intrusion detection;machine learning;misuse detection;network security;network traffic;unauthorized access","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"Training restricted Boltzmann Machine with dynamic learning rate","L. Luo; Y. Wang; H. Peng; Z. Tang; S. You; X. Huang","Department of Automation, Xiamen University, Xiamen, China","2016 11th International Conference on Computer Science & Education (ICCSE)","20161006","2016","","","103","107","Restricted Boltzmann Machine (RBM) has been successfully applied to many different machine learning and pattern recognition problems. Usually, fixed learning rate (FLR) is used for training RBM. However, the reconstruction error (RCERR) with FLR may not be declined each iteration, which will result in a slow convergence speed. In this paper, we propose a method to dynamically choose the learning rate by reducing RCERR properly. The experiments on MNIST database and Caltech 101 Silhouettes database show the RBMs trained with dynamic learning rate (DLR) are better than that trained with FLR in classification accuracy and stability. It indicates DLR may be more suitable for training RBM.","","","10.1109/ICCSE.2016.7581563","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581563","Deep learning;Restricted Boltzmann Machine;learning rate","Bars;Databases;Feature extraction;Heuristic algorithms;Neural networks;Training;Training data","Boltzmann machines;learning (artificial intelligence);pattern recognition","Caltech 101 Silhouettes database;DLR;FLR;MNIST database;RBM;RCERR;dynamic learning rate;fixed learning rate;machine learning;pattern recognition problems;reconstruction error;restricted Boltzmann machine","","","","","","","23-25 Aug. 2016","","IEEE","IEEE Conference Publications"
"Application of supervised machine learning algorithms in diagnosis of abnormal voltage","L. Yifei; W. He; P. Shuai; S. Weiqiong; D. Ning; W. Fang","State Grid Beijing Electric Power Research Institute, Beijing 100045, China","2016 China International Conference on Electricity Distribution (CICED)","20160926","2016","","","1","5","This paper introduce a typical supervised machine learning algorithm, which is called C4.5 decision tree classification algorithm. On the basis of analyzing the reason of metering device abnormal voltage, we established a metering device voltage judgment model to evaluate the model performance, then use this model screen out the measurement anomaly, the on-site test results verified the accuracy of the model. This method can effectively improve the accuracy of this kind of abnormal judgment and avoid failure, improve the work efficiency of on-site troubleshooting.","","","10.1109/CICED.2016.7576347","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576347","Electricity information collection system;Intelligent watt-hour meter;Machine learning;abnormal voltage;decision tree","Analytical models;Classification algorithms;Decision trees;Machine learning algorithms;Predictive models;Training;Voltage measurement","learning (artificial intelligence);metering;trees (mathematics)","C4.5 decision tree classification algorithm;measurement anomaly;metering device abnormal voltage;metering device voltage judgment model;on-site troubleshooting;supervised machine learning algorithm","","","","","","","10-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning anomaly detection in large systems","J. Murphree","DRS Technologies, Huntsville, AL","2016 IEEE AUTOTESTCON","20161013","2016","","","1","9","We have a need for methods to efficiently determine the health of a system. Diagnostics and prognostics determine system heath through analysis of data from sensors. Anomalies in the data can help us determine if there is a failure or a pending failure. There are common statistical methods to detect anomalies in individual measurements. For systems with many measurements, the anomalies may occur as specific combinations of values. Large systems have various associated states and modes which define the valid measurements. The amount of data to analyze grows very quickly as the system becomes more complex. In recent years techniques have been developed to address large data analysis. Machine Learning encompasses a broad selection of tools to optimize a statistical model of the data. These tools include supervised learning techniques, such as linear regression and logistic regression, in which training data exists to tune the model. Unsupervised learning, such as clustering, is used to explore data which does not have a defined output label associated with inputs data. Standard approaches to training supervised learning systems require a large sample of positive and negative outcome data. Some uses of machine learning involve data where there are very few cases of negative outcomes. There are machine learning algorithms defined as Anomaly Detection which are designed to deal with this type of data. Simple algorithms include Gaussian Distribution Analysis, which assumes independence in distributions of data. Large Systems with anomalies defined in the dependent combinations of data require either a manual creation of combinations of independent variables, or Multivariate Gaussian Distribution Analysis, which does not scale well for large systems. A further complication is the mixture of linear and discrete data. Neural Networks are a type of learning system which has been applied to each of the individual needs addressed above. This paper describes an approach to anoma- y detection using neural networks for the specific problems in large systems to efficiently determine system health.","","","10.1109/AUTEST.2016.7589589","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589589","Anomaly Detection;Large Systems;Machine Learning;Neural Networks;Sparse Autoencoder","Correlation;Data models;Neural networks;Performance evaluation;Sensors;Supervised learning;Training data","data analysis;fault diagnosis;learning (artificial intelligence);neural nets;program diagnostics","anomaly detection;data analysis;diagnostics;failure;large systems;learning system;machine learning;neural networks;prognostics;sensors;statistical methods;system health","","","","","","","12-15 Sept. 2016","","IEEE","IEEE Conference Publications"
"Evaluation of Free Answer Comment Using Machine Learning by Word Evaluation","A. Shiwaku; N. Kobayashi; F. Kitagawa; H. Shiina","Grad. Sch. of Inf., Okayama Univ. of Sci., Okayama, Japan","2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)","20160901","2016","","","134","137","There is a call for an increase to education quality in response to FD (Faculty Development) activities becoming mandatory due to a revision of the University Establishment Standards in 2008. A survey on lectures was carried out as part of FD activities and used for improving lectures based on lecture evaluation and satisfaction from students. However, it is difficult to give an objective evaluation of this text data since these subjects of evaluation are wide-ranging, such as expectations towards lectures or opinions on teachers. An aggregation of the opinions gained through surveys also has its limits for manual assessment because of the heavy artificial costs and time costs required. Therefore, it is difficult to evaluate all comments given in a survey. For this reason there has been extensive research done in recent years on categorizing evaluation texts and documents through evaluation expression of words and machine learning. There has been related research estimating the sentiment polarity of the entirety of the writing using both positives and negatives that appear within, as well as research that extracts elements of evaluation information and measures its sentiment polarity. Machine learning for writing categorization has also achieved a high level of accuracy, with research existing which uses the Naive Bayes Classifier and a Support Vector Machine. This report uses actual open ended responses to surveys to describe a method for re-evaluating a comment by estimating the evaluation of words within comments from a comment evaluation done through machine learning, as well as a method that uses a dictionary of emotional words to attach a polarity value to vocabulary and estimate the evaluation of the body of a comment from that value. Finally, it describes the results of teacher and lecture evaluations from each comment evaluation.","","Electronic:978-1-4673-8985-3; POD:978-1-4673-8986-0","10.1109/IIAI-AAI.2016.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557590","Comment Evaluation;Lecture Evaluation;Semi-supervised learning;Word Polarity","Dictionaries;Education;Electronic mail;Estimation;Informatics;Standards;Supervised learning","Bayes methods;data aggregation;learning (artificial intelligence);pattern classification;sentiment analysis;support vector machines","FD activities;artificial costs;comment body evaluation;comment evaluation;document categorization;education quality;element extraction;emotional word dictionary;faculty development activities;free-answer comment evaluation;lecture evaluation;lecture improvement;machine learning;naive Bayes classifier;negative word polarity;objective evaluation;opinion aggregation;polarity value;positive word polarity;sentiment polarity estimation;student satisfaction;support vector machine;teacher lecture evaluation;text categorization;text data;time costs;university establishment standards;vocabulary;word evaluation;word evaluation expression;writing entirety","","","","","","","10-14 July 2016","","IEEE","IEEE Conference Publications"
"Using machine learning and SAR data for the upscaling of large scale modelled soil moisture in the Alps","F. Greifeneder; C. Notarnicola; W. Wagner","","Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar","20160905","2016","","","1","4","Knowledge of the spatial and temporal distribution of soil moisture is important for many geoscience disciplines. Currently available remote sensing soil moisture products are not able to fully represent the heterogeneous patterns in mountain areas. In this paper we present a machine learning based approach for the upscaling of coarse scale modelled soil moisture based on Synthetic Aperture Radar data. For this purpose a statistical model was trained using the Gradient Tree Boosting approach. Results show that with the trained model it is possible to reproduce the reference data and increase spatial detailed of mapped soil moisture content.","","Paper:978-3-8007-4228-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559488","","","","","","","","","","","6-9 June 2016","","VDE","VDE Conference Publications"
"Fall detection algorithms for real-world falls harvested from lumbar sensors in the elderly population: A machine learning approach","A. K. Bourke; J. Klenk; L. Schwickert; K. Aminian; E. A. F. Ihlen; S. Mellone; J. L. Helbostad; L. Chiari; C. Becker","Department of Neuroscience, Faculty of Medicine, Norwegian University of Science and Technology, Trondheim, Norway","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","3712","3715","Automatic fall detection will promote independent living and reduce the consequences of falls in the elderly by ensuring people can confidently live safely at home for linger. In laboratory studies inertial sensor technology has been shown capable of distinguishing falls from normal activities. However less than 7% of fall-detection algorithm studies have used fall data recorded from elderly people in real life. The FARSEEING project has compiled a database of real life falls from elderly people, to gain new knowledge about fall events and to develop fall detection algorithms to combat the problems associated with falls. We have extracted 12 different kinematic, temporal and kinetic related features from a data-set of 89 real-world falls and 368 activities of daily living. Using the extracted features we applied machine learning techniques and produced a selection of algorithms based on different feature combinations. The best algorithm employs 10 different features and produced a sensitivity of 0.88 and a specificity of 0.87 in classifying falls correctly. This algorithm can be used distinguish real-world falls from normal activities of daily living in a sensor consisting of a tri-axial accelerometer and tri-axial gyroscope located at L5.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591534","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591534","","Acceleration;Accelerometers;Angular velocity;Feature extraction;Gyroscopes;Machine learning algorithms;Sensors","accelerometers;angular measurement;biomedical equipment;feature extraction;geriatrics;gyroscopes;kinematics;learning (artificial intelligence);medical computing;pattern classification;sensors","elderly people;elderly population;fall classification;fall detection algorithms;inertial sensor technology;kinematic related feature extraction;kinetic related feature extraction;lumbar sensors;machine learning approach;real-world falls;temporal related feature extraction;triaxial accelerometer;triaxial gyroscope","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"An Investigation Into Machine Learning Regression Techniques for the Leaf Rust Disease Detection Using Hyperspectral Measurement","D. Ashourloo; H. Aghighi; A. A. Matkan; M. R. Mobasheri; A. M. Rad","Remote Sensing and GIS Center, Faculty of Earth Sciences, Shahid Beheshti University, Tehran, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20161003","2016","9","9","4344","4351","The complex impacts of disease stages and disease symptoms on spectral characteristics of the plants lead to limitation in disease severity detection using the spectral vegetation indices (SVIs). Although machine learning techniques have been utilized for vegetation parameters estimation and disease detection, the effects of disease symptoms on their performances have been less considered. Hence, this paper investigated on 1) using partial least square regression (PLSR), v support vector regression (v-SVR), and Gaussian process regression (GPR) methods for wheat leaf rust disease detection, 2) evaluating the impact of training sample size on the results, 3) the influence of disease symptoms effects on the predictions performances of the above-mentioned methods, and 4) comparisons between the performances of SVIs and machine learning techniques. In this study, the spectra of the infected and non infected leaves in different disease symptoms were measured using a non imaging spectroradiometer in the electromagnetic region of 350 to 2500 nm. In order to produce a ground truth dataset, we employed photos of a digital camera to compute the disease severity and disease symptoms fractions. Then, different sample sizes of collected datasets were utilized to train each method. PLSR showed coefficient of determination (R<sup>2</sup>) values of 0.98 (root mean square error (RMSE) = 0.6) and 0.92 (RMSE = 0.11) at leaf and canopy, respectively. SVR showed R<sup>2</sup> and RMSE close to PLSR at leaf (R<sup>2</sup> = 0.98, RMSE = 0.05) and canopy (R<sup>2</sup> = 0.95, RMSE = 0.12) scales. GPR showed R<sup>2</sup> values of 0.98 (RMSE = 0.03) and 0.97 (RMSE = 0.11) at leaf and canopy scale, respectively. Moreover, GPR represents better performances than others using small training sample size. The results represent that the machine learning techniques in contrast to SVIs are not sensitive to different disease symptoms and their results are reliable.","1939-1404;19391404","","10.1109/JSTARS.2016.2575360","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533506","$nu$ support vector regression $(nu$-SVR);Disease severity;Gaussian process regression (GPR);PLSR;disease symptoms;hyperspectral measurement;plant disease;wheat leaf rust (WLR)","Diseases;Ground penetrating radar;Hyperspectral sensors;Support vector machines;Training;Vegetation mapping","Gaussian processes;diseases;hyperspectral imaging;learning (artificial intelligence);regression analysis;remote sensing;vegetation","Gaussian process regression;digital camera;disease severity detection;disease stage;disease symptom;hyperspectral measurement;machine learning regression technique;partial least square regression;plant spectral characteristics;spectral vegetation index;spectroradiometer;support vector regression;vegetation parameter estimation;wheat leaf rust disease detection","","1","","","","20160804","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Learning Boltzmann machine with EM-like method","J. Song; C. Yuan","Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","2282","2289","We propose an expectation-maximization-like(EM-like) method to train Boltzmann machine with unconstrained connectivity. It adopts Monte Carlo approximation in the E-step, and replaces the intractable likelihood objective with efficiently computed objectives or directly approximates the gradient of likelihood objective in the M-step. The EM-like method is a modification of alternating minimization. We prove that EM-like method will be the exactly same with contrastive divergence in restricted Boltzmann machine if the M-step of this method adopts special approximation. We also propose a new measure to assess the performance of Boltzmann machine as generative models of data, and its computational complexity is O(Rmn). Finally, we demonstrate the performance of EM-like method using numerical experiments.","","","10.1109/IJCNN.2016.7727482","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727482","","Approximation algorithms;Computational modeling;Data models;Estimation;Minimization;Monte Carlo methods;Training","Boltzmann machines;Monte Carlo methods;computational complexity;expectation-maximisation algorithm;learning (artificial intelligence)","Boltzmann machine learning;EM-like method;Monte Carlo approximation;computational complexity;expectation-maximization;restricted Boltzmann machine","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"A hybrid rule and machine learning based generic alerting platform for smart environments","J. Rafferty; J. Synnott; C. Nugent","Ulster University, Jordanstown, Northern Ireland, BT37 0QB, UK","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","5405","5408","Existing smart environment based alert solutions have adopted a relatively complex and tailored approach to supporting individuals. These solutions have involved sensor based monitoring, activity recognition and assistance provisioning. Traditionally they have suffered from a number of issues, rooted in scalability and performance, associated with complex activity recognition processes. This paper introduces a generic approach to realizing an alerting platform within a smart environment. The core concept of this approach is presented and placed within the context of related work. A description of the approach is provided, followed by an evaluation. This evaluation shows the approach offers reasonable accuracy, future work will increase accuracy.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591949","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591949","","Computer vision;Monitoring;Temperature measurement;Temperature sensors","biomedical engineering;intelligent sensors;learning (artificial intelligence)","complex activity recognition;generic alerting platform;hybrid rule-machine learning;smart environments","","2","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach to automated correction of ETgX documents","K. Chuvilin","Institute of Computing for Physics and Technology, Protvino, Russia, Moscow Institute of Physics and Technology (State University), Moscow, Russia","2016 18th Conference of Open Innovations Association and Seminar on Information Security and Protection of Information Technology (FRUCT-ISPIT)","20160908","2016","","","33","40","The problem is the automatic synthesis of formal correcting rules for LATEX documents. Each document is represented as a syntax tree. Tree node mappings of initial documents to edited documents form the training set, which is used to generate the rules. Rules with a simple structure, which implement removal, insertion or replacing operations of single node and use linear sequence of nodes to select a position are synthesized primarily. The constructed rules are grouped based on the positions of applicability and quality. The rules that use tree-like structure of nodes to select the position are studied. The changes in the quality of the rules during the sequential increase of the training document set are analyzed.","","Electronic:978-9-5268-3973-8; POD:978-1-5090-2500-8","10.1109/FRUCT-ISPIT.2016.7561505","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7561505","","Particle separators;Pattern matching;Physics;Publishing;Syntactics;Training;Vegetation","document handling;learning (artificial intelligence)","LATEX document correction;formal correcting rule synthesis;machine learning approach;syntax tree representation;tree node mappings;tree-like node structure","","","","","","","18-22 April 2016","","IEEE","IEEE Conference Publications"
"A review of supervised machine learning algorithms","A. Singh; N. Thakur; A. Sharma","Bharati Vidyapeeth's College of Engineering, A-4, Paschim Vihar, Rohtak Road, New Delhi - 110063, India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","1310","1315","Supervised machine learning is the construction of algorithms that are able to produce general patterns and hypotheses by using externally supplied instances to predict the fate of future instances. Supervised machine learning classification algorithms aim at categorizing data from prior information. Classification is carried out very frequently in data science problems. Various successful techniques have been proposed to solve such problems viz. Rule-based techniques, Logic-based techniques, Instance-based techniques, stochastic techniques. This paper discusses the efficacy of supervised machine learning algorithms in terms of the accuracy, speed of learning, complexity and risk of over fitting measures. The main objective of this paper is to provide a general comparison with state of art machine learning algorithms.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724478","Artificial Neural Networks (ANN);Bayesian Network (BN);Decision Trees (DT);Logistic Regression (LR);Random Forests (RF);Supervised Machine Learning;Support Vector Machine (SVM);k-Nearest Neighbors (k-NN)","Artificial neural networks;Classification algorithms;Decision support systems;Handheld computers;Machine learning algorithms;Roads;Support vector machines","learning (artificial intelligence);pattern classification","data categorization;data science problems;externally supplied instances;instance-based techniques;logic-based techniques;rule-based techniques;stochastic techniques;supervised machine learning classification algorithms","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques for intrusion detection on public dataset","U. S. K. P. M. Thanthrige; J. Samarabandu; X. Wang","Electrical and Computer Engineering, University Of Western Ontario, London, Ontario, Canada","2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","20161103","2016","","","1","4","The development of computer based systems expands the usage of computer based application in human life. It can be observed that illegal activities such as unauthorized data access, data theft, data modification and various other intrusion activities are rapidly growing during last decade. Hence, deployment and continuous improvement of Intrusion Detection Systems (IDS) are of paramount importance. Training, testing and evaluation of IDS with real network traffic is significant challenge, so most of IDS evaluation is based on intrusion datasets. Therefore, analysis of intrusion datasets are of paramount importance. In this paper, we evaluated Aegean Wi-Fi Intrusion Dataset (AWID) with different machine learning techniques. Feature reduction techniques such as Information Gain (IG) and Chi-Squared statistics (CH) were applied to evaluate dataset performance with feature reduction. Results of experiments show that feature reduction can lead to better analysis in terms of accuracy, processing time and complexity. It was observed that, the maximum increment of classification accuracy with feature reduction from 110 to 41 is 2.4%.","","","10.1109/CCECE.2016.7726677","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726677","","Computers;IEEE 802.11 Standard;Intrusion detection;Labeling;Testing;Training;Vegetation","computer network security;learning (artificial intelligence);pattern classification;telecommunication traffic;wireless LAN","AWID;Aegean Wi-Fi Intrusion Dataset;IDS evaluation;IDS testing;IDS training;classification accuracy;computer based application;computer based systems;feature reduction techniques;illegal activities;intrusion dataset analysis;intrusion detection systems;machine learning techniques;public dataset","","","","","","","15-18 May 2016","","IEEE","IEEE Conference Publications"
"A memristor network with coupled oscillator and crossbar towards L2-norm based machine learning","Leibin Ni; Hantao Huang; H. Yu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798","2016 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)","20160915","2016","","","179","184","This paper introduces a memristor-network based accelerator for L2-norm based machine learning. A coupled-memristor-oscillator network is developed for a L2-norm calculation; and a binary-memristor-crossbar network is developed to accelerate matrix-vector multiplication. As such, one can map gradient-descent (of L2-norm) based on-line machine learning on the proposed memristor-network that is composed of coupled-oscillator (to sample L2-norm) and binary-crossbar (to digitize L2-norm). Experiment results have shown that such a memristor-network based accelerator can achieve significant power reduction and runtime speed-up for both training and testing compared to the conventional CMOS-CPU based implementation.","","Electronic:978-1-4503-4330-5; POD:978-1-4673-8927-3","10.1145/2950067.2950083","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7568650","","Data analysis;Memristors;Neurons;Oscillators;Resistance;Training","coupled circuits;gradient methods;learning (artificial intelligence);matrix multiplication;memristors;oscillators","L2-norm based machine learning;L2-norm calculation;binary-memristor-crossbar network;coupled-memristor-oscillator network;gradient-descent based online machine learning;matrix-vector multiplication;memristor-network based accelerator","","","","","","","18-20 July 2016","","IEEE","IEEE Conference Publications"
"A novel hidden danger prediction method in cloud-based intelligent industrial production management using timeliness managing extreme learning machine","X. Luo; Xiaona Yang; Weiping Wang; X. Chang; X. Wang; Zhigang Zhao","School of Computer and Communication Engineering, University of Science and Technology Beijing, 100083, China","China Communications","20160902","2016","13","7","74","82","To prevent possible accidents, the study of data-driven analytics to predict hidden dangers in cloud service-based intelligent industrial production management has been the subject of increasing interest recently. A machine learning algorithm that uses timeliness managing extreme learning machine is utilized in this article to achieve the above prediction. Compared with traditional learning algorithms, extreme learning machine (ELM) exhibits high performance because of its unique feature of a high generalization capability at a fast learning speed. Timeliness managing ELM is proposed by incorporating timeliness management scheme into ELM. When using the timeliness managing ELM scheme to predict hidden dangers, newly incremental data could be added prior to the historical data to maximize the contribution of the newly incremental training data, because the incremental data may be able to contribute reasonable weights to represent the current production situation according to practical analysis of accidents in some industrial productions. Experimental results from a coal mine show that the use of timeliness managing ELM can improve the prediction accuracy of hidden dangers with better stability compared with other similar machine learning methods.","1673-5447;16735447","","10.1109/CC.2016.7559078","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559078","cloud service;extreme learning machine;incremental learning;prediction","Artificial neural networks;Cloud computing;Data models;Machine learning algorithms;Neurons;Prediction algorithms;Production","accident prevention;cloud computing;learning (artificial intelligence);production engineering computing;production management","ELM;cloud-based intelligent industrial production management;data-driven analytics;hidden danger prediction method;machine learning algorithm;timeliness management scheme;timeliness managing extreme learning machine","","","","","","","July 2016","","IEEE","IEEE Journals & Magazines"
"A machine learning approach for predicting bank credit worthiness","R. E. Turkson; E. Y. Baagyere; G. E. Wenya","School of Computer Science and Engineering, University of Electronic Science and Technology of China","2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR)","20161013","2016","","","1","7","Machine learning is an emerging technique for building analytic models for machines to ""learn"" from data and be able to do predictive analysis. The ability of machines to ""learn"" and do predictive analysis is very important in this era of big data and it has a wide range of application areas. For instance, banks and financial institutions are sometimes faced with the challenge of what risk factors to consider when advancing credit/loans to customers. For several features/attributes of the customers are normally taken into consideration, but most of these features have little predictive effect on the credit worthiness or otherwise of the customer. Furthermore, a robust and effective automated bank credit risk score that can aid in the prediction of customer credit worthiness very accurately is still a major challenge facing many banks. In this paper, we examine a real bank credit data and conduct several machine learning algorithms on the data for comparative analysis and to choose which algorithms are the best fit for learning bank credit data. The algorithms gave over 80% accuracy in prediction. Furthermore, the most important features that determine whether a customer will default or otherwise in paying his/her credit the next month are extracted from a total of 23 features. We then applied these most important features on some selected machine learning algorithms and compare their predictive accuracy with the other algorithms that used all the 23 features. The results show no significant di erence, signifying that these features can accurately determine the credit worthiness of the customers. Finally, we formulate a predictive model using the most important features to predict the credit worthiness of a given customer.","","","10.1109/ICAIPR.2016.7585216","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585216","bank credit;classification;confusion matrix;machine learning;predictive analysis","Algorithm design and analysis;Data mining;Feature extraction;Machine learning algorithms;Neural networks;Prediction algorithms;Predictive models","bank data processing;data analysis;feature extraction;learning (artificial intelligence)","bank credit risk score;bank credit worthiness prediction;feature extraction;machine learning;predictive analysis","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Unsupervised 3D Local Feature Learning by Circle Convolutional Restricted Boltzmann Machine","Z. Han; Z. Liu; J. Han; C. M. Vong; S. Bu; X. Li","Northwestern Polytechnical University, Xi&#x2019;an, China","IEEE Transactions on Image Processing","20160926","2016","25","11","5331","5344","Extracting local features from 3D shapes is an important and challenging task that usually requires carefully designed 3D shape descriptors. However, these descriptors are hand-crafted and require intensive human intervention with prior knowledge. To tackle this issue, we propose a novel deep learning model, namely circle convolutional restricted Boltzmann machine (CCRBM), for unsupervised 3D local feature learning. CCRBM is specially designed to learn from raw 3D representations. It effectively overcomes obstacles such as irregular vertex topology, orientation ambiguity on the 3D surface, and rigid or slightly non-rigid transformation invariance in the hierarchical learning of 3D data that cannot be resolved by the existing deep learning models. Specifically, by introducing the novel circle convolution, CCRBM holds a novel ring-like multi-layer structure to learn 3D local features in a structure preserving manner. Circle convolution convolves across 3D local regions via rotating a novel circular sector convolution window in a consistent circular direction. In the process of circle convolution, extra points are sampled in each 3D local region and projected onto the tangent plane of the center of the region. In this way, the projection distances in each sector window are employed to constitute a novel local raw 3D representation called projection distance distribution (PDD). In addition, to eliminate the initial location ambiguity of a sector window, the Fourier transform modulus is used to transform the PDD into the Fourier domain, which is then conveyed to CCRBM. Experiments using the learned local features are conducted on three aspects: global shape retrieval, partial shape retrieval, and shape correspondence. The experimental results show that the learned local features outperform other state-of-the-art 3D shape descriptors.","1057-7149;10577149","","10.1109/TIP.2016.2605920","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559748","3D shapes;Circle convolutional restricted Boltzmann machine;deep learning;fourier transform modulus;geometry processing;projection distance distribution","Convolution;Feature extraction;Machine learning;Shape;Solid modeling;Three-dimensional displays;Two dimensional displays","Boltzmann machines;Fourier transforms;convolution;feature extraction;feedforward neural nets;image representation;image retrieval;unsupervised learning","3D local regions;3D shape descriptors;CCRBM;Fourier transform modulus;PDD;circle convolutional restricted Boltzmann machine;consistent circular direction;deep learning model;global shape retrieval;local feature extraction;novel circular sector convolution window;novel local raw 3D representation;novel ring-like multilayer structure;partial shape retrieval;projection distance distribution;shape correspondence;unsupervised 3D local feature learning","","","","","","20160902","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Performance of Machine Learning Algorithms for Class-Imbalanced Process Fault Detection Problems","T. Lee; K. B. Lee; C. O. Kim","Department of Information and Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Semiconductor Manufacturing","20161027","2016","29","4","436","445","In recent years, the semiconductor manufacturing industry has recognized class imbalance as a major impediment to the development of high-performance fault detection (FD) models. Class imbalance refers to skews in class distribution in which normal wafer samples are considerably more abundant than fault samples. In such a situation, standard machine learning algorithms create FD models with classification boundaries that are biased toward majority-class data, resulting in high type II error rates. In this paper, we compare the performance of machine learning algorithms for class-imbalanced FD problems. We evaluate the performance of three sampling-based algorithms, four ensemble algorithms, four instance-based algorithms, and two support vector machine algorithms. Two experiments were conducted to compare algorithm performance using etching process data and chemical vapor deposition process data. Different data scenarios were considered by setting the imbalance ratio to three levels. The results of the experiments indicated that the instance-based algorithms presented excellent performance even when the imbalance ratio increased.","0894-6507;08946507","","10.1109/TSM.2016.2602226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549079","Class imbalance;fault detection;machine learning;performance evaluation;semiconductor manufacturing","Boosting;Fault detection;Machine learning algorithms;Performance evaluation;Semiconductor device modeling;Support vector machines","chemical vapour deposition;electronic engineering computing;etching;fault diagnosis;learning (artificial intelligence);semiconductor industry;semiconductor materials;semiconductor technology;support vector machines","FD models;chemical vapor deposition process data;class imbalance;ensemble algorithms;etching process data;instance-based algorithms;machine learning algorithms;majority-class data;process fault detection problems;sampling-based algorithms;semiconductor manufacturing industry;support vector machine algorithms;type II error rates;wafer samples","","","","","","20160824","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Novel RRAM-enabled 1T1R synapse capable of low-power STDP via burst-mode communication and real-time unsupervised machine learning","S. Ambrogio; S. Balatti; V. Milo; R. Carboni; Z. Wang; A. Calderoni; N. Ramaswamy; D. Ielmini","DEIB, Politecnico di Milano and IU.NET, 20133, Italy","2016 IEEE Symposium on VLSI Technology","20160922","2016","","","1","2","We present a new electronic synapse for neuromorphic computing consisting of a 1T1R structure based on HfO<sub>2</sub> RRAM technology, and capable of STDP and pattern learning. Power consumption is reduced by adopting short POST spike and burst-mode integration. MNIST classification shows promising learning and classification efficiency. These results support RRAM as an enabling technology for low-power neuromorphic hardware.","","","10.1109/VLSIT.2016.7573432","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573432","","Fires;Logic gates;Neuromorphics;Neurons;Power demand;Robustness","hafnium compounds;low-power electronics;plasticity;resistive RAM;unsupervised learning","1T1R structure;HfO2 RRAM technology;HfO<sub>2</sub>;MNIST classification;STDP;burst-mode communication;burst-mode integration;electronic synapse;low-power neuromorphic hardware;neuromorphic computing;pattern learning;power consumption;real-time unsupervised machine learning;short POST spike","","","","","","","14-16 June 2016","","IEEE","IEEE Conference Publications"
"Identification of promising research directions using machine learning aided medical literature analysis","V. Andrei; O. ArandjeloviÄ‡","School of Computer Science, University of St Andrews, St Andrews KY16 9SX, Fife, Scotland, United Kingdom","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2471","2474","The rapidly expanding corpus of medical research literature presents major challenges in the understanding of previous work, the extraction of maximum information from collected data, and the identification of promising research directions. We present a case for the use of advanced machine learning techniques as an aide in this task and introduce a novel methodology that is shown to be capable of extracting meaningful information from large longitudinal corpora, and of tracking complex temporal changes within it.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591231","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591231","","Bayes methods;Data mining;Data models;Evolution (biology);Mixture models;Probability distribution;Vocabulary","learning (artificial intelligence);medical computing","longitudinal corpora;machine learning;maximum information extraction;medical literature analysis;medical research literature;research direction","","1","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning with Sensitivity Analysis to Determine Key Factors Contributing to Energy Consumption in Cloud Data Centers","Y. W. Foo; C. Goh; Y. Li","Sch. of Eng., Univ. of Glasgow, Glasgow, UK","2016 International Conference on Cloud Computing Research and Innovations (ICCCRI)","20161020","2016","","","107","113","Machine learning (ML) approach to modeling and predicting real-world dynamic system behaviours has received widespread research interest. While ML capability in approximating any nonlinear or complex system is promising, it is often a black-box approach, which lacks the physical meanings of the actual system structure and its parameters, as well as their impacts on the system. This paper establishes a model to provide explanation on how system parameters affect its output(s), as such knowledge would lead to potential useful, interesting and novel information. The paper builds on our previous work in ML, and also combines an evolutionary artificial neural networks with sensitivity analysis to extract and validate key factors affecting the cloud data center energy performance. This provides an opportunity for software analysts to design and develop energy-aware applications and for Hadoop administrator to optimize the Hadoop infrastructure by having Big Data partitioned in bigger chunks and shortening the time to complete MapReduce jobs.","","","10.1109/ICCCRI.2016.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600185","artificial neural networks;cloud computing;energy efficiency;genetic algorithm;machine learning;sensitivity analysis","Artificial neural networks;Biological cells;Cloud computing;Computational modeling;Data models;Servers;Temperature sensors","Big Data;cloud computing;computer centres;energy conservation;learning (artificial intelligence);neural nets;parallel processing;power aware computing;sensitivity analysis","Big Data;Hadoop administrator;Hadoop infrastructure;ML approach;MapReduce jobs;black-box approach;cloud data center energy performance;cloud data centers;complex system;energy consumption;energy-aware applications;evolutionary artificial neural networks;machine learning;real-world dynamic system behaviours;sensitivity analysis;software analysts","","","","","","","4-5 May 2016","","IEEE","IEEE Conference Publications"
"Data-driven machine learning approach for predicting volumetric moisture content of concrete using resistance sensor measurements","K. Thiyagarajan; S. Kodagoda; N. Ulapane","Centre for Autonomous Systems, University of Technology Sydney, Sydney, Australia","2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)","20161024","2016","","","1288","1293","In sewerage industry, hydrogen sulphide induced corrosion of reinforced concretes is a global problem. To achieve a comprehensive knowledge of the propagation of concrete corrosion, it is vital to monitor the critical factors such as moisture. In this context, this paper investigates the use of resistance measuring and processing for estimating the concrete moisture content. The behavior of concrete moisture with resistance and surface temperature are studied and the effects of pH concentration on concrete are analyzed. Gaussian Process regression modeling is carried out to predict volumetric moisture content of concrete, where the results from experimental data are used to train the prediction model.","","","10.1109/ICIEA.2016.7603783","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603783","Gaussian Process;hydrogen sulphide corrosion;prediction;resistance sensor;volumetric concrete moisture content","Concrete;Corrosion;Moisture;Resistance;Surface treatment;Temperature measurement;Temperature sensors","Gaussian processes;corrosion;data handling;estimation theory;hydrogen compounds;learning (artificial intelligence);regression analysis;reinforced concrete;sanitary engineering","Gaussian process regression modeling;concrete moisture content estimation;concrete volumetric moisture content prediction;data-driven machine learning approach;hydrogen sulphide induced corrosion;pH concentration;reinforced concrete corrosion;resistance sensor measurements;sewerage industry","","","","","","","5-7 June 2016","","IEEE","IEEE Conference Publications"
"Centralized and Localized Data Congestion Control Strategy for Vehicular Ad Hoc Networks Using a Machine Learning Clustering Algorithm","N. Taherkhani; S. Pierre","Mobile Computing and Networking Research Laboratory (LARIM), Department of Computer and Software Engineering, &#x00C9;cole Polytechnique de Montr&#x00E9;al, Montreal, QC, Canada","IEEE Transactions on Intelligent Transportation Systems","20161028","2016","17","11","3275","3285","In an urban environment, intersections are critical locations in terms of road crashes and number of killed or injured people. Vehicular ad hoc networks (VANETs) can help reduce the traffic collisions at intersections by sending warning messages to the vehicles. However, the performance of VANETs should be enhanced to guarantee delivery of the messages, particularly safety messages to the destination. Data congestion control is an efficient way to decrease packet loss and delay and increase the reliability of VANETs. In this paper, a centralized and localized data congestion control strategy is proposed to control data congestion using roadside units (RSUs) at intersections. The proposed strategy consists of three units for detecting congestion, clustering messages, and controlling data congestion. In this strategy, the channel usage level is measured to detect data congestion in the channels. The messages are gathered, filtered, and then clustered by machine learning algorithms. K-means algorithm clusters the messages based on message size, validity of messages, and type of messages. The data congestion control unit determines appropriate values of transmission range and rate, contention window size, and arbitration interframe spacing for each cluster. Finally, RSUs at the intersections send the determined communication parameters to the vehicles stopped before the red traffic lights to reduce communication collisions. Simulation results show that the proposed strategy significantly improves the delay, throughput, and packet loss ratio in comparison with other congestion control strategies using the proposed congestion control strategy.","1524-9050;15249050","","10.1109/TITS.2016.2546555","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458837","$K$-means algorithm;Congestion control;machine learning algorithms;quality of service;vehicular ad hoc networks","Delays;Machine learning algorithms;Quality of service;Reliability;Roads;Safety;Vehicles","control engineering computing;learning (artificial intelligence);pattern clustering;telecommunication computing;telecommunication congestion control;vehicular ad hoc networks","K-means algorithm;RSU;VANET;centralized data congestion control strategy;channel usage level;congestion control strategy;data congestion detection;localized data congestion control strategy;machine learning clustering algorithm;roadside units;vehicular ad hoc networks","","","","","","20160425","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Prognosis of NBTI aging using a machine learning scheme","N. Karimi; K. Huang","Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854, United States","2016 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)","20161027","2016","","","7","10","Circuit aging is an important failure mechanism in nanoscale designs and is a growing concern for the reliability of future systems. Aging results in circuit performance degradation over time and the ultimate circuit failure. Among aging mechanisms, Negative-Bias Temperature Instability (NBTI) is the main limiting factor of circuits lifetime. Estimating the effect of aging-related degradation, before it actually occurs, is crucial for developing aging prevention/mitigations actions to avoid circuit failures. In this paper, we propose a general-purpose IC aging prognosis approach by considering a comprehensive set of IC operating conditions including workload, usage time and operating temperature. In addition, our model considers process variation by using a calibration technique applied at the time of manufacturing. Experimental results confirms that our model is able to accurately predict the NBTI-related path delay degradation under various operating conditions. The proposed model is robust to process variations.","","","10.1109/DFT.2016.7684060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7684060","","Aging;Degradation;Delays;Integrated circuit modeling;Negative bias temperature instability;Prognostics and health management;Thermal variables control","ageing;calibration;failure analysis;integrated circuit reliability;learning (artificial intelligence);negative bias temperature instability","IC aging prognosis approach;NBTI aging;aging-related degradation;calibration technique;circuit failure;circuit performance degradation;circuits lifetime;failure mechanism;machine learning scheme;negative-bias temperature instability;process variation","","","","","","","19-20 Sept. 2016","","IEEE","IEEE Conference Publications"
"Hardware Trojans classification for gate-level netlists based on machine learning","K. Hasegawa; M. Oya; M. Yanagisawa; N. Togawa","Department of Computer Science and Communications Engineering, Waseda University","2016 IEEE 22nd International Symposium on On-Line Testing and Robust System Design (IOLTS)","20161024","2016","","","203","206","Recently, we face a serious risk that malicious third-party vendors can very easily insert hardware Trojans into their IC products but it is very difficult to analyze huge and complex ICs. In this paper, we propose a hardware-Trojan classification method to identify hardware-Trojan infected nets (or Trojan nets) using a support vector machine (SVM). Firstly, we extract the five hardware-Trojan features in each net in a netlist. Secondly, since we cannot effectively give the simple and fixed threshold values to them to detect hardware Trojans, we represent them to be a five-dimensional vector and learn them by using SVM. Finally, we can successfully classify a set of all the nets in an unknown netlist into Trojan ones and normal ones based on the learned SVM classifier. We have applied our SVM-based hardware-Trojan classification method to Trust-HUB benchmarks and the results demonstrate that our method can much increase the true positive rate compared to the existing state-of-the-art results in most of the cases. In some cases, our method can achieve the true positive rate of 100%, which shows that all the Trojan nets in a netlist are completely detected by our method.","","","10.1109/IOLTS.2016.7604700","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604700","gate-level netlist;hardware Trojan;machine learning;static detection;support vector machine (SVM)","Benchmark testing;Feature extraction;Hardware;Integrated circuits;Logic gates;Support vector machines;Trojan horses","invasive software;learning (artificial intelligence);pattern classification;support vector machines","IC products;SVM-based hardware-Trojan classification;five-dimensional vector;gate-level netlists;hardware Trojans classification;hardware-Trojan infected nets;learned SVM classifier;machine learning;malicious third-party vendors;support vector machine;trust-HUB benchmarks","","","","","","","4-6 July 2016","","IEEE","IEEE Conference Publications"
"A Behavioral Biometrics Based and Machine Learning Aided Framework for Academic Integrity in E-Assessment","A. Amigud; J. Arnedo-Moreno; T. Daradoumis; A. E. Guerrero-Roldan","Dept. of Comput. Sci., Multimedia & Telecommun., Univ. Oberta de Catalunya, Barcelona, Spain","2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)","20161027","2016","","","255","262","The common approaches to academic integrity in the e-learning environment are resource-intensive and require technology and/or dedicated invigilation staff to monitor assessment activities. These approaches are observational and often external to the learning spaces where the majority of the instructional content resides. Authentic assessments such as discussions, projects and portfolios may not always undergo the same scrutiny as high-stakes examinations and therefore differ in the level of identity and authorship assurance they provide. In this paper, we propose an integrated approach to enhancing academic integrity of e-assessments. The approach is based on behavioral biometrics and aided by machine-learning techniques. It provides continuous identity and authorship assurance throughout the learning activities within the existing learning space. It can be applied to measure the degree of learner collaboration with peers and interaction with the course content, concurrently verifying learner identity and validating authorship of the academic artifacts. We present the preliminary results and discuss future directions.","","","10.1109/INCoS.2016.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7695181","Academic Integrity;Behavioral Biometrics;E-Assessment;Machine Learning","Authentication;Biometrics (access control);Collaboration;Education;Feature extraction;Monitoring;Plagiarism","biometrics (access control);computer aided instruction;learning (artificial intelligence)","academic artifacts;academic integrity;behavioral biometrics;e-assessment;e-learning environment;electronic assessment;learner identity verification;learning activities;learning spaces;machine learning aided framework","","","","","","","7-9 Sept. 2016","","IEEE","IEEE Conference Publications"
"InfluenceRank: A machine learning approach to measure influence of Twitter users","A. Nargundkar; Y. S. Rao","Dept. of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, Maharashtra","2016 International Conference on Recent Trends in Information Technology (ICRTIT)","20160919","2016","","","1","6","We devise a system for measuring influence of Twitter users, which we call InfluenceRank, based on certain features extracted from their Twitter profiles and tweets authored over the duration of two months. As in the real world, influence of a user on social media may be judged by the engagement they drive through the content they publish. For a tweet, engagement can be most obviously measured by the number of retweets (RTs), favourites and replies it gets. Our system comprises of a regression based machine learning approach with InfluenceRank as the predictor variable against the set of our proposed features. The regression model has shown reasonable accuracy despite being fit on limited data.","","","10.1109/ICRTIT.2016.7569535","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569535","PageRank;Twitter;influence measurement;machine learning;social media influence;support vector regression","Data models;Feature extraction;Market research;Measurement;Predictive models;Twitter","feature extraction;learning (artificial intelligence);regression analysis;social networking (online)","InfluenceRank;Twitter profiles;feature extraction;regression-based machine learning;social media","","","","","","","8-9 April 2016","","IEEE","IEEE Conference Publications"
"An empirical semi-supervised machine learning approach on extracting and ranking document level multi-word product names using improved C-value approach","R. Sivashankari; B. Valarmathi","School of Information Technology and Engineering, VIT University, Vellore, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20161103","2016","","","770","775","In recent years, the volume of data submissions (E-Commerce data) in online on products, service, and organizations is increasing exponentially. This online data is abundantly unstructured; extracting knowledge from that huge volume of data is a non-trivial task. In recent years, extracting product names become a very popular approach and also one of the important methods in sentiment analysis. This product name extraction is very useful in E-commerce, because it helps in identifying people interest on products, generation of reviews' metadata and identification of product attributes, etc. The existing approaches in product name extraction are capable of extracting single word product names. However, the product names can be a sequence of words, which is also called multi-word product names that cannot be obtained automatically by the existing methods. In this paper, a combined approach of semi-supervised machine learning and improved C-value approach is proposed to discover the multi-word product names, ranking those product names and identifying a dominant product in review documents.","","","10.1109/ICACCI.2016.7732139","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732139","POS Tagging;Sentiment analysis;document-level mining;machine learning;text mining","Artificial neural networks;DVD;Data mining;Feature extraction;Informatics;Organizations;Tagging","document handling;information retrieval;learning (artificial intelligence);natural language processing","document level multiword product name extraction;document level multiword product name ranking;improved C-value approach;semisupervised machine learning approach","","","","","","","21-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"A 16-channel noise-shaping machine learning analog-digital interface","F. N. Buhler; A. E. Mendrela; Y. Lim; J. A. Fredenburg; M. P. Flynn","University of Michigan, Ann Arbor, USA","2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits)","20160922","2016","","","1","2","A 16-channel machine learning digitizing interface embeds Inner-Product calculation within a Delta-Sigma Modulator (IPDSM) array canceling quantization noise and noise shaping the multiplicand. The prototype, with 16 independent IPDSM channels occupies a core area of 0.95mm<sup>2</sup> in 65 nm CMOS. Each channel performs up to 100M multiplications/s. The system is demonstrated with a standard machine learning scheme for image recognition. It achieves the same classification accuracy for the MNIST set of hand-written digits as with the same algorithm on floating point DSP.","","","10.1109/VLSIC.2016.7573509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573509","","Bandwidth;Digital signal processing;Modulation;Noise shaping;Principal component analysis;Prototypes;Quantization (signal)","CMOS integrated circuits;analogue-digital conversion;delta-sigma modulation;image recognition;learning (artificial intelligence)","16-channel machine learning digitizing interface;CMOS;IPDSM array;MNIST set;delta-sigma modulator;hand-written digits;image recognition;inner-product calculation;noise shaping;quantization noise;size 65 nm","","","","","","","15-17 June 2016","","IEEE","IEEE Conference Publications"
"Predicting the sentiment of SaaS online reviews using supervised machine learning techniques","A. M. Alkalbani; A. M. Ghamry; F. K. Hussain; O. K. Hussain","Decision Support and e-Service Intelligence Lab, Center of Quantum Computation and Intelligent Systems, School of Software, University of Technology, Sydney, NSW 2007, Australia","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","1547","1553","There has been a dramatic increase in the sharing of opinions and information across different web platforms and social media, especially online product reviews. Cloud web portals, such as getApp.com, were designed to amalgamate cloud service information and to also examine how consumers evaluate their experience of using cloud computing products. The current literature shows the growing importance of online users' reviews, hence this study focuses on investigating consumers' feedback on Software-as-a- Service (SaaS) products by developing models to predict reviewers' attitudes. The goal of this paper is to develop prediction models to predict the sentiment of SaaS consumers' reviews (positive or negative). This research proposes five models that are based on five algorithms, the Support Vector Machine algorithm, Naive Bayes algorithm, Naive Bayes (Kernel) algorithm, k-nearest neighbors algorithm, and the decision tree algorithm to predict the attitude of SaaS reviews. The prediction accuracy of the space vector algorithm (5-fold cross-validation) is 92.37% which suggests that this algorithm is able to better determine the sentiment of online reviews compared with the other models. The results of this study provide valuable insight into online SaaS reviews and will assist in the design of SaaS review websites.","","","10.1109/IJCNN.2016.7727382","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727382","SaaS polarity dataset;SaaS reviews;classification model;supervised machine learning","Kernel;Machine learning algorithms;Prediction algorithms;Predictive models;Software as a service;Support vector machines;Training","Bayes methods;cloud computing;consumer behaviour;electronic commerce;learning (artificial intelligence);social networking (online);support vector machines","SaaS online reviews;Web sites;cloud Web portals;cloud computing;consumers reviews;k-nearest neighbors algorithm;naive Bayes algorithm;online product reviews;social media;software-as-a-service;supervised machine learning;support vector machine","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Teeth periapical lesion prediction using machine learning techniques","Y. E. Mahmoud; S. S. Labib; H. M. O. Mokhtar","Faculty of Computer Sciences, Modern Sciences and Arts University, Giza, Egypt","2016 SAI Computing Conference (SAI)","20160901","2016","","","129","134","Teeth Periapical lesion is used to be diagnosed by dentists according to patient's x-ray. But most of the time there were a problematic issue to reach a definitive diagnosis. It takes too much time, case and chief complaint history needed, many tests and tools are needed and sometimes taking too many radiographs is required. Even though, before starting the treatment sometimes reaching definitive diagnosis is difficult. Therefore, the objective of this research is to predict whether the patient has teeth periapical lesion or not and its type using machine learning techniques. The proposed system consists of four main steps: Data collection, image preprocessing using median and average filters for removing noise and Histogram equalization for image enhancement, feature extraction using two dimensional discrete wavelet transform algorithm, and finally machine learning (classification) using Feed Forward Neural Networks and K-Nearest Neighbor Classifier. It has been concluded from the results that the K-Nearest Neighbor Classifier performs better than Feed Forward Neural Network on our real database.","","Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2","10.1109/SAI.2016.7555972","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555972","Classification;Discrete Wavelet Transform;Histogram Equalization","Dentistry;Digital filters;Discrete wavelet transforms;Filtering algorithms;Histograms;Lesions;Teeth","dentistry;discrete wavelet transforms;feature extraction;feedforward neural nets;image enhancement;image filtering;learning (artificial intelligence);median filters;medical image processing","average filter;data collection;feature extraction;feedforward neural network;histogram equalization;image enhancement;image preprocessing;k-nearest neighbor classifier;machine learning;median filter;noise removal;teeth periapical lesion prediction;two dimensional discrete wavelet transform","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"Comparative analysis of machine learning algorithms in OCR","V. Jain; A. Dubey; A. Gupta; S. Sharma","Dept of IT, Bharati Vidyapeeth's College of Engineering, New Delhi, India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","1089","1092","The purpose of this research is to implement different machine learning algorithms in optical character recognition. The algorithms used the pixel density of image of handwritten digits as an input. The algorithms when implemented produced the value of labels of each handwritten digit. The value of labels generated, was then matched with the actual value of labels of the MNIST handwritten digits to determine the accuracy of an algorithm. Machine learning algorithms that have been used for this research are NaiÌˆve Bayes, NaiÌˆve Bayes with Laplace Smoothing, Sequential Minimal Optimization, C4.5 decision trees and Logistic Regression. The accuracy for each of the algorithm was calculated and Logistic regression was found out to be the most accurate of them all for handwritten digits.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724430","Logistic Regression;Machine Learning;NaÃ¯ve Bayes;Optical Character Recognition","Algorithm design and analysis;Classification algorithms;Decision trees;Logistics;Machine learning algorithms;Optical character recognition software;Training","Bayes methods;decision trees;handwritten character recognition;image matching;learning (artificial intelligence);optical character recognition;optimisation;regression analysis;smoothing methods","C4.5 decision tree algorithm;MNIST handwritten digits;OCR;image matching;image pixel density;logistic regression algorithm;machine learning algorithm;naiÌˆve Bayes-with-Laplace smoothing algorithm;optical character recognition;sequential minimal optimization algorithm","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Extreme Learning Machines for approximating nonlinear dimensionality reduction mappings: Application to Haptic handwritten signatures","J. J. ValdÃ©s; F. A. Alsulaiman; A. El Saddik","National Research Council Canada, Information and Communications Technologies, Ottawa, Ontario, Canada","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","2915","2922","The abundance of computing and mobile devices makes the problem of user identification and verification an essential requirement for many applications. Haptics devices include the sense of touch in the form of kinesthetic and tactile feedback which provide additional features within handwritten signatures. However, they generate high dimensional data and dimensionality reduction techniques become useful for data mining, machine learning and visualization. Nonlinear transformations have been used for this, but in present day scenarios (Big Data, the Internet of Things, massive data streams, etc.) the computation becomes more complex, time consuming or impractical. Moreover, the relationships between the features of the original and the target spaces are more difficult to uncover. Extreme Learning Machines (ELM) are used for approximating nonlinear manifold learning methods in two ways: as a functional representation for implicit methods, and as simpler surrogate models for explicit mapping techniques. In the context of Haptic handwritten signatures, five implicit and explicit nonlinear transformation methods are investigated. In all cases it was found that ELM approximations to the mappings obtained with the original methods exhibit very good behavior and can be used either as functional representations for the implicit methods or as simpler surrogate models for explicit techniques.","","","10.1109/IJCNN.2016.7727568","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727568","","Aerospace electronics;Data visualization;Electronic mail;Haptic interfaces;Manifolds;Mobile handsets;Neural networks","data mining;data reduction;formal verification;handwriting recognition;haptic interfaces;identification technology;learning (artificial intelligence)","ELM approximations;complex computation;data mining;dimensional data generation;dimensionality reduction techniques;explicit mapping techniques;extreme learning machines;haptic handwritten signatures;haptics devices;implicit method functional representation;kinesthetic feedback;machine learning;mobile devices;nonlinear dimensionality reduction mappings approximation;nonlinear manifold learning method approximation;nonlinear transformations;surrogate models;tactile feedback;user identification;user verification;visualization","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"A machine learning based approach for identifying traumatic brain injury patients for whom a head CT scan can be avoided","S. Molaei; F. K. Korley; S. M. R. Soroushmehr; H. Falk; H. Sair; K. Ward; K. NajarÃ­an","Emergency Medicine Department, University of Michigan, Ann Arbor, MI, USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2258","2261","Head CT scan is more often used to evaluate patients with suspected traumatic brain injury (TBI). However, the use of head CT scans in evaluating TBI is costly with low value endeavor. In this paper, we propose a new algorithm and a set of features to help clinicians determine which patients evaluated for TBI need a head CT scan using cost sensitive random forest (CSRF) classifier. We show that random forest (RF) and CSRF are useful methods for identifying patients likely to have a positive head CT scan. The proposed algorithm has superior diagnostic accuracy in comparison to the Canadian head CT algorithm, which is currently the most accurate and widely used algorithm for determining which TBI patients need a head CT scan. In the highest sensitivity (i.e. 100%), our method outperforms the Canadian rule in terms of specificity, accuracy and area under ROC curve using cost sensitive classifier. Clinical implementation of this algorithm can help decrease financial costs associated with Emergency Department evaluations for traumatic brain injury, while decreasing patient exposure to avoidable ionizing radiation.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591179","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591179","Canadian head CT rule;Classification;Head CT scan;Random forest;Traumatic brain injury","Brain injuries;Computed tomography;Head;Magnetic heads;Radio frequency;Sensitivity","brain;computerised tomography;image classification;injuries;learning (artificial intelligence);medical image processing","Canadian head CT algorithm;computed tomography;cost sensitive random forest classifier;diagnostic accuracy;head CT scan;ionizing radiation;machine learning-based approach;patient exposure;suspected traumatic brain injury","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Heterogeneous extreme learning machines","J. J. ValdÃ©s","National Research Council Canada, Information and Communications Technologies, Ottawa, Ontario, Canada","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","1678","1685","The developments in communication, sensor and computing technologies are generating information at increasing rates and the nature of the data is becoming highly heterogeneous. Accordingly, the objects under study are described by collections of variables of very different kinds (e.g. numeric, non-numeric, images, signals, videos, documents, etc.) with different degrees of imprecision and incompleteness. Many data mining and machine learning methods do not handle heterogeneity well, requiring variables of the same type, information completeness (or imputation), also assuming no imprecision. Extreme learning machines (ELM) are very interesting computational algorithms because of their structural simplicity, their good performance and their speed. Accordingly, extending their scope by making them capable of processing heterogeneous information may increase their attractiveness as a modeling tool for addressing complex problems. ELMs are discussed in the context of heterogeneous data and approaches to build ELMs capable of performing classification and regression tasks in such cases are presented. Their performance is illustrated with real world examples of classification and regression involving heterogeneous information with scalar data described by nominal, ordinal, interval, ratio, and fuzzy variables as well as with entire empirical probability distributions as predictor variables.","","","10.1109/IJCNN.2016.7727400","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727400","","Artificial neural networks;Manganese","data mining;feedforward neural nets;fuzzy set theory;learning (artificial intelligence);pattern classification;probability;regression analysis","classification tasks;data mining;fuzzy variables;heterogeneous ELM;heterogeneous data;heterogeneous extreme learning machines;imputation;information completeness;interval variables;machine learning;nominal variables;ordinal variables;predictor variables;probability distributions;ratio variables;regression tasks;scalar data","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Comparative analysis of features based machine learning approaches for phishing detection","A. K. Jain; B. B. Gupta","Department of Computer Engineering, National Institute of Technology, Kurukshetra, India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","2125","2130","Machine learning based anti-phishing techniques are based on various features extracted from different sources. These features differentiate a phishing website from a legitimate one. Features are taken from various sources like URL, page content, search engine, digital certificate, website traffic, etc, of a website to detect it as a phishing or non-phishing. The websites are declared as phishing sites if the heuristic design of the websites matches with the predefined rules. The accuracy of the anti-phishing solution depends on features set, training data and machine learning algorithm. This paper presents a comprehensive analysis of Phishing attacks, their exploitation, some of the recent machine learning based approaches for phishing detection and their comparative study. It provides a better understanding of the phishing problem, current solution space in machine learning domain, and scope of future research to deal with Phishing attacks efficiently using machine learning based approaches.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724641","Domain Name System;Machine learning;Phishing;Support vector machine;neural networks;soft computing","Data mining;Electronic mail;Feature extraction;Machine learning algorithms;Online banking;Support vector machines;Uniform resource locators","Web sites;computer crime;feature extraction;learning (artificial intelligence)","feature extraction;machine learning;phishing Web site;phishing attack;phishing detection","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach to edge type inference in Internet AS graphs","J. S. Varghese; Lu Ruan","Department of Computer Science, Iowa State University, United States of America","2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","20160908","2016","","","71","76","The Internet AS topology can be represented by AS graphs where nodes represent ASes and edges represent business relationships between ASes. AS relationship can be broadly classified into two types: provider-to-customer (p2c) and peer-to-peer (p2p). In this paper we present a machine learning approach to edge type inference in AS graphs. Given an AS graph derived from publicly available data source, we use the Gentle AdaBoost machine learning algorithm to train a classifier that classifies the edge types (p2c and p2p) based on a set of node features. We use our method to train classifiers for three AS graphs derived from different data sources-a BGP graph, a traceroute graph, and an IRR graph. The three classifiers achieve 93.97%-97.73% accuracy when validated against ground truth and achieve 81.76%-95.66% accuracy when validated against CAIDA's AS relationship inference dataset. We merge the three individual graphs to obtain a combined graph and propose a method to compute edge types in the combined graph. We analyze the characteristics of the three individual graphs and the combined graph and show that combining the three individual graphs gives us a significantly more complete view of both the p2p and p2c ecosystems in the Internet.","","Electronic:978-1-4673-9955-5; POD:978-1-4673-9956-2","10.1109/INFCOMW.2016.7562048","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562048","","Data mining;Internet;Machine learning algorithms;Peer-to-peer computing;Routing;Topology;Training","Internet;graph theory;inference mechanisms;learning (artificial intelligence);peer-to-peer computing;telecommunication network topology","AS relationship;BGP graph;IRR graph;Internet AS graphs;Internet AS topology;business relationships;edge type inference;gentle AdaBoost machine learning algorithm;machine learning approach;node features;peer-to-peer;provider-to-customer;publicly available data source;traceroute graph","","","","","","","10-14 April 2016","","IEEE","IEEE Conference Publications"
"LBP-HF features and machine learning applied for automated monitoring of insulators for overhead power distribution lines","P. S. Prasad; B. P. Rao","Department of ECE, MVGR College of Engineering, Vizianagaram, India","2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","20160915","2016","","","808","812","With ever-increasing awareness on quality and reliable power distribution, the research in the area of automation of distribution system has great relevance from the practical point of view. Electric power utilities throughout the world are more and more adopting computer aided control, monitoring and management of electric power distribution system to offer improved services to the consumers of electricity. The purpose of on-line condition monitoring of cables or any electrical equipment is to predict possible failures before they actually occur. With phenomenal growth of distribution network even to remote areas, the traditional methods of inspecting the lines by foot-patrolling and pole-climbing to check them in close proximity do not seem to be viable. Since the damaged insulators of the distribution system affects the performance of distribution system significantly in terms of reduction in voltage, aerial patrolling has been adopted in developed countries for the purpose of insulator monitoring. The development of an efficient and alternative method for insulator condition monitoring uses image processing and machine learning techniques and is found to be a sustainable method. This work covers automatic defect detection and classification of insulator systems of electric power lines using vision-based techniques.","","","10.1109/WiSPNET.2016.7566245","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566245","Classification;LBP-HF;feature extraction;rotation invariance","Condition monitoring;Discrete Fourier transforms;Feature extraction;Histograms;Insulators;Monitoring;Support vector machines","insulators;learning (artificial intelligence);power engineering computing;power overhead lines;power system measurement","LBP-HF features;automatic defect detection;electric power lines;insulator automated monitoring;insulator system classification;machine learning;overhead power distribution lines;vision-based techniques","","","","","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"Airline Passenger Profiling Based on Fuzzy Deep Machine Learning","Y. J. Zheng; W. G. Sheng; X. M. Sun; S. Y. Chen","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou 310023, China.","IEEE Transactions on Neural Networks and Learning Systems","","2016","PP","99","1","13","Passenger profiling plays a vital part of commercial aviation security, but classical methods become very inefficient in handling the rapidly increasing amounts of electronic records. This paper proposes a deep learning approach to passenger profiling. The center of our approach is a Pythagorean fuzzy deep Boltzmann machine (PFDBM), whose parameters are expressed by Pythagorean fuzzy numbers such that each neuron can learn how a feature affects the production of the correct output from both the positive and negative sides. We propose a hybrid algorithm combining a gradient-based method and an evolutionary algorithm for training the PFDBM. Based on the novel learning model, we develop a deep neural network (DNN) for classifying normal passengers and potential attackers, and further develop an integrated DNN for identifying group attackers whose individual features are insufficient to reveal the abnormality. Experiments on data sets from Air China show that our approach provides much higher learning ability and classification accuracy than existing profilers. It is expected that the fuzzy deep learning approach can be adapted for a variety of complex pattern analysis tasks.","2162-237X;2162237X","","10.1109/TNNLS.2016.2609437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577870","Biogeography-based optimization (BBO);Pythagorean fuzzy set (PFS).;deep Boltzmann machine (DBM);deep learning;evolutionary neural networks;passenger profiling","Airports;Atmospheric modeling;Fuzzy sets;Inspection;Machine learning;Terrorism","","","","","","","","20160927","","","IEEE","IEEE Early Access Articles"
"Ultra-Lightweight Malware Detection of Android Using 2-Level Machine Learning","L. Ma; Y. Yang; X. Wang; J. He","Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China","2016 3rd International Conference on Information Science and Control Engineering (ICISCE)","20161103","2016","","","729","733","As Android becoming the most popular smart phone operating system, malicious applications running on the Android platform appears very frequently and poses the major threat to the security of Android. Considering the resources of smart phone are severely limited, a stable, simple and quick malware detection method for Android is indispensable. In this paper, we propose an ultra-lightweight malware detection method which is able to detect unknown malicious Android applications with limited resources. Firstly, a few features are extracted and divided into three sets for every application. Then, these three feature sets are embedded in the corresponding joint vector spaces and we can get apps's feature vectors. After that, feature vectors of every vector space are classified using a machine learning algorithm. Finally, the three classification results are considered as a group and embedded in a new space and classified again. We evaluate our detection with 3427 malicious samples and 1550 benign applications. Experimental results show that our detection approach has a stable performance that the detection accuracy (true-positive rate) is always higher than 98% and the detection procedure costs only 30ms per sample.","","","10.1109/ICISCE.2016.161","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726258","2-levelmachine learning;Android;SVM;smart-phone malware detection;ultra-lightweight","Androids;Feature extraction;Humanoid robots;Machine learning algorithms;Malware;Smart phones;Support vector machines","Android (operating system);invasive software;learning (artificial intelligence);smart phones;vectors","2-level machine learning;feature vectors;joint vector spaces;malicious Android applications;security;smart phone operating system;ultra-lightweight malware detection","","","","","","","8-10 July 2016","","IEEE","IEEE Conference Publications"
"Comparison of machine learning algorithms for classification of Penaeid prawn species","V. Sucharita; S. Jyothi; P. V. Rao","K.L. University, Vaddeswaram, India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","1610","1613","Classification of Penaeid prawn species is an important research problem in the area of aquaculture. In literature, many ML algorithms have been proposed for classification. In this paper, performance and usability of penaied prawn species are compared using K-Nearest Neighbourhood (KNN) algorithm, Support Vector Machines (SVM) and Back Propagation Neural Networks (BPNN). For experimental evaluation a dataset containing 100 samples of each species are classified. Also the classification accuracy of each species is analyzed using the above mentioned three classifiers. Experimental results indicate that the SVM out performs KNN classifier and ANN classifiers and may potentially fill gap for the current use or for future.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724539","BPNN;Classifier;KNN;Neural Network;Penaeid Prawns;SVM","Decision support systems;Handheld computers;Support vector machines","aquaculture;backpropagation;learning (artificial intelligence);neural nets;pattern classification;support vector machines","BPNN;K-nearest neighbourhood algorithm;KNN algorithm;KNN classifier;ML algorithms;SVM;aquaculture;back propagation neural networks;machine learning algorithms;penaeid prawn species classification;support vector machines","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Machine Learning In Incremental Sheet Forming","D. D. Stoerkle; P. Seim; L. Thyssen; B. Kuhlenkoetter","","Proceedings of ISR 2016: 47st International Symposium on Robotics","20160905","2016","","","1","7","Within this article, the authors propose a new methodology to increase the geometric accuracy in the robot-based, incremental sheet forming process ROBOFORMING. This process addresses the production of sheet metal components in small lot sizes and prototypes. In ROBOFORMING, two cooperating industrial robots are applied for the kinematic-based forming of sheet metal workpieces. Hereby, workpiece-dependent tooling and dies are omitted. This offers very high flexibility for the geometrical design of the sheet metal workpieces. One of the major drawbacks of incremental sheet forming processes is the low geometrical accuracy, which limits the widespread industrial application of these. Responding to these constraints, the authors propose the application of machine learning techniques to increase the geometric accuracy in incremental sheet forming processes. In this context, they present a learning approach which applies reinforcement learning as a flexible and promising solution.","","Paper:978-3-8007-4231-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559184","","","","","","","","","","","21-22 June 2016","","VDE","VDE Conference Publications"
"Modeling the condition of lithium ion batteries using the extreme learning machine","A. Densmore; M. Hanif","Department of Electrical Engineering, University of Cape Town, Cape Town, South Africa","2016 IEEE PES PowerAfrica","20160901","2016","","","184","188","Recent years have seen increased interest in the use of off-grid solutions for electrification of rural areas. Off-grid electrification (such as solar home systems and micro-grids) are particularly applicable to the rural African context, where little infrastructure exists and in many regions grid extension is prohibitively expensive. To be economically viable, these systems must maximize the power delivered while ensuring the health of energy storage devices. Batteries in particular are a key weakness and typically the first major component to fail. In this paper we present an improved and simplified method for simulating the state of charge (SoC) and state of health (SoH) of lithium-ion batteries. SoC and SoH are predicted using the Extreme Learning Machine (ELM) algorithm. ELM is a state of the art single layer, feed-forward neural network that is characterized by its good generalized performance and fast learning speed. Real-life battery data from the NASA-AMES dataset provides the benchmark for evaluation of the ELM model.","","Electronic:978-1-4673-9981-4; POD:978-1-4673-9982-1","10.1109/PowerAfrica.2016.7556597","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556597","lithium-ion;machine learning;simulation;state of charge;state of health","Africa;Conferences;Hafnium","feedforward neural nets;learning (artificial intelligence);lithium compounds;power engineering computing;secondary cells","ELM algorithm;NASA-AMES dataset;SoC prediction;SoH prediction;energy storage device health;extreme learning machine;feedforward neural network;grid extension;lithium ion battery condition model;off-grid electrification;rural area electrification;state of charge prediction;state of health prediction","","","","","","","June 28 2016-July 3 2016","","IEEE","IEEE Conference Publications"
"Comparison of Different Machine Learning Approaches to Predict Small for Gestational Age Infants","J. Li; L. Liu; J. Sun; H. Mo; J. J. Yang; S. Chen; H. Liu; Q. Wang; H. Pan","","IEEE Transactions on Big Data","","2016","PP","99","1","1","Diagnosing infants who are small for gestational age (SGA) at early stages could help physicians to introduce interventions for SGA infants earlier. Machine learning (ML) is envisioned as a tool to identify SGA infants. However, ML has not been widely studied in this field. To develop effective SGA prediction models, we conducted four groups of experiments that considered basic ML methods, imbalanced data, feature selection and the time characteristics of variables, respectively. Infants with SGA data collected from 2010 to 2013 with gestational weeks between 24 and 42 were detected. Support vector machine (SVM), random forest (RF), logistic regression (LR) and Sparse LR models were trained on 10-fold cross validation. Precision and the area under the curve (AUC) of the receiver operator characteristic curve were evaluated. For each group, the performance of SVM and Sparse LR was similarly well. LR without any sparsity penalties performed worst, possibly caused by the overfitting problem. With the combination of handling imbalanced data and feature selection, the RF ensemble classifier performed best, which even obtained the highest AUC value (0.8547) with the help of expert knowledge. In other cases, RF performed worse than Sparse LR and SVM, possibly because of fully grown trees.","","","10.1109/TBDATA.2016.2620981","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725951","feature selection;machine learning;prediction model;small for gestational age","Big data;Pediatrics;Predictive models;Radio frequency;Support vector machines;Training;Vegetation","","","","","","","","20161031","","","IEEE","IEEE Early Access Articles"
"Spectral anomaly detection with machine learning for wilderness search and rescue","J. Proft; J. Suarez; R. Murphy","Connecticut College New London, CT","2015 IEEE MIT Undergraduate Research Technology Conference (URTC)","20160912","2015","","","1","3","In wilderness search and rescue missions, unmanned aerial vehicles (UAVs) may be deployed to collect high-resolution imagery which is later reviewed by a first responder. The volume of images and the altitude from which they are taken makes manually identifying potential items of interest, like clothing or other man-made material, a difficult task. For this reason, we created a program that automatically detects unusually-colored objects in aerial imagery in order to assist responders in locating signs of missing persons. The program uses the Reed-Xiaoli (RX) spectral anomaly detection algorithm to determine which pixels in an image are anomalous and then generates an ""anomaly map"" where brighter pixels signify greater abnormality. While the RX algorithm has previously been proposed for search and rescue missions, up until now it has not been evaluated in a high-fidelity setting with real responders and real equipment. We tested the program on 150 aerial images taken over the Blanco River area in Hays County, Texas after the May 2015 flooding and demonstrated the results at a workshop on flooding hosted by Texas A&M's Center for Emergency Informatics. Early feedback from responders suggests that RX spectral anomaly detection is a valuable tool for quickly locating atypically-colored objects in images taken with UAVs for wilderness search and rescue.","","","10.1109/URTC.2015.7563746","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563746","anomaly detection;emergency informatics;machine learning;search and rescue;unmanned aerial vehicles","Detectors;Floods;Hyperspectral imaging;Image color analysis;Search problems;Unmanned aerial vehicles","autonomous aerial vehicles;emergency services;image colour analysis;learning (artificial intelligence);object detection","Blanco River area;Hays County;RX spectral anomaly detection algorithm;Reed-Xiaoli spectral anomaly detection algorithm;Texas;UAVs;anomaly map;high-resolution imagery collection;machine learning;unmanned aerial vehicles;unusually-colored object detection;wilderness search and rescue mission","","","","","","","7-8 Nov. 2015","","IEEE","IEEE Conference Publications"
"Research on Crime Degree of Internet Speech Based on Machine Learning and Dictionary","Y. Hu; S. Wang","Sch. of Commun. Eng., Jilin Univ., Jilin, China","2016 3rd International Conference on Information Science and Control Engineering (ICISCE)","20161103","2016","","","532","537","Intelligent security technology provides important clues and basis for case detection, however, the traditional intelligent security technology can not alarm a crime's happening in advance. By researching the relationship between criminal psychology and speech feature, we proposed a crime degree theory of internet speech which can alarm a crime's happening in advance by taking advantage of the internet speech. The theory which was based on criminal psychology, using multiple analytical methods such as machine learning and emotional dictionary, establishing the mathematical model and theoretical framework, gave the preliminary implementation method of the real system. Experimental result has shown that the pre-alarming system based on our theory has good pre-alarming capability against crime.","","","10.1109/ICISCE.2016.120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726217","crime degree;crime pre-alarming;dictionary;machine learing","Bayes methods;Dictionaries;Internet;Psychology;Semantics;Speech;Speech recognition","Internet;computer crime;dictionaries;learning (artificial intelligence);psychology;speech processing","Internet speech crime degree theory;case detection;criminal psychology;emotional dictionary;intelligent security technology;machine learning;mathematical model;multiple analytical methods;pre-alarming system","","","","","","","8-10 July 2016","","IEEE","IEEE Conference Publications"
"Forecasting Obsolescence Risk and Product Life Cycle With Machine Learning","C. Jennings; D. Wu; J. Terpenny","Department of Industrial and Manufacturing Engineering, The Pennsylvania State University, State College, PA, USA","IEEE Transactions on Components, Packaging and Manufacturing Technology","20160920","2016","6","9","1428","1439","Rapid changes in technology have led to an increasingly fast pace of product introductions. For long-life systems (e.g., planes, ships, and nuclear power plants), rapid changes help sustain useful life, but at the same time, present significant challenges associated with obsolescence management. Over the years, many approaches for forecasting obsolescence risk and product life cycle have been developed. However, gathering inputs required for forecasting is often subjective and laborious, causing inconsistencies in predictions. To address these issues, the objective of this research is to develop a machine learning-based methodology capable of forecasting obsolescence risk and product life cycle accurately while minimizing maintenance and upkeep of the forecasting system. Specifically, this new methodology enables prediction of both the obsolescence risk level and the date when a part becomes obsolete. A case study of the cell phone market is presented to demonstrate the effectiveness and efficiency of the new approach. Results have shown that machine learning algorithms (i.e., random forest, artificial neural networks, and support vector machines) can classify parts as active or obsolete with over 98% accuracy and predict obsolescence dates within a few months.","2156-3950;21563950","","10.1109/TCPMT.2016.2589206","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7543522","Diminishing manufacturing sources and material shortages;electronic parts;life cycle stages;machine learning;obsolescence;sustainment","Aging;Data models;Forecasting;Industries;Manufacturing;Market research;Predictive models","cellular radio;learning (artificial intelligence);maintenance engineering;product life cycle management;risk management","cell phone market;machine learning;maintenance minimization;obsolescence management;obsolescence risk forecasting;product life cycle forecasting","","","","","","20160815","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"A comprehensive survey for sentiment analysis tasks using machine learning techniques","E. AydoÄŸan; M. A. Akcayol","Department of Computer Engineering, Gazi University, Ankara, Turkey","2016 International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)","20160922","2016","","","1","7","Sentiment analysis is one of the most popular research topics in last years. There are lots of data on web which require analysis in order for them to become useful. Many researchers have focused on making sense of these data. Therefore, sentiment analysis concept is proposed. Sentiment analysis methods try to emerge any opinions, feelings, and subjectivity behind the text. Machine learning algorithms and vocabulary based methods are used to perform sentiment analysis. In this research, (i) recently studied researches on machine learning based sentiment analysis are investigated to give background; (ii) they are classified according to their tasks on extracting information; (iii) the encountered and potential challenges on this research topic are revisited and discussed.","","","10.1109/INISTA.2016.7571856","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571856","machine learning;sentiment analysis;sentiment classification","Algorithm design and analysis;Classification algorithms;Machine learning algorithms;Niobium;Sentiment analysis;Support vector machines;Unsupervised learning","Internet;learning (artificial intelligence);sentiment analysis;vocabulary","World Wide Web;machine learning;sentiment analysis;vocabulary based method","","","","","","","2-5 Aug. 2016","","IEEE","IEEE Conference Publications"
"A machine-learning classifier implemented in a standard 6T SRAM array","Jintao Zhang; Zhuo Wang; N. Verma","Princeton University, NJ, USA","2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits)","20160922","2016","","","1","2","This paper presents a machine-learning classifier where the computation is performed within a standard 6T SRAM array. This eliminates explicit memory operations, which otherwise pose energy/performance bottlenecks, especially for emerging algorithms (e.g., from machine learning) that result in high ratio of memory accesses. We present an algorithm and prototype IC (in 130nm CMOS), where a 128Ã—128 SRAM array performs storage of classifier models and complete classifier computations. We demonstrate a real application, namely digit recognition from MNIST-database images. The accuracy is equal to a conventional (ideal) digital/SRAM system, yet with 113Ã— lower energy. The approach achieves accuracy >95% with a full feature set (i.e., 28Ã—28=784 image pixels), and 90% when reduced to 82 features (as demonstrated on the IC due to area limitations). The energy per 10-way digit classification is 633pJ at a speed of 50MHz.","","","10.1109/VLSIC.2016.7573556","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573556","","Boosting;Classification algorithms;Discharges (electric);Prototypes;Random access memory;Standards;Training","CMOS memory circuits;SRAM chips;learning (artificial intelligence);pattern classification","MNIST-database images;digit recognition;explicit memory operations;machine-learning classifier;prototype IC;size 130 nm;standard 6T SRAM array","","","","","","","15-17 June 2016","","IEEE","IEEE Conference Publications"
"Design of a diamond adsorption detection system based on machine learning techniques","Z. Fan; Y. Zuo; F. Li; S. Wang","Guangdong Provincial Key Laboratory of Digital Signal and Image Processing and the Department of Electronic Engineering, Shantou University, Shantou, 515063 CHN","2016 12th World Congress on Intelligent Control and Automation (WCICA)","20160929","2016","","","3124","3128","A diamond adsorption detecting system based on machine learning is presented in this paper. The paper describes the system from the perspective of hardware and software design, and presents the image processing and machine learning algorithms applied in the system. The hardware includes three major parts - the camera, light source and support platform. The software includes modules of image acquisition, image preprocessing, feature extraction, and machine learning. This paper utilizes three supervised machine learning algorithms, namely Support Vector Machine (SVM), Classification and Regression Tree (CART) and C4.5 decisions. Through the comparison study of the three algorithms, SVM is found to have the best performance for this system. It is demonstrated in experimental tests that the algorithm can obtain an accuracy of 97.84%, which improves the detection efficacy of the system significantly.","","","10.1109/WCICA.2016.7578715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578715","Decision tree;SVM;machine learning","Adsorption;Classification algorithms;Decision trees;Diamond;Machine learning algorithms;Support vector machines;Training","feature extraction;image classification;learning (artificial intelligence);regression analysis;support vector machines;trees (mathematics)","C4.5 decisions;CART;SVM;camera;classification and regression tree;diamond adsorption detection system;feature extraction;hardware design;image acquisition;image processing;light source;software design;supervised machine learning algorithms;support platform;support vector machine","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Dermatological disease detection using image processing and machine learning","V. B. Kumar; S. S. Kumar; V. Saboo","Computer Science Department PES Institute of Technology","2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR)","20161013","2016","","","1","6","Dermatological diseases are the most prevalent diseases worldwide. Despite being common, its diagnosis is extremely difficult and requires extensive experience in the domain. In this research paper, we provide an approach to detect various kinds of these diseases. We use a dual stage approach which effectively combines Computer Vision and Machine Learning on clinically evaluated histopathological attributes to accurately identify the disease. In the first stage, the image of the skin disease is subject to various kinds of pre-processing techniques followed by feature extraction. The second stage involves the use of Machine learning algorithms to identify diseases based on the histopathological attributes observed on analysing of the skin. Upon training and testing for the six diseases, the system produced an accuracy of up to 95 percent.","","","10.1109/ICAIPR.2016.7585217","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585217","Automated Disease Diagnosis;Computational Intelligence;Computer Vision;Data Mining;Dermatology;Image Processing;Machine Learning","Artificial neural networks;Computational modeling;Computer vision;Diseases;Feature extraction;Image color analysis;Skin","computer vision;diseases;feature extraction;learning (artificial intelligence);medical image processing;skin","computer vision;dermatological disease detection;dual stage approach;feature extraction;histopathological attribute evaluation;image processing;machine learning;skin analysis","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Preliminary application of machine-learning techniques for thermal-electrical parameter optimization in 3-D IC","S. J. Park; H. Yu; M. Swaminathan","Center for Co-Design of Chip, Package, System (C3PS), Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332","2016 IEEE International Symposium on Electromagnetic Compatibility (EMC)","20160922","2016","","","402","405","Three-dimensional (3-D) integration technique, a promising integration technique, can increase system density but at the cost of increased thermal and power density, leading to thermal-related problems. Design of three-dimensional integrated circuits and systems requires considerations of temperature and gradients observed across the die, because temperature gradients can vary the delay of clock paths. As we need to analyze a large number of parameters for thermal-electrical design, optimization of those parameters becomes important for achieving efficiency and accuracy. Machine learning methods have been applied in the past for artificial intelligence, data analysis, and for general optimization problems. In this paper we propose the application of machine learning methods for parameter optimization in 3-D systems.","","","10.1109/ISEMC.2016.7571681","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571681","3-D IC;Bayesian optimization. Introduction;Machine learning;TSV (Throung Silicon Via);Temperature gradient;Thermal-induced skew","Decision support systems","data analysis;learning (artificial intelligence);optimisation;three-dimensional integrated circuits","3D integrated circuit;artificial intelligence;clock path delay;data analysis;machine learning;power density;temperature gradients;thermal density;thermal-electrical parameter optimization;thermal-related problems;three-dimensional integrated circuits;three-dimensional integration","","","","","","","25-29 July 2016","","IEEE","IEEE Conference Publications"
"IoT and distributed machine learning powered optimal state recommender solution","M. Sewak; S. Singh","Advanced Analytics Division, BNY Mellon Innovation Center, Pune, India - 411 028","2016 International Conference on Internet of Things and Applications (IOTA)","20160908","2016","","","101","106","Recommender systems add significant benefits to E-commerce in terms of sale conversion, revenues, customer experience, loyalty and lifetime value. But the recommendations from these systems do not change on inputs beyond user and item profile and transaction data. There have been some attempts in the past to optimize on more varied data in recommenders, example of which is the location based recommenders. But location is just one dimension of the state that a user could have shared with GPS/GLONASS/BaiDeu sensor available in most Smartphones. With an upcoming era of Smart-wears and pervasive IoTs, there are a lot many other dimensions of a user state which can be utilized to optimize upon the concept of Optimal State Recommender Solutions. This paper suggests upgrading from conventional recommendations that are based on user/ item preferences alone with systems that provide the best recommendation at the most optimal state when the user is most receptive to accept the recommendation, the â€œoptimal state recommendation solutionâ€ and proposes solutions and architectures to overcome the challenges of dealing with real time, distributed machine learning on IoT scale data in implementing this solution. The paper leverages some of the advance distributed machine learning algorithms like variants of Distributed Kalman Filters, Distributed Alternating Least Square Recommenders, Distributed Mini-Batch Stochastic Gradient Descent(SGD) based Classifiers, and highly scalable distributed computation and machine learning platforms like Apache Spark, (Apache) Spark MLlib, Spark Streaming, Python/PySpark, R/SparkR, Apache Kafka in an high performance, distributed, fault tolerant architecture. The solution also aspires to be compliant with upcoming IoT standards and architectures like IEEE P2413 to provide a standard solution for such problems beyond the current scope of this paper.","","","10.1109/IOTA.2016.7562703","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562703","Apache Kafka;Apache Spark;Distributed ALS Recommender;Distributed Kalman Filter;Distributed Machine Learning;IEEE P2413;IoT;MLlib;Mini Batch SGD;PySpark;Recommender System;Sensors;Smartwears;SparkR","Band-pass filters;Distributed databases;Internet of things;Kalman filters;Machine learning algorithms;Real-time systems;Sparks","Internet of Things;electronic commerce;learning (artificial intelligence);recommender systems","Apache Kafka;Apache Spark MLlib;GPS/GLONASS/BaiDeu sensor;IEEE P2413;IoT scale data;IoT standards;Python/PySpark;R/SparkR;SGD based classifiers;Spark Streaming;customer experience;distributed Kalman filters;distributed alternating least square recommenders;distributed machine learning algorithms;distributed mini-batch stochastic gradient descent;e-commerce;fault tolerant architecture;item preferences;item profile;location based recommenders;machine learning platforms;optimal state recommender solution;pervasive IoT;recommender systems;revenues;sale conversion;smart phones;smart wears;transaction data;user preferences","","","","","","","22-24 Jan. 2016","","IEEE","IEEE Conference Publications"
"A wearable computing platform for developing cloud-based machine learning models for health monitoring applications","S. Patel; R. S. McGinnis; I. Silva; S. DiCristofaro; N. Mahadevan; E. Jortberg; J. Franco; A. Martin; J. Lust; M. Raj; B. McGrane; P. DePetrillo; A. J. Aranyosi; M. Ceruolo; J. Pindado; R. Ghaffari","MC10, Inc, Lexington, MA USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","5997","6001","Wearable sensors have the potential to enable clinical-grade ambulatory health monitoring outside the clinic. Technological advances have enabled development of devices that can measure vital signs with great precision and significant progress has been made towards extracting clinically meaningful information from these devices in research studies. However, translating measurement accuracies achieved in the controlled settings such as the lab and clinic to unconstrained environments such as the home remains a challenge. In this paper, we present a novel wearable computing platform for unobtrusive collection of labeled datasets and a new paradigm for continuous development, deployment and evaluation of machine learning models to ensure robust model performance as we transition from the lab to home. Using this system, we train activity classification models across two studies and track changes in model performance as we go from constrained to unconstrained settings.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7592095","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592095","","Biological system modeling;Data models;Feature extraction;Legged locomotion;Pipelines;Testing;Training","biomedical telemetry;cloud computing;learning (artificial intelligence);medical computing;patient monitoring;wireless sensor networks","activity classification model;clinical-grade ambulatory health monitoring;cloud-based machine learning model;health monitoring application;vital sign measurement;wearable computing platform;wearable sensor","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Symbolic execution of complex program driven by machine learning based constraint solving","X. Li; Y. Liang; H. Qian; Y. Q. Hu; L. Bu; Y. Yu; X. Chen; X. Li","State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","20161006","2016","","","554","559","Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582790","Complicated Path Condition;Constraint Solving;Machine Learning;Symbolic Execution","Algorithm design and analysis;Engines;Java;Libraries;Machine learning algorithms;Optimization;Transforms","arithmetic;constraint handling;learning (artificial intelligence);optimisation;program diagnostics","MLB;black-box function calls;complex path conditions;complex program;constraint solvers;constraint solving;dissatisfaction degree;library methods;linear path conditions;machine learning;nonlinear arithmetic operations;nonlinear constraints;optimization problems;optimization solver;program analysis;symbolic PathFinder;symbolic execution tool;symbolic path conditions","","","","","","","3-7 Sept. 2016","","IEEE","IEEE Conference Publications"
"Improvements of Classification Accuracy of Film Defects by Using GPU-accelerated Image Processing and Machine Learning Frameworks","H. Ando; Y. Niitsu; M. Hirasawa; H. Teduka; M. Yajima","Dept. of Comput. Sci. & Eng., Univ. of Yamanashi, Kofu, Japan","2016 Nicograph International (NicoInt)","20160912","2016","","","83","87","Research on image classification for natural images are quite actively worked on and recent achievements using deep learning techniques are tremendous. On the other hand, image classification techniques of defects in industrial products are mostly kept secret, partly because defective images contain very sensitive information about the products and the confidential manufacturing technologies. With the help of a leading company in a visual inspection of film defects, we investigated the effectivity of using machine learning techniques for classification of defect images. We also made use of GPU to accelerate both image processing to assist detection of defects and machine learning. We propose the combination of deep neural networks with random forest classifier for image classification of film defects, which performed better than using either of the two techniques alone.","","","10.1109/NicoInt.2016.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564050","Deep Learning;Film Defects;GPU;Image Classification;Random Forest","Feature extraction;Films;Graphics processing units;Image classification;Inspection;Machine learning;Visualization","films;graphics processing units;image classification;inspection;learning (artificial intelligence);neural nets;product quality;production engineering computing","GPU;deep learning technique;deep neural network;film defect classification;graphics processing unit;image processing;industrial product;machine learning;manufacturing technology;natural image classification;random forest classifier;visual inspection","","","","","","","6-8 July 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques to build geometrical transformations for object matching a review","P. A. Jadhav; P. N. Chatur","Department of Computer science and Engineering, Government College of Engineering, Amravati (MH), India","2016 3rd International Conference on Advanced Computing and Communication Systems (ICACCS)","20161010","2016","01","","1","6","Image matching or object matching is one of the cutting edge research fields in machine learning or computer vision domain. Whereas aim of image matching techniques is to build geometrical transformations over source image and target image, videos, real time moving object to extract similarity measure. Several research methods devised for image matching but efficiency of techniques is bounded with various parameters such as image rotation, speed, blurriness, quality etc., these parameters are important while understanding and devising robust image matching techniques. Study and analysis of image matching parameters is highly important while learning and understanding, predicting performance when time is a limiting factor for implementation. Several approaches have been presented to achieve efficiency over real time object matching. Now in this paper we have presented fundamentals of object matching based on geometrical transformation to match object. Comprehensive review of existing methods with analysis of image matching parameters is presented to determine the limitations of existing methods. This review also addresses comparative study of existing image matching techniques to generalize criteria for design of robust technique.","","","10.1109/ICACCS.2016.7586381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586381","Computer Vision;Geometrical Transformations;Image;Machine Learning;Object Matching;Rotation","Communication systems;Computer vision;Feature extraction;Image color analysis;Image edge detection;Image matching;Image registration","computer vision;geometry;image matching;image motion analysis;learning (artificial intelligence);object recognition;real-time systems","computer vision;geometrical transformations;image matching;machine learning;object matching;real time moving object","","","","","","","22-23 Jan. 2016","","IEEE","IEEE Conference Publications"
"Combining Crop Proportion Phenology Index models with machine learning algorithms for estimating winter wheat areas","L. Li; Y. Z. Pan; Q. C. Xin","College of Life Science, Beijing Normal University, 100875, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","7137","7140","Monitoring crop areas is a key issue in remote sensing studies. A Crop Proportion Phenology Index (CPPI) model has previously been developed for estimation of winter wheat areas. Here we test the CPPI model in different areas using remote sensing data for varied kernel functions, including linear regression (LR), Artificial Neural Network (ANN), and Support Vector Regression (SVR). The differences of the model performances among different kernel functions were found to be small for areas with simple planting structure. For areas where multiple crop types have similar phenology cycles, the non-linear model of ANN was found to perform the best. This study indicates that the CPPI model can be applied to map winter wheat distribution in areas with complex planting structures, thus it holds promises for estimating fractional areas of winter wheat areas over large geographic areas.","","","10.1109/IGARSS.2016.7730862","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730862","CPPI;MODIS;fractional crop area;kernel functions;time series","Agriculture;Artificial neural networks;Kernel;MODIS;Machine learning algorithms;Remote sensing;Time series analysis","agricultural engineering;crops;neural nets;regression analysis;support vector machines;vegetation mapping","ANN;CPPI;CPPI model;SVR;artificial neural network;complex planting structures;crop proportion phenology index models;kernel functions;large geographic areas;linear regression;machine learning algorithms;multiple crop types;nonlinear model;phenology cycles;planting structure;remote sensing data;support vector regression;winter wheat area estimation","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"A study of 4D trajectory prediction based on machine deep learning","X. Guan; R. Lv; L. Sun; Y. Liu","Civil Aviation Management Institute of China, Beijing, 100121, China","2016 12th World Congress on Intelligent Control and Automation (WCICA)","20160929","2016","","","24","27","Trajectory prediction is the core module of modern air traffic management system. Focused on the trajectory prediction model and key algorithm of the system, this paper tries to overcome the disadvantages of the traditional linear prediction method via employing machine learning technology in the problem. The statistical historical flight data of different aircraft type is taken as the training sample to build the predictive model. Besides, current state information is used as the input data of the model and to get the predictive flight trajectory information. Then machine learning technology is adopted to improve the efficiency. The result shows that the proposed method has obviously higher performance compared to the traditional predictive methods.","","","10.1109/WCICA.2016.7578458","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578458","","Air traffic control;Aircraft;Aircraft propulsion;Atmospheric modeling;Data models;Predictive models;Trajectory","air traffic;aircraft;learning (artificial intelligence);statistical analysis","4D trajectory prediction;air traffic management system;aircraft;linear prediction method;machine deep learning;predictive model;statistical historical flight data","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Detection and classification of diseases of Grape plant using opposite colour Local Binary Pattern feature and machine learning for automated Decision Support System","H. Waghmare; R. Kokare; Y. Dandawate","Dept. of Electronics and Telecommunication Engineering, Vishwakarma Institute of Information Technology, Pune, India","2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN)","20160915","2016","","","513","518","Plant diseases cause major economic and production losses as well as curtailment in both quantity and quality of agricultural production. Now a day's, for supervising large field of crops there is been increased demand for plant leaf disease detection system. The critical issue here is to monitor the health of the plants and detection of the respective diseases. Studies show that most of the plant disease can be diagnosed from the properties of the leaf. Thus leaf based disease analysis for plants is an exciting new domain. The technique proposed for identification of plant disease through the leaf texture analysis and pattern recognition. In this work we focus on Grapes plant leaf disease detection system. The system takes a single leaf of a plant as an input and segmentation is performed after background removal. The segmented leaf image is then analyzed through high pass filter to detect the diseased part of the leaf. The segmented leaf texture is retrieved using unique fractal based texture feature. Fractal based features are locally invariant in nature and therefore provides a good texture model. The texture of every independent disease will be different. The extracted texture pattern is then classified using multiclass SVM. The work classifies focus on major diseases commonly observed in Grapes plant which are downy mildew & black rot. The proposed approach avails advice of agricultural experts easily to farmers with the accuracy of 96.6%.","","","10.1109/SPIN.2016.7566749","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566749","Decision Support System (DSS);HSV (Hue Saturation Value);OC-LBP;Plant disease;Support Vector Machine (SVM);Texture analysis","Agriculture;Decision support systems;Diseases;Feature extraction;Image color analysis;Pipelines;Support vector machines","agriculture;decision support systems;fractals;image classification;image segmentation;image texture;learning (artificial intelligence);support vector machines","agricultural production;automated decision support system;background removal;fractal based features;grapes plant leaf disease detection system;high pass filter;leaf image segmentation;leaf texture analysis;machine learning;multiclass SVM classification;opposite colour local binary pattern feature;pattern recognition;plant disease classification;texture pattern extraction","","","","","","","11-12 Feb. 2016","","IEEE","IEEE Conference Publications"
"Homogenizing social networking with smart education by means of machine learning and Hadoop: A case study","A. Jagtap; B. Bodkhe; B. Gaikwad; S. Kalyana","Department of Computer Engineering, MES College of Engineering, Pune, India","2016 International Conference on Internet of Things and Applications (IOTA)","20160908","2016","","","85","90","In today's age of ever increasing use of internet, there are around 74% active internet users out of which 60% users contribute to social networking and most of them are students from the age group 16-30 [1]. If this young generation is targeted specifically towards educational activities keeping the same social networking environment in the background would create interest in students for educational activities and also yield productive results. Using Big Data analytics, machine learning and recommender system on the student data and activity would provide them with useful information and suggestions which would help them gain knowledge and make proper decisions to make their future in right direction. This can be implemented by creating a social-cum-educational portal with recommender systems, also data can be generated and displayed on the same place after analysis through recommenders. There is large amount of social, educational information generated on a rapid basis on the web which can be analysed and used for the betterment of the students and also the analysed information can be provided to the students based on their interests. Specific information to specific student can be provided. Use of such technology can reduce the gap between students and the information which can lead to their inherent development and success! However, most of the existing Social Recommender systems do not have good scalabilities which are unable to process huge volumes of data. Aiming to this problem we can design a social recommender system based on Hadoop and its parallel computing platform.","","","10.1109/IOTA.2016.7562700","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562700","Education recommender system;Education visualization;Hadoop;Machine learning;Smart student visualization;Social network;Social recommender system","Big data;Data analysis;Education;Portals;Recommender systems;Relational databases;Social network services","Big Data;computer aided instruction;data analysis;learning (artificial intelligence);parallel processing;portals;recommender systems;social networking (online)","Big Data analytics;Hadoop;Internet;educational activities;machine learning;parallel computing;smart education;social networking;social recommender systems;social-cum-educational portal;student activity;student data","","","","","","","22-24 Jan. 2016","","IEEE","IEEE Conference Publications"
"Quantum Machine Learning Based on Minimizing Kronecker-Reed-Muller Forms and Grover Search Algorithm with Hybrid Oracles","B. Lee; M. Perkowski","Dept. of Electr. & Comput. Eng., Portland State Univ., Portland, OR, USA","2016 Euromicro Conference on Digital System Design (DSD)","20161027","2016","","","413","422","This paper formulates the generic Machine Learning (ML) problem into finding the simplest spectral transform form (i.e. one having as many zero coefficients as possible) for an (in)complete binary function. The classical binary logic synthesis problem can be modeled to minimize a single output Boolean function with a two-level structure consisting of an exclusive-OR (EXOR) of ANDs of literals. The innovative approach in this paper is to build and simulate an accelerator that reduces learning to find the exact minimum expression of all 3n Kronecker Reed Muller (KRO) forms of a Boolean function with n input variables. This is in contrast to the previously studied quantum algorithm for the Fixed Polarity Reed-Muller forms (FPRM) which only selects from 2n possible forms. The algorithm, based on repeated application of a ternary Grover's Quantum Search algorithm, was simulated to find the minimum KRO form using a hybrid ternary/binary quantum oracle. This hybrid quantum system was simulated in Matlab and proved to be correct. The method can be also used as a future Quantum EDA Tool for exact minimization of AND/EXOR circuits, including reversible and quantum circuits.","","","10.1109/DSD.2016.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723581","EDA tools (CAD tools);Grover's search algorithm;Hybrid binary/ternary quantum circuits;Kronecker-Reed-Muller forms;Quantum Machine Learning;Quantum Oracle;exact minimization of AND/EXOR circuits","Boolean functions;Computers;Logic gates;Machine learning algorithms;Quantum computing;Quantum mechanics;Transforms","Boolean functions;Reed-Muller codes;learning (artificial intelligence);minimisation;quantum computing;search problems;transforms","Boolean function;FPRM;Grover search algorithm;Kronecker-Reed-Muller form;binary function;binary logic synthesis;binary quantum oracle;fixed polarity Reed-Muller forms;hybrid ternary oracle;n input variables;quantum EDA tool;quantum machine learning;spectral transform","","","","","","","Aug. 31 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"Text categorization with machine learning and hierarchical structures","M. Krendzelak; F. Jakab","Technical University of Kosice/KPI FEI TUKE, Kosice, Slovakia","2015 13th International Conference on Emerging eLearning Technologies and Applications (ICETA)","20160905","2015","","","1","5","Text categorization with machine learning algorithms usually assumes to have flat set of categories. Such classifiers are very domain specific and not reusable for some other generic text classifications. It is very possible that a hierarchically structured set of categories might have a higher impact on the way classifiers are used and built. As presented in this document, the list of most common approaches for text categorization disregards hierarchy. The main reason is because measured performance of hierarchical trials has not showed a significant difference. Nerveless, it is encouraged to perform additional research in text categorization with hierarchical structures.","","Electronic:978-1-4673-8534-3; POD:978-1-4673-8535-0","10.1109/ICETA.2015.7558486","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558486","","Algorithm design and analysis;Classification algorithms;Expert systems;Machine learning algorithms;Support vector machines;Text categorization;Training data","learning (artificial intelligence);pattern classification;set theory;text analysis","generic text classifications;hierarchically structured set;machine learning algorithms;text categorization","","","","","","","26-27 Nov. 2015","","IEEE","IEEE Conference Publications"
"Side-channel attacks and machine learning approach","A. Levina; D. Sleptsova; O. Zaitsev","ITMO University, Saint Petersburg, Russia","2016 18th Conference of Open Innovations Association and Seminar on Information Security and Protection of Information Technology (FRUCT-ISPIT)","20160908","2016","","","181","186","Most modern devices and cryptoalgorithms are vulnerable to a new class of attack called side-channel attack. It analyses physical parameters of the system in order to get secret key. Most spread techniques are simple and differential power attacks with combination of statistical tools. Few studies cover using machine learning methods for pre-processing and key classification tasks. In this paper, we investigate applicability of machine learning methods and their characteristic. Following theoretical results, we examine power traces of AES encryption with Support Vector Machines algorithm and decision trees and provide roadmap for further research.","","Electronic:978-9-5268-3973-8; POD:978-1-5090-2500-8","10.1109/FRUCT-ISPIT.2016.7561525","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7561525","","Decision trees;Kernel;Power demand;Principal component analysis;Side-channel attacks;Support vector machines","decision trees;learning (artificial intelligence);private key cryptography;support vector machines","AES encryption;cryptoalgorithms;decision trees;differential power attacks;key classification tasks;machine learning;power traces;secret key;side-channel attacks;statistical tools;support vector machines algorithm;system physical parameters","","","","","","","18-22 April 2016","","IEEE","IEEE Conference Publications"
"A machine learning framework for auto classification of imaging system exams in hospital setting for utilization optimization","M. A. Patil; R. B. Patil; P. Krishnamoorthy; J. John","Philips Research, Department of Health care Applications, Bangalore 560045 India","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2423","2426","In clinical environment, Interventional X-Ray (IXR) system is used on various anatomies and for various types of the procedures. It is important to classify correctly each exam of IXR system into respective procedures and/or assign to correct anatomy. This classification enhances productivity of the system in terms of better scheduling of the Cath lab, also provides means to perform device usage/revenue forecast of the system by hospital management and focus on targeted treatment planning for a disease/anatomy. Although it may appear classification of each exam into respective procedure/anatomy a simple task. However, in real-life hospital settings, it is well-known that same system settings are used to perform different types of procedures. Though, such usage leads to under-utilization of the system. In this work, a method is developed to classify exams into respective anatomical type by applying machine-learning techniques (SVM, KNN and decision trees) on log information of the systems. The classification result is promising with accuracy of greater than 90%.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591219","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591219","","Data mining;Decision trees;Feature extraction;Hospitals;Kernel;Support vector machines","decision trees;diagnostic radiography;diseases;image classification;learning (artificial intelligence);medical image processing;support vector machines","Cath lab;anatomical type;anatomy;decision trees;disease;hospital management;hospital setting;image autoclassification;imaging system;interventional X-ray system;k-nearest neighbors;machine-learning technique;support vector machines;treatment planning;utilization optimization","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning application in MOOCs: Dropout prediction","J. Liang; C. Li; L. Zheng","Department of Computer Science and Technology, Tsinghua University","2016 11th International Conference on Computer Science & Education (ICCSE)","20161006","2016","","","52","57","Massive Open Online Course(MOOC) is undergoing explosive growth recently, both the number of MOOC platforms and courses are increasing dramatically during these years. One of the major concerns in MOOC is high dropout rate, we study dropout prediction in MOOCs, using student's learning activities data in a period of time to measure how likely students would drop out in next couple of days. We collect 39 courses data from XuetangX platform, which is based on the open source Edx platform. Using supervised classification approach in the machine learning field, we achieve 89% accuracy in dropout prediction task with gradient boosting decision tree model. We describe details in drop out prediction framework, including data extraction from Edx platform, data preprocessing, feature engineering and performance test on several supervised classification models.","","","10.1109/ICCSE.2016.7581554","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581554","Classification;Feature enginnering;MOOCs;Supervised learning","Context;Data mining;Data models;Data preprocessing;Education;Feature extraction;Predictive models","computer aided instruction;decision trees;educational courses;learning (artificial intelligence);pattern classification;public domain software","MOOCs;XuetangX platform;data extraction;data preprocessing;dropout prediction;feature engineering;gradient boosting decision tree model;machine learning;massive open online course;open source Edx platform;performance test;student learning activities data;supervised classification models","","","","","","","23-25 Aug. 2016","","IEEE","IEEE Conference Publications"
"Improving DRAM Fault Characterization through Machine Learning","E. Baseman; N. DeBardeleben; K. Ferreira; S. Levy; S. Raasch; V. Sridharan; T. Siddiqua; Q. Guan","","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)","20161003","2016","","","250","253","As high-performance computing systems continue to grow in scale and complexity, the study of faults and errors is critical to the design of future systems and mitigation schemes. Fault modes in system DRAM are a frequently-investigated key aspect of memory reliability. While current schemes require offline analysis for proper classification, current state-of-the-art mitigation techniques require accurate online prediction for optimal performance. In this work, we explore the predictive performance of an online machine learning-based approach in classifying DRAM fault modes from two leadership-class supercomputing facilities. Our results compare the predictive performance of this online approach with the current rule-based approach based on expert knowledge, finding a 12% predictive performance improvement. We also investigate the universality of our classifiers by evaluating predictive performance using training data from disparate computing systems to achieve a 7% improvement in predictive performance. Our work provides a critical analysis of this online learning technique and can benefit system designers to help inform best practices for dealing with reliability on future systems.","","Electronic:978-1-5090-3688-2; POD:978-1-5090-3689-9","10.1109/DSN-W.2016.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575388","fault characterization;fault models;machine learning;supervised classification","Error correction codes;Machine learning algorithms;Prediction algorithms;Random access memory;Reliability engineering;Supercomputers","DRAM chips;electronic engineering computing;fault tolerant computing;integrated circuit reliability;learning (artificial intelligence);mainframes;parallel processing;pattern classification;performance evaluation","DRAM fault characterization;DRAM fault mode classification;high-performance computing systems;memory reliability;online machine learning-based approach;online optimal performance prediction;predictive performance evaluation;predictive performance improvement;supercomputing facilities","","","","","","","June 28 2016-July 1 2016","","IEEE","IEEE Conference Publications"
"Experimental Robot Inverse Dynamics Identification Using Classical and Machine Learning Techniques","V. Bargsten; J. d. Gea Fernandez; Y. Kassahun","","Proceedings of ISR 2016: 47st International Symposium on Robotics","20160905","2016","","","1","6","This paper shows the experimental identification of the inverse dynamics model of a KUKA iiwa lightweight robot. We use experimental data from optimal identification experiments to evaluate and compare two different identification approaches: a classical method using a parametrized robot dynamical model and a machine learning method. Both methods accurately estimate the dynamics model and this paper will discuss the pros and cons of each method.","","Paper:978-3-8007-4231-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558451","","","","","","","","","","","21-22 June 2016","","VDE","VDE Conference Publications"
"A Lockdown Technique to Prevent Machine Learning on PUFs for Lightweight Authentication","M. D. Yu; M. Hiller; J. Delvaux; R. Sowell; S. Devadas; I. Verbauwhede","Computer Security & Industrial Cryptography Laboratory, Verayo, Inc., Katholieke Universiteit Leuven, San Jose, Leuven, CA, Belgium","IEEE Transactions on Multi-Scale Computing Systems","20161021","2016","2","3","146","159","We present a lightweight PUF-based authentication approach that is practical in settings where a server authenticates a device, and for use cases where the number of authentications is limited over a device's lifetime. Our scheme uses a server-managed challenge/response pair (CRP) lockdown protocol: unlike prior approaches, an adaptive chosen-challenge adversary with machine learning capabilities cannot obtain new CRPs without the server's implicit permission. The adversary is faced with the problem of deriving a PUF model with a limited amount of machine learning training data. Our system-level approach allows a so-called strong PUF to be used for lightweight authentication in a manner that is heuristically secure against today's best machine learning methods through a worst-case CRP exposure algorithmic validation. We also present a degenerate instantiation using a weak PUF that is secure against computationally unrestricted adversaries, which includes any learning adversary, for practical device lifetimes and read-out rates. We validate our approach using silicon PUF data, and demonstrate the feasibility of supporting 10, 1,000, and 1M authentications, including practical configurations that are not learnable with polynomial resources, e.g., the number of CRPs and the attack runtime, using recent results based on the probably-approximately-correct (PAC) complexity-theoretic framework.","","","10.1109/TMSCS.2016.2553027","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450665","Physical unclonable function;authentication;computationally unrestricted adversary;heuristic security;machine learning;probably approximately correct (PAC) learning","Authentication;Cryptography;Manufacturing;Protocols;Servers;Silicon","computational complexity;cryptographic protocols","PAC complexity-theoretic framework;adaptive chosen-challenge adversary;challenge-response pair lockdown protocol;computationally unrestricted adversaries;lightweight PUF-based authentication;physically unclonable functions;probably-approximately-correct complexity-theoretic framework;server-managed CRP lockdown protocol;worst-case CRP exposure algorithmic validation","","","","","","20160411","July-Sept. 1 2016","","IEEE","IEEE Journals & Magazines"
"Machine learning based power quality event classification using wavelet â€” Entropy and basic statistical features","F. UÃ§ar; Ã–. F. AlÃ§in; B. Dandil; F. Ata","Dept. of Electrical Sciences, Faculty of Technical Education, Firat University, Elazig, Turkey","2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR)","20160926","2016","","","414","419","Today's industrial environment is smarter than ever before. Most production lines include electrical devices which are able to communicate each other and controlled from a single station with automation systems. Most of those elements have an internet connection link known as industrial internet. Development of smart technology with industrial internet comes with a need of monitoring. Monitoring technologies are emergent systems that focus on fault detection, grid self - healings and online tracking of power quality issues. Present study deals with one of the essential part of an electricity grid monitoring system called power quality event classification in a manner of machine learning topic. Power quality events to be processed are generated synthetically by means of a comprehensive software tool. Classification of real-like dataset is executed using extreme learning machine which is an extremely fast learning algorithm applied to single layer neural networks. Basic statistical criteria and wavelet - entropy methods are handled to achieve distinctive features of dataset. As a performance evaluation instrument, conventional artificial neural network structure is run too. Detailed results are discussed to prove the satisfactory performance of proposed pattern recognition model.","","","10.1109/MMAR.2016.7575171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575171","extreme learning machine;pattern recognition;power quality events;smart grid;wavelet transform","Classification algorithms;Discrete wavelet transforms;Entropy;Feature extraction;Mathematical model;Power quality","entropy;fault diagnosis;learning (artificial intelligence);neural nets;pattern classification;power engineering computing;power grids;power supply quality;power system measurement;statistical analysis;wavelet transforms","Internet connection link;artificial neural network structure;automation systems;basic statistical features;comprehensive software tool;electrical devices;electricity grid monitoring system;extreme learning machine;fast learning algorithm;fault detection;grid self-healings;industrial Internet;industrial environment;machine learning based power quality event classification;monitoring technology;online tracking;pattern recognition model;performance evaluation instrument;production lines;real-like dataset classification;single layer neural networks;smart technology;statistical criteria;wavelet-entropy methods","","","","","","","Aug. 29 2016-Sept. 1 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning-Based Approach to Estimate the CPU-Burst Time for Processes in the Computational Grids","T. Helmy; S. Al-Azani; O. Bin-Obaidellah","Dept. of Inf. & Comput. Sci., King Fahd Univ. of Pet. & Miner., Dhahran, Saudi Arabia","2015 3rd International Conference on Artificial Intelligence, Modelling and Simulation (AIMS)","20161024","2015","","","3","8","The implementation of CPU-Scheduling algorithms such as Shortest-Job-First (SJF) and Shortest Remaining Time First (SRTF) is relying on knowing the length of the CPU-bursts for processes in the ready queue. There are several methods to predict the length of the CPU-bursts, such as exponential averaging method, however these methods may not give an accurate or reliable predicted values. In this paper, we will propose a Machine Learning (ML) based approach to estimate the length of the CPU-bursts for processes. The proposed approach aims to select the most significant attributes of the process using feature selection techniques and then predicts the CPU-burst for the process in the grid. ML techniques such as Support Vector Machine (SVM) and K-Nearest Neighbors (K-NN), Artificial Neural Networks (ANN) and Decision Trees (DT) are used to test and evaluate the proposed approach using a grid workload dataset named ""GWA-T-4 Auver Grid"". The experimental results show that there is a strength linear relationship between the process attributes and the burst CPU time. Moreover, K-NN performs better in nearly all approaches in terms of CC and RAE. Furthermore, applying attribute selection techniques improves the performance in terms of space, time and estimation.","","","10.1109/AIMS.2015.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604542","CPU Scheduling Algorithm;CPU-Burst;Feature Selection;Machine Learning","Artificial intelligence;Computational modeling;Scheduling algorithms;Single machine scheduling;Testing","feature selection;learning (artificial intelligence);microprocessor chips;operating systems (computers);performance evaluation","CPU-burst time;CPU-scheduling algorithms;GWA-T-4 AuverGrid;K-NN;K-nearest neighbors;ML-based approach;burst CPU time;computational grids;feature selection;grid workload dataset;machine learning-based approach;strength linear relationship","","","","","","","2-4 Dec. 2015","","IEEE","IEEE Conference Publications"
"Deep semi-supervised learning using Multi-Layered Extreme Learning Machines","M. J. Er; A. Kashyap; N. Wang","Marine Engineering College, Dalian Maritime University, China","2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)","20160926","2016","","","457","462","Graph-based and deep learning-based semi supervised learning algorithms have been successfully used for semantic extraction from large unstructured data, alongside alternative methods like dimensionality reduction and embedding. Semantic information in the form of edges linking similar instances can be utilized to learn from unlabeled data and low-dimensional embeddings can be used to visualize and classify them. In this paper, we solve the problem of extending nonlinear embedding algorithms to Multi-Layered Extreme Learning Machines by plugging the network to auxiliary layers to improve the semi-supervised learning performance by further building on the structure assumption of data. Our model is significantly different from the previous models in two ways.","","Electronic:978-1-5090-2733-0; POD:978-1-5090-2734-7; USB:978-1-5090-2732-3","10.1109/CYBER.2016.7574869","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574869","Extreme Learning Machines;Semi-supervised Learning;deep learning;machine learning","Data models;Decoding;Manifolds;Neurons;Optimization;Semisupervised learning;Training","data structures;learning (artificial intelligence)","data structure;deep semi-supervised learning;edges;graph-based learning algorithms;multilayered extreme learning machines;nonlinear embedding algorithms;semantic data extraction;semantic information;unlabeled data","","","","","","","19-22 June 2016","","IEEE","IEEE Conference Publications"
"Enabling future progress in machine-learning","O. Temam","Google (Inria), France","2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits)","20160922","2016","","","1","3","Amazing progress in machine-learning, largely based on deep neural networks, has started to make applications once considered impossible, such as real-time translation or self-driving cars, a reality. However, even if, on some restricted problems, machine-learning is getting close to human-level performance, we are still far from the capabilities of the human brain. Machine-learning researchers themselves acknowledge that the progress observed in the past 10 years has been largely due to rapid increase in computing performance, allowing to tackle larger neural networks and larger training sets. So the computer systems and circuits communities can play a very significant role in enabling future progress. While GPUs have been a major driver of this recent progress, both the slowing rate of improvement of standard CMOS technology and the need for even faster progress suggest to at least explore alternative approaches. In this talk, we will discuss lessons learned from research on architectures for machine-learning, and that some of the hurdles ahead largely lie at the circuit level, but can possibly be overcome in the near future.","","","10.1109/VLSIC.2016.7573457","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573457","","Bandwidth;Biological neural networks;Computer architecture;Google;Hardware;Market research;Training","CMOS integrated circuits;learning (artificial intelligence);neural nets","CMOS;GPU;computer systems;human brain;machine-learning;neural networks;self-driving cars","","","","","","","15-17 June 2016","","IEEE","IEEE Conference Publications"
"Effort estimation of web-based applications using machine learning techniques","S. M. Satapathy; S. K. Rath","School of Computing and Information Technology, Manipal University Jaipur, Jaipur - 303007, Rajasthan, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20161103","2016","","","973","979","Effort estimation techniques play a crucial role in planning of the development of web-based applications. Web-based software projects, considered in the present-day scenario are different from conventional object oriented projects, and hence the task of effort estimation is a complex one. It is observed that the literature do not provide a guidance to the analysts to use a particular model as being the most suitable one, for effort estimation of web-based applications. A number of models like IFPUG Function Point Model, NESMA, MARK-II, etc. are being considered for web effort estimation purpose. The efficiency of these models can be improved by employing certain intelligent techniques on them. Keeping in mind the end goal to enhance the efficiency of evaluating the effort required to develop web-based application, certain machine learning techniques such as Stochastic Gradient Boosting and Support Vector Regression Kernels are considered in this study for effort estimation of web-based applications using IFPUG Function Point approach. The ISBSG dataset, Release 12 has been considered in this study for obtaining the IFPUG Function Point data. The performance effort estimation models based on various machine learning techniques is assessed with the help of certain metrics, in order to examine them critically.","","","10.1109/ICACCI.2016.7732171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732171","Software Effort Estimation;Stochastic Gradient Boosting;Support Vector Regression;Web-based Applications","Conferences;Informatics","Internet;learning (artificial intelligence);regression analysis;support vector machines","IFPUG function point data;IFPUG function point model;ISBSG dataset;MARK-II;NESMA;Web effort estimation;Web-based applications;Web-based software projects;analysts;effort estimation models;machine learning techniques;stochastic gradient boosting;support vector regression kernels","","","","","","","21-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"Is sentiment analysis an art or a science? Impact of lexical richness in training corpus on machine learning","S. Garg; A. Saini; N. Khanna","Department of Computer Science and Engineering, HMR Institute of Technology & Management, Hamidpur, Delhi, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20161103","2016","","","2729","2735","Social Media is exploding with data - that can help you derive an optimal marketing strategy in the internet world, engage with your audience on the fly, and protect your reputation from smearing campaigns if it is processed and analyzed in a timely fashion. Digital marketing analysts and data scientists rely on social media analytics tools to deduce customer sentiment from countless opinions and reviews. While numerous attempts have been made to improve their accuracy in the past, yet we know surprisingly little about how accurate their results are. We present an unbiased study of users' tweets and the methods that leverage the available tools & technologies for opinion mining. Our prime focus is on improving the consistency of text classifiers used for linguistic analysis. We also measure the impact of lexical richness in the sample data on the trained algorithm. This paper attempts to improve the reliability of sentiment classification process by the creation of a custom vote classifier using natural language processing techniques and various machine learning algorithms.","","","10.1109/ICACCI.2016.7732474","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732474","lexical richness;machine learning;natural language processing;opinion mining;sentiment classification;social media","Blogs;Data mining;Informatics;Motion pictures;Natural language processing;Twitter","Internet;learning (artificial intelligence);marketing;natural language processing;social networking (online);text analysis","Internet world;customer sentiment;data scientists;digital marketing analysts;lexical richness;linguistic analysis;machine learning algorithms;natural language processing techniques;opinion mining;optimal marketing strategy;sentiment analysis;sentiment classification process;smearing campaigns;social media analytics;training corpus","","","","","","","21-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"Construction of gazetteers from geo big data using machine learning technique on Hadoop","S. Pradeepa; K. R. Manjula","Information Technology, SASTRA University, Thanjavur","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","20161031","2016","","","1619","1622","Most gazetteers have been built and maintained for the purpose of visualizing geographical location on the Geographical Information system (GIS) client. The advent of big data allows us to construct gazetteers by directly mining rich volunteered information from the web. In this, we propose a technique for extracting location based spatial information from the web documents and media services like flickr, twitter, facebook for construction of gazetteers. To achieve this, we need to search the web for existing data pertaining location. A web crawler (Google search engine) generates the web pages based on the location keyword given by the user and maintaining the index of the web pages and the proposed system passes it to the Hadoop environment. For further simplification, the name node transfers the index group of web pages to different data nodes for extraction of spatial information from the dynamic web documents that we gather using machine learning process. Each data node is then utilized for the generation of a common template. The common template allows the extraction of location based spatial information from the dynamic web documents and media services. Resultant information from the data node is further merged using map reduce algorithms and the Hadoop Distributed File System (HDFS) is produced which is then converted to Geo-Java Script Object Notation (JSON) format, thus aiding in the task of visualizing the extracted information on the GIS client.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724541","Geographical Information Systems;HDFS;Hadoop;JSCON;Web Mining","Crawlers;Data mining;Data visualization;File systems;Hospitals;Spatial databases;Web pages","Big Data;Java;data handling;data mining;document handling;geographic information systems;learning (artificial intelligence);parallel processing;social networking (online)","Facebook;Flickr;GIS;HDFS;Hadoop distributed file system;JSON;Twitter;Web crawler;Web documents;data pertaining location;gazetteers;geo big data;geo-Java script object notation format;geographical information system client;location keyword;machine learning technique;media services;rich volunteered information mining;spatial information","","","","","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Machine-learning based detection of corresponding interest points in optical and SAR images","R. HÃ¤nsch; O. Hellwich; X. Tu","Computer Vision & Remote Sensing, Technische Universit&#x00E4;t Berlin, Germany","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","1492","1495","One of the major problems of keypoint-based alignment of SAR and optical images is that keypoint operators react to very different object structures in both image types. This leads to a small mutual overlap in the corresponding sets of keypoints. This paper proposes to cast the task of keypoint detection as a classification problem. A machine-learning based classifier is trained to predict whether a SAR image pixel corresponds to a keypoint in the optical image or not. Experimental results indicate, that the mutual overlap of keypoints can be doubled by the proposed approach.","","","10.1109/IGARSS.2016.7729381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729381","Keypoint detection;Random Forest;SAR data;image registration","Adaptive optics;Optical detectors;Optical imaging;Optical polarization;Synthetic aperture radar;Training","geophysical techniques;learning (artificial intelligence);optical images;synthetic aperture radar","SAR image;SAR image pixel;SAR keypoint-based alignment;classification problem;image object structures;keypoint detection;machine-learning based classifier is;machine-learning based detection;optical image","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Efficient compressive sensing of ECG segments based on machine learning for QRS-based arrhythmia detection","J. K. Pant; S. Krishnan","Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON, M5B 2K3, Canada","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","4731","4734","A novel method for efficient telemonitoring of arrhythmia based on using QRS complexes is proposed. Two features, namely, sum of absolute differences (SAD) and maximum of absolute differences (MAD) are efficiently computed for each ECG segment in the bio-sensor. The computed features can be transmitted from the bio-sensor using wireless channel, and they can be used in the receiver for determining the absence of QRS complex in the segment. By avoiding computationally expensive signal reconstruction for the ECG segments without QRS complex, it is shown, using simulation results, that computation time can be reduced by approximately 7.4% for long-term telemonitoring of QRS-based arrhythmia. Detection of the absence of QRS complex can be carried out in around 7 milliseconds in a standard laptop computer with 2.2GHz processor and 8GB RAM.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591784","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591784","","Biomedical measurement;Compressed sensing;Electrocardiography;Feature extraction;Memory management;Signal reconstruction;Support vector machines","compressed sensing;diseases;electrocardiography;learning (artificial intelligence);medical signal processing","ECG;MAD;QRS-based arrhythmia detection;SAD;compressive sensing;machine learning;maximum of absolute differences;signal reconstruction;sum of absolute differences;telemonitoring","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning-based behavior recognition system for a basketball player using multiple Kinect cameras","W. Y. Kuo; C. H. Kuo; S. W. Sun; P. C. Chang; Y. T. Chen; W. H. Cheng","Department of Communication Engineering, National Central University, Taiwan","2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20160926","2016","","","1","1","In this paper, a real-time behavior recognition demo system is proposed. By utilizing the captured skeletons and depth information from multiple Kinect cameras mounted at different locations with different view points, the occluded parts of a player and the ball information in the depth channels can be compensated by another Kinect camera without occlusion situations. Besides, a machine learning process trained from the the skeletons and depth channel information from two Kinect cameras makes the the behavior recognition rate to be more than 80% in real-time usage from three of the trained behaviors, i.e. right-hand dribble, left-hand dribble, and shooting behaviors.","","","10.1109/ICMEW.2016.7574661","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574661","behavior recognition;depth;machine learning;multiple Kinects;skeleton","Art;Cameras;Information technology;Media;Real-time systems;Skeleton;User interfaces","behavioural sciences computing;cameras;image recognition;learning (artificial intelligence)","basketball player;captured skeleton information;depth channel information;left-hand dribble behavior;machine learning-based behavior recognition system;multiple kinect cameras;real-time behavior recognition demo system;right-hand dribble behavior;shooting behavior","","","","","","","11-15 July 2016","","IEEE","IEEE Conference Publications"
"Investigating combinations of machine learning and classification techniques in a game environment","R. Edmundson; R. Danby; K. Brotherton; E. Livingstone; L. Allcock","Department of Computer Science and Technologies, Faculty of Engineering and Environment, Northumbria University, Newcastle upon Tyne, NE18ST, United Kingdom","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","20161024","2016","","","1306","1311","An open world turn based monster battle game was developed in Java using the popular LibGDX game framework applying multiple machine learning algorithms for its mechanics consisting of an ID3 decision tree, perceptron, naiÌˆve Bayes classifier and A* pathfinding in an attempt to imitate `machine intelligence'. A tiled map was used as the game area containing multiple AI agents with different personalities that change depending on the difficulty level chosen. The aim of the game focuses on the player defeating each `intelligent machine' non-player character's (NPC) upon interaction with each other, when player and enemy NPC sprites meet a battle screen appears to allow the player and enemy to engage in a turn-based battle with their monsters. When a battle is lost the player loses a life, otherwise they can approach and engage other enemy agents to battle on the map, and thus the game is called `Battle Monsters'.","","","10.1109/FSKD.2016.7603367","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603367","Aâˆ— pathfinding algorithms;ID3 decision tree;collision detection;ensemble methods;game development;hybrid methods;java;libGDX;machine learning;naÃ¯ve Bayes classifier;neural network","Artificial intelligence;Classification algorithms;Decision trees;Games;Neurons;Training;Training data","Bayes methods;Java;computer games;decision trees;learning (artificial intelligence);pattern classification;perceptrons;software agents","A* pathfinding;AI agents;Battle Monsters;ID3 decision tree;Java;LibGDX game framework;NPC;classification techniques;enemy agents;game environment;intelligent machine nonplayer characters;machine intelligence;machine learning combinations;multiple machine learning algorithms;naiÌˆve Bayes classifier;open world turn based monster battle game;perceptron;tiled map game area","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conference Publications"
"Linear, machine learning and probabilistic approaches for time series analysis","B. M. Pavlyshenko","Ivan Franko National University of Lviv, 1, Universytetska St., Lviv, 79000, Ukraine","2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP)","20161006","2016","","","377","381","In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA algorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.","","","10.1109/DSMP.2016.7583582","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583582","ARIMA;forecasting;predictive analytics;time series","Bayes methods;Boosting;Forecasting;Linear regression;Predictive models;Probabilistic logic;Time series analysis","Bayes methods;forecasting theory;learning (artificial intelligence);time series","ARIMA algorithm;Bayesian inference;XGBoost machine learning algorithm;copulas;forecasting approach;linear model approach;probabilistic modeling;time series analysis","","","","","","","23-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Features selection for building an early diagnosis machine learning model for Parkinson's disease","A. B. Soliman; M. Fares; M. M. Elhefnawi; M. Al-Hefnawy","Communication and Information Technology Department Nile University, Egypt","2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR)","20161013","2016","","","1","4","In this work, different approaches were evaluated to optimize building machine learning classification models for the early diagnosis of the Parkinson disease. The goal was to sort the medical measurements and select the most relevant parameters to build a faster and more accurate model using feature selection techniques. Decreasing the number of features to build a model could lead to more efficient machine learning algorithm and help doctors to focus on what are the most important measurements to take into account. For feature selection we compared the Filter and Wrapper techniques. Then we selected a good machine learning algorithm to detect which technique could help us by calculate the crossover scores for each technique. This research is based on a dataset which was created by Athanasius Tsanas and Max Little of the University of Oxford, in collaboration with 10 medical centers in the US and Intel Corporation. This target of these medical measurements is to find the Unified Parkinson's disease rating scale (UPDRS) which is the most commonly used scale for clinical studies of Parkinson's disease.","","","10.1109/ICAIPR.2016.7585225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585225","Features Selection;Parkinson;UPDRS","Biomedical measurement;Filtering algorithms;Machine learning algorithms;Medical diagnostic imaging;Parkinson's disease;Support vector machines","diseases;feature selection;learning (artificial intelligence);medical computing;patient diagnosis;pattern classification","Parkinson disease diagnosis;features selection;machine learning classification model","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Data Integration Using Machine Learning","M. Birgersson; G. Hansson; U. Franke","Dept. of Comput. Sci. & Eng., Chalmers Univ. of Technol., Gothenburg, Sweden","2016 IEEE 20th International Enterprise Distributed Object Computing Workshop (EDOCW)","20161010","2016","","","1","10","Today, enterprise integration and cross-enterprise collaboration is becoming evermore important. The Internet of things, digitization and globalization are pushing continuous growth in the integration market. However, setting up integration systems today is still largely a manual endeavor. Most probably, future integration will need to leverage more automation in order to keep up with demand. This paper presents a first version of a system that uses tools from artificial intelligence and machine learning to ease the integration of information systems, aiming to automate parts of it. Three models are presented and evaluated for precision and recall using data from real, past, integration projects. The results show that it is possible to obtain F<sub>0.5</sub> scores in the order of 80% for models trained on a particular kind of data, and in the order of 60%-70% for less specific models trained on a several kinds of data. Such models would be valuable enablers for integration brokers to keep up with demand, and obtain a competitive advantage. Future work includes fusing the results from the different models, and enabling continuous learning from an operational production system.","","","10.1109/EDOCW.2016.7584357","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7584357","","Biological system modeling;Computational modeling;Computer science;Data integration;Data models;Semantics;Training data","Internet of Things;data integration;groupware;information systems;learning (artificial intelligence);project management","Internet of things;artificial intelligence;continuous learning;cross-enterprise collaboration;data integration;digitization;enterprise integration;globalization;information system integration;integration brokers;integration market;integration projects;machine learning;operational production system","","1","","","","","5-9 Sept. 2016","","IEEE","IEEE Conference Publications"
"Implementation of a smartphone as a wireless gyroscope platform for quantifying reduced arm swing in hemiplegie gait with machine learning classification by multilayer perceptron neural network","R. LeMoyne; T. Mastroianni","Department of Biological Sciences and Center for Bioengineering Innovation, Northern Arizona University, Flagstaff, AZ 86011-5640 USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2626","2630","Natural gait consists of synchronous and rhythmic patterns for both the lower and upper limb. People with hemiplegia can experience reduced arm swing, which can negatively impact the quality of gait. Wearable and wireless sensors, such as through a smartphone, have demonstrated the ability to quantify various features of gait. With a software application the smartphone (iPhone) can function as a wireless gyroscope platform capable of conveying a gyroscope signal recording as an email attachment by wireless connectivity to the Internet. The gyroscope signal recordings of the affected hemiplegic arm with reduced arm swing arm and the unaffected arm are post-processed into a feature set for machine learning. Using a multilayer perceptron neural network a considerable degree of classification accuracy is attained to distinguish between the affected hemiplegic arm with reduced arm swing arm and the unaffected arm.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591269","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591269","arm swing;feature set;hemiplegic gait;iPhone;machine learning;multilayer perceptron neural network;reduced arm swing;smartphone;wireless gyroscope","Biological neural networks;Gyroscopes;Multilayer perceptrons;Sensors;Software;Wireless communication;Wireless sensor networks","Internet;body sensor networks;gait analysis;gyroscopes;learning (artificial intelligence);medical signal processing;multilayer perceptrons;neurophysiology;smart phones;telemedicine","Internet;gyroscope signal recording;hemiplegic arm;hemiplegie gait analysis;lower limb;machine learning classification;multilayer perceptron neural network;reduced arm swing quantification;rhythmic patterns;smartphone;synchronous patterns;upper limb;wearable sensors;wireless gyroscope platform;wireless sensors","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"MimeoDroid: Large Scale Dynamic App Analysis on Cloned Devices via Machine Learning Classifiers","P. Faruki; A. Zemmari; M. S. Gaur; V. Laxmi; M. Conti","Dept. of Comput. Eng., Gov. MCA Coll., India","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)","20161003","2016","","","60","65","The exponential adoption of Android applications (apps) among the users has attracted malware authors to evade the default emulator based dynamic analysis systems. The evolving Android malware behaves benign once it identifies Goldfish emulator, often used for app development and malware analysis. Once a malware identifies the Goldfish virtual device, it behaves benign or prevents malicious code execution. The exponential increase of such stealth malware necessitates a detection approach which coerces the malicious apps to reveal the hidden behavior. To detect malicious apps and characterize their association we propose MimeoDroid (enriched replica of real Android device), a modified virtual clone to coerce the malware to believe being executed on an actual device. We automate relevant feature extraction and classification of Processor, memory usage, Binder IPC transfers, network interaction, battery charging status and manifest permission(s) to detect malicious behavior using Tree based machine learning classifiers. MimeoDroid is a lightweight machine learning based malware analysis and characterization to detect malicious apps that would evade the existing analyzers.","","Electronic:978-1-5090-3688-2; POD:978-1-5090-3689-9","10.1109/DSN-W.2016.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575351","","Androids;Cloning;Feature extraction;Humanoid robots;Malware;Proposals;Training","Android (operating system);learning (artificial intelligence);mobile computing","Android applications;Android malware;Binder IPC transfers;Goldfish emulator;Goldfish virtual device;MimeoDroid;actual device;battery charging status;cloned devices;exponential adoption;feature extraction;large scale dynamic App analysis;machine learning classifiers;malicious code execution;malware authors;manifest permission;network interaction;processor classification","","","","","","","June 28 2016-July 1 2016","","IEEE","IEEE Conference Publications"
"Machine Learning-Based Image Classification for Wireless Camera Sensor Networks","J. Ahn; J. Paek; J. Ko","Dept. of Software & Comput. Eng., Ajou Univ., Ajou, South Korea","2016 IEEE 22nd International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)","20161003","2016","","","103","103","Wireless sensor networks, with their capability to capture physical phenomena at micro-scale, have changed how we collect and analyze data from the real world. Especially, using low-power cameras we can design interesting applications that provide us with previously difficult-to-capture information from the real-world. While cameras hold privacy threats in human-residing environments, they can be actively used in natural world analyzing applications. However, the disadvantage of using cameras is that the samples themselves (e.g., images) have large sizes, forcing the system to exchange more data, and in turn, decreasing energy efficiency. Given that camera-based sensor networks are usually deployed in remote locations, the lifetime of individual devices become a major concern. In this work, we target to utilize machine-learning algorithms to characterize the context of images captured from a real-world deployments. Specifically, using images from the James Reserve bird nest deployment [1], we utilize and optimize machine learning algorithms to operate on embedded low-power, resource-limited platforms. Using both a systematic and algorithmic approach, our proposed systems architecture and algorithms hold the potential to reduce the amount of data to be transmitted by as much a eight-fold.","","","10.1109/RTCSA.2016.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579939","Image classification;Wireless Camera Sensor Networks","Birds;Cameras;Histograms;Machine learning algorithms;Servers;Systems architecture;Wireless sensor networks","cameras;embedded systems;energy conservation;image classification;learning (artificial intelligence);low-power electronics;wireless sensor networks","James Reserve bird nest deployment;embedded low-power resource-limited platforms;energy efficiency;low-power cameras;machine learning-based image classification;privacy threats;wireless camera sensor networks","","","","","","","17-19 Aug. 2016","","IEEE","IEEE Conference Publications"
