"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7194735,7216746,7216731,7208635,7207408,7207250,7210335,7194741,7207196,7207374,7207291,7210439,7203318,7169364,7196542,7184892,7184901,7184914,7184688,7184877,7190443,7193038,7184808,7184891,7184668,7194280,7193054,7184916,7159062,7094280,7181948,7181905,7178188,7175902,7168469,7175952,7178346,7176280,7177225,7120172,7180092,7178127,7177954,7180063,7175752,7178343,7178657,7178484,6951423,7069208,6682892,7173581,6994292,6990595,7055837,7168938,7170477,7170394,7169194,7170624,7171099,6861439,7167279,7167186,7167480,7167521,7166576,7167251,7167384,7096998,7164140,7164189,7104094,7164035,7164069,7164645,7164008,7164064,7165222,7163895,7163876,7163858,6905710,7163042,7161571,7161501,7063914,7158676,7158334,7160186,7158930,7158067,7160142,7158929,7155971,7029116,7156012,6879477,7154740,7153948",2017/05/05 21:53:00
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A Practical Evaluation of Information Processing and Abstraction Techniques for the Internet of Things","F. Ganz; D. Puschmann; P. Barnaghi; F. Carrez","Centre for Communication Systems Research, University of Surrey, Surrey, U.K.","IEEE Internet of Things Journal","20150730","2015","2","4","340","354","The term Internet of Things (IoT) refers to the interaction and communication between billions of devices that produce and exchange data related to real-world objects (i.e. things). Extracting higher level information from the raw sensory data captured by the devices and representing this data as machine-interpretable or human-understandable information has several interesting applications. Deriving raw data into higher level information representations demands mechanisms to find, extract, and characterize meaningful abstractions from the raw data. This meaningful abstractions then have to be presented in a human and/or machine-understandable representation. However, the heterogeneity of the data originated from different sensor devices and application scenarios such as e-health, environmental monitoring, and smart home applications, and the dynamic nature of sensor data make it difficult to apply only one particular information processing technique to the underlying data. A considerable amount of methods from machine-learning, the semantic web, as well as pattern and data mining have been used to abstract from sensor observations to information representations. This paper provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current state-of-the-art in this area. This paper also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, we introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets.","2327-4662;23274662","","10.1109/JIOT.2015.2411227","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7055837","Data Abstraction;Data abstraction;Internet of Things;Internet of Things (IoT);Semantic Web;Software Tools;machine-learning;semantic Web;software tools","Band-pass filters;Context;Data mining;Information filters;Internet of things;Vectors","Internet of Things;data mining;learning (artificial intelligence);semantic Web","Internet of Things;IoT;abstraction techniques;data mining;human-understandable information data;information abstraction;information processing;information processing technique;machine-interpretable data;machine-learning;pattern mining;semantic Web;sensor devices","","14","","51","","20150306","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Markerless augmented reality based on local binary pattern","Y. Hbali; M. Sadgal; A. EL Fazziki","Faculty of Sciences SEMLALIA, Cadi Ayyad University, Prince Moulay Abdellah Avenue, Marrakech, Morocco","2013 International Conference on Signal Processing and Multimedia Applications (SIGMAP)","20150813","2013","","","137","141","Augmented reality is becoming the future of e-commerce, throw their mobile devices, customers have access to all kind of information, going from weather, news papers, shops and so on. Today's mobiles devices are so powerful to the point that they can be used as a platform of virtual try-on systems. Over this paper we present a virtual eye glasses try-on system based on augmented reality and LBP for face and eyes detection. The well-known machine learning Ada Boost algorithm is used for real time eyes tracking, the resulting face and eyes positions are continuously utilized to overlay the glasses model over the face. The system helps evaluating glasses before trying them in the store and makes possible the design of its own style.","","Electronic:978-9-8975-8129-8; POD:978-1-4799-7136-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184688","Augmented Reality;Eyes Detection;Face Detection;Local Binary Pattern;Machine Learning","Biological system modeling;Computational modeling;Databases;Detectors;Electronic commerce;Lighting;Time factors","Ada;augmented reality;electronic commerce;face recognition;learning (artificial intelligence)","e-commerce;eyes detection;eyes tracking;face detection;local binary pattern;machine learning Ada Boost algorithm;markerless augmented reality;mobile devices;virtual eye glasses try-on system;virtual try-on systems","","0","","16","","","29-31 July 2013","","IEEE","IEEE Conference Publications"
"Q-learning based hyper-heuristic for scheduling system self-parameterization","D. Falcão; A. Madureira; I. Pereira","GECAD Research Group-School of Engineering, Polytechnic Institute of Porto, Porto, Portugal","2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","20150730","2015","","","1","7","Optimization in current decision support systems has a highly interdisciplinary nature related with the need to integrate different techniques and paradigms for solving real-world complex problems. Computing optimal solutions in many of these problems are unmanageable. Heuristic search methods are known to obtain good results in an acceptable time interval. However, parameters need to be adjusted to allow good results. In this sense, learning strategies can enhance the performance of a system, providing it with the ability to learn, for instance, the most suitable optimization technique for solving a particular class of problems, or the most suitable parameterization of a given algorithm on a given scenario. Hyper-heuristics arise in this context as efficient methodologies for selecting or generating (meta) heuristics to solve NP-hard optimization problems. This paper presents the specification of a hyper-heuristic for selecting techniques inspired in nature, for solving the problem of scheduling in manufacturing systems, based on previous experience. The proposed hyper-heuristic module uses a reinforcement learning algorithm, which enables the system with the ability to autonomously select the meta-heuristic to use in optimization process as well as the respective parameters. A computational study was carried out to evaluate the influence of the hyper-heuristics on the performance of a scheduling system. The obtained results allow to conclude about the effectiveness of the proposed approach.","2166-0727;21660727","Electronic:978-9-8998-4345-5; POD:978-1-4799-8330-8","10.1109/CISTI.2015.7170394","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170394","Hyper-heuristics;Machine Learning;Meta-heuristics;Multi-Agent Systems;Optimization;Q-Learning;Scheduling","Context;Job shop scheduling;Learning (artificial intelligence);Optimal scheduling;Processor scheduling","computational complexity;decision support systems;learning (artificial intelligence);manufacturing systems;production engineering computing;scheduling;search problems","NP-hard optimization problems;Q-learning based hyper-heuristic module;decision support systems;heuristic search methods;manufacturing systems;metaheuristics;optimization technique;reinforcement learning algorithm;system self-parameterization scheduling","","0","","30","","","17-20 June 2015","","IEEE","IEEE Conference Publications"
"Cloning your mind: Security challenges in cognitive system designs and their solutions","Beiye Liu; Chunpeng Wu; Hai Li; Yiran Chen; Qing Wu; M. Barnell; Qinru Qiu","University of Pittsburgh, PA, 15261, USA","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20150727","2015","","","1","5","With the booming of big-data applications, cognitive information processing systems that leverage advanced data processing technologies, e.g., machine learning and data mining, are widely used in many industry fields. Although these technologies demonstrate great processing capability and accuracy in the relevant applications, several security and safety challenges are also emerging against these learning based technologies. In this paper, we will first introduce several security concerns in cognitive system designs. Some real examples are then used to demonstrate how the attackers can potentially access the confidential user data, replicate a sensitive data processing model without being granted the access to the details of the model, and obtain some key features of the training data by using the services publically accessible to a normal user. Based on the analysis of these security challenges, we also discuss several possible solutions that can protect the information privacy and security of cognitive systems during different stages of the usage.","0738-100X;0738100X","Electronic:978-1-4799-8052-9; POD:978-1-4799-8053-6","10.1145/2744769.2747915","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167279","Cognitive Systems;Machine Learning;Security","Data models;Neural networks;Predictive models;Security;Training;Training data","Big Data;cognition;security of data","Big-Data application;cognitive information processing systems;cognitive system design;data mining;data security;machine learning;sensitive data processing model","","1","","13","","","8-12 June 2015","","IEEE","IEEE Conference Publications"
"Impacts of Raw Data Temporal Resolution Using Selected Clustering Methods on Residential Electricity Load Profiles","R. Granell; C. J. Axon; D. C. H. Wallom","Oxford e-Research Centre, University of Oxford, Oxford, UK","IEEE Transactions on Power Systems","20150803","2015","30","6","3217","3224","There is growing interest in discerning behaviors of electricity users in both the residential and commercial sectors. With the advent of high-resolution time-series power demand data through advanced metering, mining this data could be costly from the computational viewpoint. One of the popular techniques is clustering, but depending on the algorithm the resolution of the data can have an important influence on the resulting clusters. This paper shows how temporal resolution of power demand profiles affects the quality of the clustering process, the consistency of cluster membership (profiles exhibiting similar behavior), and the efficiency of the clustering process. This work uses both raw data from household consumption data and synthetic profiles. The motivation for this work is to improve the clustering of electricity load profiles to help distinguish user types for tariff design and switching, fault and fraud detection, demand-side management, and energy efficiency measures. The key criterion for mining very large data sets is how little information needs to be used to get a reliable result, while maintaining privacy and security.","0885-8950;08858950","","10.1109/TPWRS.2014.2377213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6994292","Classification algorithms;clustering algorithms;data mining;energy consumption;machine learning;power demand;smart grids","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Energy consumption;Machine learning;Power demand;Smart grids","data mining;data privacy;demand side management;fault diagnosis;learning (artificial intelligence);pattern classification;pattern clustering;power meters;power system analysis computing;power system economics;power system faults;power system security;tariffs;time series;very large databases","advanced metering;classification algorithms;cluster membership consistency;clustering process efficiency;clustering process quality;commercial sectors;data resolution;demand-side management;energy efficiency measures;fault detection;fraud detection;high-resolution time-series power demand data;information needs;machine learning;power demand profiles;privacy maintenance;raw data temporal resolution;residential electricity load profiles;residential sectors;security maintenance;selected clustering methods;tariff design;tariff switching;very large data set mining","","1","","29","","20141219","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"An energy-efficient memory-based high-throughput VLSI architecture for convolutional networks","M. Kang; S. K. Gonugondla; M. S. Keel; N. R. Shanbhag","University of Illinois at Urbana-Champaign, United States","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","1037","1041","In this paper, an energy efficient, memory-intensive, and high throughput VLSI architecture is proposed for convolutional networks (C-Net) by employing compute memory (CM) [1], where computation is deeply embedded into the memory (SRAM). Behavioral models incorporating CM's circuit non-idealities and energy models in 45nm SOI CMOS are presented. System-level simulations using these models demonstrate that the probability of handwritten digit recognition P<sub>r</sub> > 0.99 can be achieved using the MNIST database [2], along with a 24.5× reduced energy delay product, a 5.0× reduced energy, and a 4.9× higher throughput as compared to the conventional system.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178127","Compute memory;Convolutional networks;Machine learning;Pattern recognition","Arrays;Computational modeling;Delays;Random access memory;Registers;Throughput","CMOS integrated circuits;SRAM chips;convolution;energy conservation;handwritten character recognition;silicon-on-insulator","C-Net;CM circuit;MNIST database;SOI CMOS;SRAM;behavioral model;compute memory;convolutional network;energy delay product;energy-efficient memory-based high-throughput VLSI architecture;handwritten digit recognition probability;size 45 nm;system-level simulation","","2","","6","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Cyber Protection of Critical Infrastructures Using Supervised Learning","A. Patrascu; V. V. Patriciu","Comput. Sci. Dept., Mil. Tech. Acad., Bucharest, Romania","2015 20th International Conference on Control Systems and Computer Science","20150806","2015","","","461","468","Interconnected computing units are used more and more in our daily lives, starting from the transportation systems and ending with gas and electricity distribution, together with tenths or hundreds of systems and sensors, called critical infrastructures. In this context, cyber protection is vital because they represent one of the most important parts of a country's economy thus making them very attractive to cyber criminals or malware attacks. Even though the detection technologies for new threats have improved over time, modern malware still manage to pass even the most secure and well organized computer networks, firewalls and intrusion detection equipments, making all systems vulnerable. This is the main reason that automatic learning is used more often than any other detection algorithms as it can learn from existing attacks and prevent newer ones. In this paper we discuss the issues threatening critical infrastructures systems and propose a framework based on machine learning algorithms and game theory decision models that can be used to protect such systems. We present the results taken after implementing it using three distinct classifiers - k nearest neighbors, decision trees and support vector machines.","2379-0474;23790474","Electronic:978-1-4799-1780-8; POD:978-1-4799-1781-5","10.1109/CSCS.2015.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168469","critical infrastructure protection;cybersecurity framework;game theory decision engine;machine learning","Biological system modeling;Game theory;Security;Sensors;Support vector machines;Testing;Training","decision trees;game theory;learning (artificial intelligence);pattern classification;security of data;support vector machines","computer networks;critical infrastructure;cyber criminals;cyber protection;decision trees;firewalls;game theory decision models;interconnected computing units;intrusion detection equipments;k nearest neighbors;machine learning algorithms;malware attacks;supervised learning;support vector machines","","1","","23","","","27-29 May 2015","","IEEE","IEEE Conference Publications"
"Approximation Errors of Online Sparsification Criteria","P. Honeine","Institut Charles Delaunay (CNRS), Universit&#x00E9; de technologie de Troyes, Troyes, France","IEEE Transactions on Signal Processing","20150806","2015","63","17","4700","4709","Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient sparsification criteria have been introduced in the literature with the distance and the coherence criteria. This paper provides a unified framework that connects these sparsification criteria in terms of approximating samples, by establishing theoretical bounds on the approximation errors. Furthermore, the error of approximating any pattern is investigated, by proposing upper bounds on the approximation error for each of the aforementioned sparsification criteria. Two classes of fundamental patterns are described in detail, the centroid (i.e., empirical mean) and the principal axes in the kernel principal component analysis. Experimental results show the relevance of the theoretical results established in this paper.","1053-587X;1053587X","","10.1109/TSP.2015.2442960","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120172","Adaptive filtering;gram matrix;kernel-based methods;machine learning;online learning;pattern recognition;resource-allocating networks;sparse approximation;sparsification criteria","Coherence;Computational modeling;Dictionaries;Kernel;Least squares approximations;Principal component analysis","","","","2","","44","","20150609","Sept.1, 2015","","IEEE","IEEE Journals & Magazines"
"Unsupervised feature learning for urban sound classification","J. Salamon; J. P. Bello","Center for Urban Science and Progress, New York University, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","171","175","Recent studies have demonstrated the potential of unsupervised feature learning for sound classification. In this paper we further explore the application of the spherical k-means algorithm for feature learning from audio signals, here in the domain of urban sound classification. Spherical k-means is a relatively simple technique that has recently been shown to be competitive with other more complex and time consuming approaches. We study how different parts of the processing pipeline influence performance, taking into account the specificities of the urban sonic environment. We evaluate our approach on the largest public dataset of urban sound sources available for research, and compare it to a baseline system based on MFCCs. We show that feature learning can outperform the baseline approach by configuring it to capture the temporal dynamics of urban sources. The results are complemented with error analysis and some proposals for future research.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7177954","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177954","Unsupervised learning;machine learning;sound classification;spherical k-means;urban","Accuracy;Context;Encoding;Engines;Noise;Speech;Training data","acoustic signal processing;pipelines;unsupervised learning","MFCC;audio signals;baseline system;error analysis;processing pipeline influence performance;public dataset;sound classification;spherical k-means algorithm;temporal dynamics;time consuming approach;unsupervised feature learning;urban sonic environment;urban sound classification;urban sound source","","8","","26","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Building an IoT Framework for Connected Dairy","A. Ilapakurti; C. Vuppalapati","IoT & Data Analytics Groupanalytics;, Hanumayamma Innovations & Technol. Inc., Fremont, CA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","275","285","Heat stress (HS) causes cows to produce less milk with the same nutritional input, which effectively increases farmers' production costs. The economic toll due to higher-temperature, heat stress is a $1 billion annual problems. Not only in the United States, but also around the globe heat stress causes an adverse impact on dairy productivity. The opportunities, however, for the dairy industry is to electronically monitor cattle temperature and implement appropriate measures so that the impact of HS can be minimized. The U.S. Department of Agriculture estimates nearly $2.4 billion a year in losses from animal illnesses that lead to death can be prevented by electronically checking on cattle's' vital signs. This research paper recommends the most innovative electronic monitor framework, the 'Smart Connected Objects"", aka, 'the Internet of Things (IoT)', that enables dairies to minimize the economic impact of HS and, at the same, capture the higher Return on Assets (ROA) & Return on Investment (ROI) by improving operational efficiencies. Happy Cow, more importantly, means happier, more profitable, dairy industry and richer and creamer dairy products. The proposed framework supports both offline and online dairy IoT. This paper presents a prototyping solution design as well as its application and certain experimental results.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184891","Android;BLE;Bluetooth Low Energy;CEP;Complex Event Processing;Dairy;Decision Tree;Heat Stress;Internet Of Things;IoT;IoT reference architecture;Machine Learning;Memory Data Cube;Regression Analysis;Sensor Tag;iOS;streaming analytics","Bluetooth;Cows;Decision trees;Heating;Stress;Temperature measurement;Temperature sensors","Internet of Things;dairy products;dairying;investment","Happy Cow;Internet of Things;IoT framework;ROA;ROI;U.S. Department of Agriculture;animal illness;cattle temperature;connected dairy;dairy industry;dairy productivity;economic toll;electronic monitor framework;farmer production cost;heat stress;higher-temperature;nutritional input;offline dairy IoT;online dairy IoT;prototyping solution design;return on assets;return on investment","","0","","18","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"Toward Deep Learning Software Repositories","M. White; C. Vendome; M. Linares-Vasquez; D. Poshyvanyk","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","334","345","Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180092","Software repositories;deep learning;machine learning;n-grams;neural networks;software language models","Computational modeling;Computer architecture;Context;Context modeling;Machine learning;Software;Training","Java;learning (artificial intelligence);natural language processing;program testing;project management;source code (software)","Java project corpus;NLP techniques;SE community;SE task;automatic compositional representation learning;code suggestion;connectionist models;deep-learning software repositories;high-quality models;hyperparameters;lexically analyzed source code files;natural language processing;programming language;sequential data model;software artifact conceptualization;software corpora;software engineering community;software language modeling;software lexicon improvement;software token streaming","","2","","76","","","16-17 May 2015","","IEEE","IEEE Conference Publications"
"Highlights on analyzing one-way traffic using different tools","E. Balkanli; A. N. Zincir-Heywood","Faculty of Computer Science, Dalhousie University, Halifax, Canada","2015 IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA)","20150820","2015","","","1","8","In this paper, we present our analysis using four different systems on two different one-way network traffic data sets. Specifically, we have explored the usage of two network traffic analyzers, namely Corsaro and Cisco ASA 5515-X, and two machine learning based systems, namely the C4.5 Decision Tree classifier and the AdaBoost.M1 classifier. We have employed these four systems on two publicly available one-way network data sets provided by CAIDA from 2008 and 2012. Our analysis on these systems are based on the detection rate, false alarm rate, computational cost and ease of use of these systems. To the best of our knowledge, this work is the first one performing such an analysis and evaluating machine learning based systems against well known commercial as well as open source ones on one-way network traffic data sets.","2329-6267;23296267","CD-ROM:978-1-4673-7556-6; Electronic:978-1-4673-7557-3; POD:978-1-4673-7558-0","10.1109/CISDA.2015.7208635","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208635","One-way traffic;machine learning;network traffic monitoring","Backscatter;Decision trees;IP networks;Monitoring;Protocols;Security;Training","computer network security;decision trees;learning (artificial intelligence);telecommunication traffic","AdaBoost.M1 classifier;C4.5 decision tree classifier;CAIDA;Cisco ASA 5515-X;different tools;false alarm rate;machine learning;network traffic data sets;one-way network traffic data sets;one-way traffic analysis","","1","","27","","","26-28 May 2015","","IEEE","IEEE Conference Publications"
"Automating Risk of Bias Assessment for Clinical Trials","I. J. Marshall; J. Kuiper; B. C. Wallace","Department of Primary Care and Public Health Sciences, King's College London, London, U.K.","IEEE Journal of Biomedical and Health Informatics","20150723","2015","19","4","1406","1412","Systematic reviews, which summarize the entirety of the evidence pertaining to a specific clinical question, have become critical for evidence-based decision making in healthcare. But such reviews have become increasingly onerous to produce due to the exponentially expanding biomedical literature base. This study proposes a step toward mitigating this problem by automating risk of bias assessment in systematic reviews, in which reviewers determine whether study results may be affected by biases (e.g., poor randomization or blinding). Conducting risk of bias assessment is an important but onerous task. We thus describe a machine learning approach to automate this assessment, using the standard Cochrane Risk of Bias Tool which assesses seven common types of bias. Training such a system would typically require a large labeled corpus, which would be prohibitively expensive to collect here. Instead, we use distant supervision, using data from the Cochrane Database of Systematic Reviews (a large repository of systematic reviews), to pseudoannotate a corpus of 2200 clinical trial reports in PDF format. We then develop a joint model which, using the full text of a clinical trial report as input, predicts the risks of bias while simultaneously extracting the text fragments supporting these assessments. This study represents a step toward automating or semiautomating extraction of data necessary for the synthesis of clinical trials.","2168-2194;21682194","","10.1109/JBHI.2015.2431314","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7104094","Evidence-based medicine;health informatics;machine learning;natural language processing","Clinical trials;Feature extraction;Informatics;Joints;Random sequences;Resource management;Systematics","data analysis;decision making;learning (artificial intelligence);medical information systems","Cochrane Database of Systematic Reviews;Cochrane Risk of Bias Tool;blinding;clinical trials;data extraction;distant supervision;evidence-based decision making;healthcare;large labeled corpus;machine learning;poor randomization;risk of bias assessment;text fragment extraction","0","1","","44","","20150508","July 2015","","IEEE","IEEE Journals & Magazines"
"Glucose-tracking: A postprandial glucose prediction system for diabetic self-management","G. Shi; S. Zou; A. Huang","State Key Laboratory of Networking and Switching Technology, BUPT","2015 2nd International Symposium on Future Information and Communication Technologies for Ubiquitous HealthCare (Ubi-HealthTech)","20150817","2015","","","1","9","Up to today, there are around 400 million diabetics in the world. In China, there are more than 100 million diabetics. How to help them track and manage real-time glucose level is significant to control diabetic progression. As well known, the glucose level is directly related with food, while the conventional tracking glucose is depending invasive or minimally invasive methods. This phenomenon causes many problems to diabetics for food selection and glucose monitoring. To solve this challenge, we are motivated to propose a postprandial glucose prediction model, which can connect food selection and diabetic self-management seamlessly, without involving invasive glucose monitoring. In this paper, we first build up a glucose tracking system based on Android platform. And then, we conceive a postprandial glucose prediction model based on machine learning methods, in which sample data from diabetics' diet are collected and analyzed for predicting a postprandial glucose level of a user. To verify our model, we compared the prediction results with our clinical tests by using traditional glucose monitoring methods. In order to extract a reliable glucose prediction model, two kinds of linear regression algorithms are adopted, which confidence margin is bounded within 20% that matching with the FDA standard. Our experiment results show that this prediction-based glucose tracking system is helpful to diabetics, and can be used as an auxiliary tool to control their glucose and diet.","","Electronic:978-1-4799-6168-9; POD:978-1-4799-6169-6","10.1109/Ubi-HealthTech.2015.7203318","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203318","DiabeticSelf-Management.;Food Selection;Glucose-Tracking;Machine Learning;Postprandial GlucosePrediction","Androids;Blood;Diabetes;Humanoid robots;Servers;Sugar;Training","Android (operating system);biomedical telemetry;diseases;learning (artificial intelligence);medical computing;regression analysis;sugar","Android platform;FDA standard;clinical tests;diabetic progression;diabetic self-management;glucose monitoring;machine learning methods;minimally invasive methods;postprandial glucose prediction model;prediction-based glucose tracking system","","0","","11","","","28-30 May 2015","","IEEE","IEEE Conference Publications"
"Power Efficient MapReduce Workload Acceleration Using Integrated-GPU","S. Kim; J. Bottleson; J. Jin; P. Bindu; S. C. Sakhare; J. S. Spisak","Intel Corp., Folsom, CA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","162","169","With the pervasiveness of MapReduce - one of the most prominent programming models for data parallelism in Apache Hadoop-, many researchers and developers have spent tremendous effort attempting to boost the computational speed and energy efficiency of MapReduce-based big data processing. However, the scalable and fault-tolerant nature of MapReduce introduces additional costs in disk IO and data transfer, caused by streaming intermediate outputs to disk. In light of these issues, many interesting research projects have been initiated with the goal of improving the compute speed and power efficiency of compute-intensive cloud computing workloads, several with the addition of discrete GPUs. In this work, we present a modified MapReduce approach focused on the iterative clustering algorithms in the Apache Mahout machine learning library that leverage the acceleration potential of the Intel integrated GPU in a multi-node cluster environment. The accelerated framework shows varying levels of speed-up (≈45x for Map tasks-only, ≈4.37x for the entire K-means clustering) as evaluated using the HiBench benchmark suite. Based on various experiments and in-depth analysis, we find that utilizing the integrated GPU via OpenCL offers significant performance and power efficiency gains over the original CPU based approach. Further analysis is also done to understand the correlations between compute, IO and power efficiency. As such, our results show that embracing the integrated GPU in the Hadoop MapReduce framework represents a promising advance in adding cost and energy efficient compute parallelism to a data parallel multinode environment.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184877","Big Data;GPGPU;Hadoop;Integrated Graphics;Machine Learning;Mahout;OpenCL","Acceleration;Graphics processing units;Java;Kernel;Optimization;Performance gain;Power demand","Big Data;graphics processing units;learning (artificial intelligence);parallel processing;pattern clustering;power aware computing","Apache Hadoop;Apache Mahout machine learning library;Hadoop MapReduce framework;HiBench benchmark suite;Intel integrated GPU;MapReduce-based big data processing;OpenCL;acceleration potential;computational speed;data parallel multinode environment;data parallelism;data transfer;discrete GPU;disk IO;energy efficiency;energy efficient compute parallelism;fault-tolerant nature;iterative clustering algorithms;multinode cluster environment;power efficient MapReduce workload acceleration;programming models","","0","","33","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"The role of data reduction for diagnosis of pathologies of the vertebral column by using supervised learning algorithms","T. J. Bah; B. Karlık","Department of Computer Engineering, The Engineering Faculty, Selcuk University, Konya, Turkey","2015 XVIII International Conference on Soft Computing and Measurements (SCM)","20150813","2015","","","163","166","Today in data mining research we are daily confronted with large amount of data. Most of the time, these data contain redundant and irrelevant data that it is important to extract before a learning task in order to get good accuracy. The fact that today's computers are more powerful does not solves the problems of this ever-growing data. It is therefore crucial to find techniques which allow handling these large databases often too big to be processed. Data reduction techniques are therefore a very important step to prepare the data before data mining and knowledge discovery. In this paper we present a comparative study on original and reduced data to see the role data reduction in a learning task. For this purpose, we used a medical dataset; especially a vertebral column pathologies database.","","Electronic:978-1-4673-6961-9; POD:978-1-4673-6962-6","10.1109/SCM.2015.7190443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7190443","Diagnosis;Lumbar Disc Hernia;Machine learning;Pattern Recognition;Spondylolisthesis","Accuracy;Artificial neural networks;Decision trees;Machine learning algorithms;Neurons;Pathology;Training","data mining;data reduction;learning (artificial intelligence);medical diagnostic computing","data mining;data reduction;knowledge discovery;learning task;medical dataset;pathology diagnosis;supervised learning algorithms;vertebral column pathologies database","","1","","20","","","19-21 May 2015","","IEEE","IEEE Conference Publications"
"Analysis of a modified Switchable Bayesian Learning Automaton for Cognitive Radio","H. Werker; S. Couturier; D. Rauschen; M. Adrat; M. Antweiler","Dept. of Communication Systems, Fraunhofer Institute for Communication, Information Processing, and Ergonomics FKIE, Wachtberg, Germany","2015 International Conference on Military Communications and Information Systems (ICMCIS)","20150716","2015","","","1","5","One of the most important topics in Cognitive Radio communications is that all communication partners change to the same frequency at the same time. A critical aspect of this process is a powerful channel selection algorithm, because each channel switching process requires resources and bears the risk of connection loss. Therefore, it is important to choose the channel that can be expected to be available for the longest time. This requires collecting information about all usable channels and developing a selection strategy. In [1] a machine learning approach, the Switchable Bayesian Learning Automaton (SBLA), is proposed for this task. Recently, we have implemented that algorithm to our cognitive radio simulator, which was presented in [2]. This paper describes the implementation, points out the advantages and drawbacks of the algorithm, and introduces improvements for its use in real-time systems.","","CD-ROM:978-8-3934-8485-0; Electronic:978-8-3934-8481-2; POD:978-1-4673-6815-5","10.1109/ICMCIS.2015.7158676","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158676","channel selection;cognitive radio;machine learning","Aging;Bayes methods;Cognitive radio;Learning automata;Real-time systems;Sensors;Switches","belief networks;cognitive radio;learning (artificial intelligence);learning automata;telecommunication computing;wireless channels","SBLA;channel selection algorithm;channel switching process;cognitive radio;machine learning approach;switchable Bayesian learning automaton analysis","","0","","5","","","18-19 May 2015","","IEEE","IEEE Conference Publications"
"Energy-efficient and high throughput sparse distributed memory architecture","M. Kang; E. P. Kim; M. s. Keel; N. R. Shanbhag","Dept. Electrical and Computer Engineering, University of Illinois at Urbana-Champaign","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","20150730","2015","","","2505","2508","This paper presents an energy-efficient VLSI implementation of Sparse Distributed Memory (SDM). High throughput and energy-efficient Hamming distance-based address decoder (CM-DEC) is proposed by employing compute memory [1], where computation is deeply embedded into a memory (SRAM). Hierarchical binary decision (HBD) is also proposed to enhance area- and energy-efficiency of read operation by minimizing data transfer. The SDM is employed as an auto-associative memory with four read iterations and 16×16 binary noisy input image with input error rates of 15%, 25%, and 30%. The proposed SDM achieves 39× smaller energy delay product with 14.5× and 2.7× reduced delay and energy, respectively as compared to conventional digital implementation of SDM in 45 nm SOI CMOS process with output error rate degradation less than 0.4%.","0271-4302;02714302","Electronic:978-1-4799-8391-9; POD:978-1-4799-8392-6","10.1109/ISCAS.2015.7169194","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169194","Associative memory;Compute memory;Machine learning;Pattern recognition;Sparse Distributed Memory","Arrays;Decoding;Error analysis;Radiation detectors;Random access memory;Throughput","CMOS memory circuits;SRAM chips;VLSI;binary decision diagrams;decoding;distributed memory systems;energy conservation;iterative methods;logic design;low-power electronics;silicon-on-insulator;synchronisation","CM-DEC;HBD;Hamming distance-based address decoder;SDM;SOI CMOS process;SRAM;VLSI;area-efficiency;autoassociative memory;binary noisy input image;data transfer;energy delay product;energy-efficiency;error rate degradation;hierarchical binary decision;read iterations;silicon-on-insulator;size 45 nm;sparse distributed memory architecture;very large scale integration","","1","","9","","","24-27 May 2015","","IEEE","IEEE Conference Publications"
"Artificial intelligence for designing user profiling system for cloud computing security: Experiment","Sahil; S. Sood; S. Mehmi; S. Dogra","Punjab Inst. of Technol., Kapurthala, India","2015 International Conference on Advances in Computer Engineering and Applications","20150723","2015","","","51","58","In Cloud Computing security, the existing mechanisms: Anti-virus programs, Authentications, Firewalls are not able to withstand the dynamic nature of threats. So, User Profiling System, which registers user's activities to analyze user's behavior, augments the security system to work in proactive and reactive manner and provides an enhanced security. This paper focuses on designing a User Profiling System for Cloud environment using Artificial Intelligence techniques and studies behavior (of User Profiling System) and proposes a new hybrid approach, which will deliver a comprehensive User Profiling System for Cloud Computing security.","","DVD:978-1-4673-6910-7; Electronic:978-1-4673-6911-4; POD:978-1-4673-6912-1","10.1109/ICACEA.2015.7164645","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164645","Artificial Intelligence;Artificial Neural Networks;Cloud Computing;Datacenters;Expert Systems;Genetics;Machine Learning;Multi-tenancy;Networking Systems;Pay-as-you-go Model","Artificial intelligence;Cloud computing;Computational modeling;Fuzzy logic;Fuzzy systems;Genetic algorithms;Security","artificial intelligence;authorisation;cloud computing;firewalls","antivirus programs;artificial intelligence techniques;authentications;cloud computing security;cloud environment;firewalls;proactive manner;reactive manner;user activities;user behavior;user profiling system","","0","","21","","","19-20 March 2015","","IEEE","IEEE Conference Publications"
"Using RFID to Detect Interactions in Ambient Assisted Living Environments","R. Parada; J. Melià-Seguí; M. Morenza-Cinos; A. Carreras; R. Pous","Universitat Pompeu Fabra","IEEE Intelligent Systems","20150714","2015","30","4","16","22","Elderly people with physical or cognitive disabilities such as visual impairment lack independence in their everyday activities, such as shopping in retail stores. Ambient assisted living (AAL) technologies can help. The authors of this article propose the use of RFID technologies to enable an AAL system in a retail store. This article presents an RFID-enabled intelligent system that can detect user-object interactions via a supervised machine learning algorithm. The authors' proposal detects RFID signals from different objects and classifies them as interacted or static objects through an automatic training system, achieving a detection accuracy over 86 percent.","1541-1672;15411672","","10.1109/MIS.2015.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156012","handicapped persons/special needs;intelligent systems;interactive systems;machine learning;sensor networks;wireless systems","Ambient networks;Assisted living;Internet of things;Navigation;Object recognition;Radio frequency;Radiofrequency identification","Internet of Things;computer based training;handicapped aids;learning (artificial intelligence);radiofrequency identification;ubiquitous computing","AAL system;Internet of Things;IoT;RFID enabled intelligent system;RFID signals;RFID technologies;ambient assisted living environments;automatic training system;cognitive disabilities;detect interactions;elderly people;retail store;retail stores;supervised machine learning algorithm;ubiquitous computing;user-object interactions","","6","","6","","","July-Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Multi-modal bike sensing for automatic geo-annotation geo-annotation of road/terrain type by participatory bike-sensing","S. Verstockt; V. Slavkovikj; P. De Potter; J. Slowack; R. Van de Walle","Multimedia Lab - ELIS Department, Ghent University - iMinds, Gaston Crommenlaan 8, bus 201, Ledeberg, Belgium","2013 International Conference on Signal Processing and Multimedia Applications (SIGMAP)","20150813","2013","","","39","49","This paper presents a novel road/terrain classification system based on the analysis of volunteered geographic information gathered by bikers. By ubiquitous collection of multi-sensor bike data, consisting of visual images, accelerometer information and GPS coordinates of the bikers' smartphone, the proposed system is able to distinguish between 6 different road/terrain types. In order to perform this classification task, the system employs a random decision forest (RDF), fed with a set of discriminative image and accelerometer features. For every instance of road (5 seconds), we extract these features and map the RDF result onto the GPS data of the users' smartphone. Finally, based on all the collected instances, we can annotate geographic maps with the road/terrain types and create a visualization of the route. The accuracy of the novel multi-modal bike sensing system for the 6-class road/terrain classification task is 92%. This result outperforms both the visual and accelerometer only classification, showing that the combination of both sensors is a win-win. For the 2-class on-road/off-road classification an accuracy of 97% is achieved, almost six percent above the state-of-the-art in this domain. Since these are the individual scores (measured on a single user/bike segment), the collaborative accuracy is expected to even further improve these results.","","Electronic:978-9-8975-8129-8; POD:978-1-4799-7136-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184668","Accelerometer Analysis;Bike-sensing;Geo-annotation;Image Classification;Machine Learning;Mobile Vision;Multi-modal Sensing","Accelerometers;Accuracy;Resource description framework;Roads;Sensors;Vibrations;Visualization","accelerometers;feature extraction;image classification;sensor fusion;smart phones;terrain mapping","2-class on-road-off-road classification;GPS coordinates;GPS data;RDF;accelerometer features;accelerometer information;bikers;discriminative image;geographic maps;multi-modal bike sensing system;multi-sensor bike data;random decision forest;road-terrain classification system;smartphone;ubiquitous collection;visual images;volunteered geographic information","","0","","23","","","29-31 July 2013","","IEEE","IEEE Conference Publications"
"Learning based prior for analyzer-based phase contrast image reconstruction","O. Caudevilla; J. G. Brankov","Illinois Institute of Technology, Chicago, IL","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1612","1615","Maximum a posteriori (MAP) method for image reconstruction is subjected to an appropriate selection of the prior distribution. In this paper, we introduce a new approach to estimate the prior distribution using a machine learning scheme based on Relevance Vector Machine (RVM). The RVM prior is applied to the Analyzer-based Imaging (ABI) reconstruction problem. ABI is a technique capable of measuring very subtle X-ray deflection and scatter phenomena when passing through an imaged object producing three parametric images (Absorption, Refraction and ultra-small angle scatter USAXS). The need of a quasi-monochromatic and highly collimated beam causes an extremely low photon count in the ABI systems detector, which leads to noisy reconstructions. Here we demonstrate the use of RVM priors to improve the resulting ABI images.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164189","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164189","Analyzer-based phase contrast imaging;Bayesian reconstruction;Gaussian process;machine-learning;multiple image radiography;phase-sensitive imaging;prior estimation;relevance vector machine","Absorption;Bayes methods;Estimation;Image reconstruction;Imaging;Support vector machines;Training","diagnostic radiography;image reconstruction;learning (artificial intelligence);maximum likelihood estimation;medical image processing;photon counting","ABI system detector;X-ray absorption;X-ray deflection;X-ray refraction;analyzer-based phase contrast image reconstruction;extremely low photon count;highly collimated beam;learning based prior distribution;machine learning scheme;maximum a posteriori method;quasimonochromatic beam;relevance vector machine;ultrasmall angle scatter","","1","","12","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Combining nearest neighbour classifiers based on small subsamples for big data analytics","B. Krawczyk; M. Woźniak","Department of Systems and Computer Networks, Wroc&#x0142;aw University of Technology, Poland","2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)","20150806","2015","","","311","316","Contemporary machine learning systems must be able to deal with ever-growing volumes of data. However, most of the canonical classifiers are not well-suited for big data analytics. This is especially vivid in case of distance-based classifiers, where their classification time is prohibitive. Recently, many methods for adapting nearest neighbour classifier for big data were proposed. We investigate simple, yet efficient technique based on random under-sampling of the dataset. As we deal with stationary data, one may assume that a subset of objects will sufficiently capture the properties of given dataset. We propose to build distance-based classifiers on the basis of very small subsamples and then combine them into an ensemble. With this, one does not need to aggregate datasets, only local decisions of classifiers. On the basis of experimental results we show that such an approach can return comparable results to nearest neighbour classifier over the entire dataset, but with a significantly reduced classification time. We investigate the number of sub-samples (ensemble members), that are required for capturing the properties of each dataset. Finally, we propose to apply our sub-sampling based ensemble in a distributed environment, which allows for a further reduction of the computational complexity of nearest neighbour rule for big data.","","Electronic:978-1-4799-8322-3; POD:978-1-4799-8323-0","10.1109/CYBConf.2015.7175952","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175952","big data;classifier ensemble;distributed classifier;machine learning;parallel computing","Accuracy;Aggregates;Big data;Computer architecture;Couplings;Prototypes;Training","Big Data;computational complexity;data analysis;learning (artificial intelligence);pattern classification;sampling methods","aggregate dataset;big data analytics;classification time;classifier decision;computational complexity;data volume;dataset random undersampling;distance-based classifier;distributed environment;ensemble member;machine learning system;nearest neighbour classifier;stationary data","","0","","19","","","24-26 June 2015","","IEEE","IEEE Conference Publications"
"Electricity demand profile prediction based on household characteristics","J. L. Viegas; S. M. Vieira; J. M. C. Sousa; R. Melício; V. M. F. Mendes","IDMEC, LAETA, Instituto, Superior T&#x00E9;cnico, Universidade de Lisboa, Portugal","2015 12th International Conference on the European Energy Market (EEM)","20150824","2015","","","1","5","This work proposes a methodology for predicting the typical daily load profile of electricity usage based on static data obtained from surveys. The methodology intends to: (1) determine consumer segments based on the metering data using the k-means clustering algorithm, (2) correlate survey data to the segments, and (3) develop statistical and machine learning classification models to predict the demand profile of the consumers. The developed classification models contribute to make the study and planning of demand side management programs easier, provide means for studying the impact of alternative tariff setting methods and generate useful knowledge for policy makers.","2165-4077;21654077","Electronic:978-1-4673-6692-2; POD:978-1-4673-6693-9; USB:978-1-4673-6691-5","10.1109/EEM.2015.7216746","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7216746","Data mining;Household energy consumption;Machine learning;Segmentation;Smart meter data","Correlation;Data mining;Education;Load modeling;Predictive models;Support vector machines;Water heating","buildings (structures);demand forecasting;demand side management;learning (artificial intelligence);statistical analysis;tariffs","daily load profile;demand side management programs;electricity demand profile prediction;electricity usage;k-means clustering algorithm;machine learning classification models;metering data;policy makers;statistical classification models;tariff setting methods","","1","","31","","","19-22 May 2015","","IEEE","IEEE Conference Publications"
"Broadening the Search in Search-Based Software Testing: It Need Not Be Evolutionary","R. Feldt; S. Poulding","Dept. of Software Eng., Belkinge Inst. of Technol., Karlskrona, Sweden","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","20150803","2015","","","1","7","Search-based software testing (SBST) can potentially help software practitioners create better test suites using less time and resources by employing powerful methods for search and optimization. However, research on SBST has typically focused on only a few search approaches and basic techniques. A majority of publications in recent years use some form of evolutionary search, typically a genetic algorithm, or, alternatively, some other optimization algorithm inspired from nature. This paper argues that SBST researchers and practitioners should not restrict themselves to a limited choice of search algorithms or approaches to optimization. To support our argument we empirically investigate three alternatives and compare them to the de facto SBST standards in regards to performance, resource efficiency and robustness on different test data generation problems: classic algorithms from the optimization literature, bayesian optimization with gaussian processes from machine learning, and nested monte carlo search from game playing / reinforcement learning. In all cases we show comparable and sometimes better performance than the current state-of-the-SBST-art. We conclude that SBST researchers should consider a more general set of solution approaches, more consider combinations and hybrid solutions and look to other areas for how to develop the field.","","Electronic:978-1-4673-7079-0; POD:978-1-4673-7080-6","10.1109/SBST.2015.8","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173581","Machine learning;Operations research;Reinforcement learning;Search-based software testing","Generators;Libraries;Machine learning algorithms;Monte Carlo methods;Optimization;Search problems;Testing","Bayes methods;Gaussian processes;Monte Carlo methods;evolutionary computation;genetic algorithms;learning (artificial intelligence);program testing","SBST;bayesian optimization;data generation problem;evolutionary search;gaussian process;genetic algorithm;machine learning;nested Monte Carlo search;optimization;reinforcement learning;search algorithms;search-based software testing","","3","","25","","","18-19 May 2015","","IEEE","IEEE Conference Publications"
"An intelligent system for diabetes prediction","Z. Tafa; N. Pervetica; B. Karahoda","Dep. of Computer Science, University for Business and Technology, Prishtina, Kosovo","2015 4th Mediterranean Conference on Embedded Computing (MECO)","20150810","2015","","","378","382","With the emerging increase of diabetes, that recently affects around 346 million people, of which more than one-third go undetected in early stage, a strong need for supporting the medical decision-making process is generated. A number of researches have focused either in using one of the algorithms or in the comparisons of the performances of algorithms on a given, usually predefined and static datasets that are accessible through the Internet. This paper focuses on the joint implementation of the support vector machine (SVM) and Naïve Bayes statistical modeling, in the dataset acquired from the medical examinations of 402 patients, in order to improve the computer-supported diagnosis reliability. The dataset contains some attributes that have not been previously used in computer-based evaluations. The results show that the joint implementation of two algorithms improves significantly the overall reliability of the system outcome, which is crucial in the computer-supported diabetes diagnostic process.","2377-5475;23775475","CD-ROM:978-1-4799-8998-0; Electronic:978-1-4799-1976-5; POD:978-1-4799-1977-2","10.1109/MECO.2015.7181948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181948","Naïve Bayes;algorithms;diabetes;joint implementation;machine learning;support vector machine","Accuracy;Algorithm design and analysis;Classification algorithms;Diabetes;Joints;Machine learning algorithms;Support vector machines","Bayes methods;decision support systems;diseases;learning (artificial intelligence);medical information systems;support vector machines","SVM;computer-supported diabetes diagnostic process;computer-supported diagnosis reliability improvement;diabetes prediction;intelligent system;medical decision-making process;medical examination;naïve Bayes statistical modeling;support vector machine","","0","","14","","","14-18 June 2015","","IEEE","IEEE Conference Publications"
"Fast algorithm for neural network reconstruction","S. Bittner; S. Chen; J. Kovačević","Dept. of ECE, Carnegie Mellon University, Pittsburgh, PA, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","866","869","We propose an efficient and accurate way of predicting the connectivity of neural networks in the brain represented by simulated calcium fluorescence data. Classical methods to neural network reconstruction compute a connectivity matrix whose entries are pairwise likelihoods of directed excitatory connections based on time-series signals of each pair of neurons. Our method uses only a fraction of this computation to achieve equal or better performance. The proposed method is based on matrix completion and a local thresholding technique. By computing a subset of the total entries in the connectivity matrix, we use matrix completion to determine the rest of the connection likelihoods, and apply a local threshold to identify which directed connections exist in the underlying network. We validate the proposed method on a simulated calcium fluorescence dataset. The proposed method outperforms the classical one with 20% of the computation.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164008","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164008","connectivity analysis;machine learning;nerves","Biological neural networks;Calcium;Entropy;Imaging;Neurons;Standards","biomedical optical imaging;brain;fluorescence;image reconstruction;matrix algebra;medical image processing;neural nets;neurophysiology;time series","brain;connectivity matrix;fast algorithm;neural network reconstruction;simulated calcium fluorescence dataset;time-series signals","","0","","6","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"CA-SVM: Communication-Avoiding Support Vector Machines on Distributed Systems","Y. You; J. Demmel; K. Czechowski; L. Song; R. Vuduc","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","2015 IEEE International Parallel and Distributed Processing Symposium","20150720","2015","","","847","859","We consider the problem of how to design and implement communication-efficient versions of parallel support vector machines, a widely used classifier in statistical machine learning, for distributed memory clusters and supercomputers. The main computational bottleneck is the training phase, in which a statistical model is built from an input data set. Prior to our study, the parallel is efficiency of a state-of-the-art implementation scaled as W = Omega(P<sup>3</sup>), where W is the problem size and P the number of processors, this scaling is worse than even a one-dimensional block row dense matrix vector multiplication, which has W = Omega(P<sup>2</sup>). This study considers a series of algorithmic refinements, leading ultimately to a Communication-Avoiding SVM (CASVM) method that improves the is efficiency to nearly W = Omega(P). We evaluate these methods on 96 to 1536 processors, and show average speedups of 3 - 16× (7× on average) over Dis-SMO, and a 95% weak-scaling efficiency on six real world datasets, with only modest losses in overall classification accuracy. The source code can be downloaded at https://github.com/fastalgo/casvm.","1530-2075;15302075","Electronic:978-1-4799-8649-1; POD:978-1-4799-8650-7","10.1109/IPDPS.2015.117","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161571","communication avoidance;distributed memory algorithms;statistical machine learning","Accuracy;Kernel;Mathematical model;Partitioning algorithms;Program processors;Support vector machines;Training","learning (artificial intelligence);parallel machines;parallel processing;statistical analysis;support vector machines","CA-SVM;communication-avoiding SVM method;communication-avoiding support vector machines;communication-efficient versions;dense matrix vector multiplication;distributed memory clusters;distributed systems;parallel support vector machines;statistical machine learning;statistical model;supercomputers","","3","","31","","","25-29 May 2015","","IEEE","IEEE Conference Publications"
"Efficient obstructive sleep apnea classification based on EEG signals","W. S. Almuhammadi; K. A. I. Aboalayon; M. Faezipour","Department of Computer Science and Engineering, University of Bridgeport, CT 06604, USA","2015 Long Island Systems, Applications and Technology","20150716","2015","","","1","6","Nowadays, analyzing EEG signals has made it easy to diagnose many sleep-related breathing disorders such as Obstructive Sleep Apnea (OSA), which is a potentially serious sleep disorder that affects the quality of human life. This paper introduces an efficient methodology that could be implemented in hardware to differentiate OSA patients from normal controls, based on the Electroencephalogram (EEG) signals. For this purpose, first, the EEG recorded datasets that were obtained from the Phsyionet website are filtered and decomposed into delta, theta alpha, beta and gamma sub-bands using Infinite Impulse Response (IIR) Butterworth band-pass filters. Second, descriptive features such as energy and variance are extracted from each frequency band that are used as input parameters for classification. Finally, several machine learning algorithms including Support Vector Machines (SVM), Artificial Neural Networks (ANN), Linear Discriminant Analysis (LDA) and Naive Bayes (NB) are employed in order to identify if the OSA exists or not, according to the objective of this study. The results that are obtained from these classifiers are then compared in terms of accuracy, sensitivity and specificity. The experimental results show that the SVM attained the best classification accuracy of 97.14% as compared to the others.","","Electronic:978-1-4799-8643-9; POD:978-1-4799-8644-6","10.1109/LISAT.2015.7160186","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160186","EEG signals;EEG sub-bands;Obstructive Sleep Apnea (OSA);classification;machine learning algorithms","Accuracy;Artificial neural networks;Electroencephalography;Feature extraction;Sleep apnea;Support vector machines","Butterworth filters;IIR filters;band-pass filters;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;signal classification;statistical analysis;support vector machines","ANN;EEG signal analysis;IIR Butterworth band-pass filters;LDA;NB;OSA;SVM;alpha subband;artificial neural networks;beta subband;delta subband;electroencephalography;energy feature;gamma subband;infinite impulse response;linear discriminant analysis;machine learning algorithms;naive Bayes;obstructive sleep apnea classification;sleep-related breathing disorders;support vector machines;theta subband;variance feature","","0","","26","","","1-1 May 2015","","IEEE","IEEE Conference Publications"
"Approximate computing and the quest for computing efficiency","S. Venkataramani; S. T. Chakradhar; K. Roy; A. Raghunathan","School of Electrical and Computer Engineering, Purdue University, USA","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20150727","2015","","","1","6","Diminishing benefits from technology scaling have pushed designers to look for new sources of computing efficiency. Multicores and heterogeneous accelerator-based architectures are a by-product of this quest to obtain improvements in the performance of computing platforms at similar or lower power budgets. In light of the need for new innovations to sustain these improvements, we discuss approximate computing, a field that has attracted considerable interest over the last decade. While the core principles of approximate computing - computing efficiently by producing results that are good enough or of sufficient quality - are not new and are shared by many fields from algorithm design to networks and distributed systems, recent e?orts have seen a percolation of these principles to all layers of the computing stack, including circuits, architecture, and software. Approximate computing techniques have also evolved from ad hoc and application-specific to more broadly applicable, supported by systematic design methodologies. Finally, the emergence of workloads such as recognition, mining, search, data analytics, inference and vision are greatly increasing the opportunities for approximate computing. We describe the vision and key principles that have guided our work in this area, and outline a holistic cross-layer framework for approximate computing.","0738-100X;0738100X","Electronic:978-1-4799-8052-9; POD:978-1-4799-8053-6","10.1145/2744769.2744904","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167251","Approximate Computing;Energy Efficiency;Input Adaptive Systems;Machine Learning Classifiers","Accuracy;Algorithm design and analysis;Complexity theory;Computational modeling;Data models;Support vector machines;Training","computer architecture;distributed processing;multiprocessing systems;performance evaluation;power aware computing","approximate computing technique;computing efficiency;computing stack;distributed system;heterogeneous accelerator-based architecture;multicore accelerator-based architecture;power budget;systematic design methodology","","5","","54","","","8-12 June 2015","","IEEE","IEEE Conference Publications"
"Poker learner: Players modeling through data-mining","N. Silva; L. P. Reis","EEUM/DSI - Escola de Engenharia da Universidade do Minho, Departamento de Sistemas de Informa&#x00E7;&#x00E3;o e Centro, ALGORITMI, Guimar&#x00E3;es, Portugal","2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","20150730","2015","","","1","6","In recent years the game of poker has created a high interest on researchers from the artificial intelligence area. Unlike board games, poker is an incomplete information game becoming a very complex game for a virtual agent. The main objective of this work is to create a data model enabling to apply data mining techniques to obtain a poker player model (pre-flop stage). To do that we used a database from a professional poker player where the data is stored in text files. The work used CRISP-DM, performing its stages. As ETL (Extract, Transform and Load) tool Talend was used and for running the data mining techniques Weka was used. As a final result, a player model was achieved with a very good ROC curve. This result, enable us to conclude that the approach is adequate for creating complete poker player models.","2166-0727;21660727","Electronic:978-9-8998-4345-5; POD:978-1-4799-8330-8","10.1109/CISTI.2015.7170624","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170624","CRISP-DM;Weka;data mining;games;machine learning;poker;talend","Adaptation models;Computational modeling;Data mining;Data models;Games;Load modeling;Transforms","computer games;data mining;data models;learning (artificial intelligence)","CRISP-DM;ETL;ROC curve;Talend;Weka;data mining techniques;data model;extract-transform-and-load tool;poker learner;poker player model","","0","","14","","","17-20 June 2015","","IEEE","IEEE Conference Publications"
"Audiovisual Fusion: Challenges and New Approaches","A. K. Katsaggelos; S. Bahaadini; R. Molina","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA","Proceedings of the IEEE","20150820","2015","103","9","1635","1653","In this paper, we review recent results on audiovisual (AV) fusion. We also discuss some of the challenges and report on approaches to address them. One important issue in AV fusion is how the modalities interact and influence each other. This review will address this question in the context of AV speech processing, and especially speech recognition, where one of the issues is that the modalities both interact but also sometimes appear to desynchronize from each other. An additional issue that sometimes arises is that one of the modalities may be missing at test time, although it is available at training time; for example, it may be possible to collect AV training data while only having access to audio at test time. We will review approaches to address this issue from the area of multiview learning, where the goal is to learn a model or representation for each of the modalities separately while taking advantage of the rich multimodal training data. In addition to multiview learning, we also discuss the recent application of deep learning (DL) toward AV fusion. We finally draw conclusions and offer our assessment of the future in the area of AV fusion.","0018-9219;00189219","","10.1109/JPROC.2015.2459017","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194741","Audiovisual (AV) fusion;deep learning (DL);machine learning;multimodal analysis;multiview learning;stream asynchrony","Data integration;Feature extraction;Hidden Markov models;Kalman filters;Multimodal sensors;Speech processing;Streaming media;Visualization","learning (artificial intelligence);speech recognition","AV speech processing;audiovisual fusion;deep learning;multiview learning;speech recognition","","1","","171","","20150813","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Improving Small-Cell Performance Through Switched Multielement Antenna Systems in Heterogeneous Networks","R. Razavi; L. Ho; H. Claussen; D. López-Pérez","Bell Labs., Alcatel-Lucent, Dublin, Ireland","IEEE Transactions on Vehicular Technology","20150714","2015","64","7","3140","3151","This paper introduces an effective yet simple and practical solution to improve small-cell performance in heterogeneous networks (HetNets). The proposed solution is based on deploying a switched multielement antenna (MEA) system capable of generating a variety of antenna patterns at small-cell base stations (BSs). Then, antenna patterns are assigned to user equipment (UE) in a dynamic basis. The antenna pattern selection for each UE is considered to be a supervised machine learning classification problem, in which the small-cell BS seek to find the optimal antenna pattern to serve each UE according to its measurement reports (i.e., UE radio-frequency fingerprint). Simulation results confirm the feasibility of the proposed approach, despite potential inaccuracies in UE measurement reports. Compared with the existing solutions comprising a single omnidirectional antenna (ODA), the proposed approach results in a 68% additional network-wide capacity increase. In addition, a technoeconomic analysis is presented in this paper, indicating the economic advantages of deploying the proposed scheme.","0018-9545;00189545","","10.1109/TVT.2014.2348319","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6879477","Classification;heterogeneous networks (HetNets);interference management;machine learning;multielement antenna (MEA);picocells;radio frequency (RF) fingerprint;small cells","Antenna measurements;Directive antennas;Interference;Radio frequency;Switches;Training","antenna arrays;antenna radiation patterns;cellular radio;learning (artificial intelligence);telecommunication computing","HetNet;antenna pattern selection;heterogeneous network;small-cell base station;small-cell performance improvement;supervised machine learning classification problem;switched multielement antenna system;user equipment","","3","","25","","20140818","July 2015","","IEEE","IEEE Journals & Magazines"
"Hair region localization with optical imaging for guided laser hair removal","M. Avşar; I. S. Yetik","Department of Electrical and Electronics Engineering, TOBB University of Economics and Technology, Ankara 06560, Turkey","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1411","1414","Laser hair removal is a popular nonsurgical aesthetic operation, where the aim is to remove unwanted hair permanently by damaging the hair follicle and shaft thermally. However, laser affects the superficial skin layers in addition to hair follicles causing health risks. Side effects of laser-assisted hair removal can be minimized by directing the laser beam only to the detected hair regions. This study proposes a feature-based hair region localization method using machine learning techniques, a first in this area. Features with low computational complexity have been proposed in order to discriminate hair and skin regions. Hair and skin region classification performances of different machine learning techniques have been applied and compared. Quantitative and visual results obtained from the proposed technique showed success in the detection of hair and skin regions. We concluded that the proposed method can be used in real-time guided laser hair removal devices.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164140","Hair region detection;guided laser hair removal device;laser hair removal;machine learning","Error analysis;Hair;Laser beams;Lasers;Sensitivity;Skin;Support vector machines","biological effects of laser radiation;biomedical optical imaging;feature extraction;image classification;laser applications in medicine;learning (artificial intelligence);medical image processing;skin","feature-based hair region localization method;guided laser hair removal device;hair follicle damage;hair region classification;hair region detection;health risk;laser beam;laser-assisted hair removal effect;low computational complexity;machine learning technique;nonsurgical aesthetic operation;optical imaging;skin region classification;skin region detection;superficial skin layer;unwanted hair removal","","0","","7","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Trusted computation with an adversarial cloud","S. D. Bopardikar; A. Speranzon; C. Langbort","Systems Department, United Technologies Research Center, USA","2015 American Control Conference (ACC)","20150730","2015","","","2445","2452","We consider the problem of computation in a cloud environment where either the data or the computation may be corrupted by an adversary. We assume that a small fraction of the data is stored locally at a client during the upload process to the cloud and that this data is trustworthy. We formulate the problem within a game theoretic framework where the client needs to decide an optimal fusion strategy using both non-trusted information from the cloud and local trusted data, given that the adversary on the cloud is trying to deceive the client by biasing the output to a different value/set of values. We adopt an Iterated Best Response (IBR) scheme for each player to update its action based on the opponent's announced computation. At each iteration, the cloud reveals its output to the client, who then computes the best response as a linear combination of its private local estimate and of the untrusted cloud output. We characterize equilibrium conditions for both the scalar and vector cases of the computed value of interest. Necessary and sufficient conditions for convergence for the IBR are derived and insightful geometric interpretations of such conditions is discussed for the vector case. Numerical results are presented showing the convergence conditions are relatively tight.","0743-1619;07431619","CD-ROM:978-1-4799-8685-9; Electronic:978-1-4799-8684-2; POD:978-1-4799-1773-0","10.1109/ACC.2015.7171099","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7171099","Adversarial Machine Learning;Equilibrium;Game theory;Trusted Computation","Algorithm design and analysis;Convergence;Cost function;Games;Protocols;Random variables;Security","cloud computing;game theory;geometry;iterative methods;optimisation;security of data;trusted computing;vectors","IBR scheme;adversarial cloud computing;game theoretic framework;geometric interpretation;iterated best response;optimal fusion strategy;trusted computation;vector case","","0","","19","","","1-3 July 2015","","IEEE","IEEE Conference Publications"
"Effective Classification Using a Small Training Set Based on Discretization and Statistical Analysis","R. Bruni; G. Bianchi","Department of Computer, Control and Management Engineering (DIAG), University of Rome &#8220;Sapienza,&#8221; Via Ariosto 25, Roma, Italy","IEEE Transactions on Knowledge and Data Engineering","20150804","2015","27","9","2349","2361","This work deals with the problem of producing a fast and accurate data classification, learning it from a possibly small set of records that are already classified. The proposed approach is based on the framework of the so-called Logical Analysis of Data (LAD), but enriched with information obtained from statistical considerations on the data. A number of discrete optimization problems are solved in the different steps of the procedure, but their computational demand can be controlled. The accuracy of the proposed approach is compared to that of the standard LAD algorithm, of support vector machines and of label propagation algorithm on publicly available datasets of the UCI repository. Encouraging results are obtained and discussed.","1041-4347;10414347","","10.1109/TKDE.2015.2416727","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069208","Classification Algorithms;Classification algorithms;Data Mining;Discrete Mathematics;Machine Learning;Optimization;data mining;discrete mathematics;machine learning;optimization","Accuracy;Decision trees;Machine learning algorithms;Prediction algorithms;Standards;Support vector machines;Training","classification;data analysis;formal logic;learning (artificial intelligence);statistical analysis;support vector machines","LAD algorithm;UCI repository;data classification;discretization;label propagation algorithm;learning;logical analysis of data;small training set;statistical analysis;support vector machines","","3","","42","","20150326","Sept. 1 2015","","IEEE","IEEE Journals & Magazines"
"Nonnegative matrix factorization with gradient vertex pursuit","D. N. Tran; T. Xiong; S. P. Chin; T. D. Tran","The Johns Hopkins University, Department of ECE, 3400 N. Charles St., Baltimore, MD 21218, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","2125","2129","Nonnegative Matrix Factorization (NMF), defined as factorizing a nonnegative matrix into two nonnegative factor matrices, is a particularly important problem in machine learning. Unfortunately, it is also ill-posed and NP-hard. We propose a fast, robust, and provably correct algorithm, namely Gradient Vertex Pursuit (GVP), for solving a well-defined instance of the problem which results in a unique solution: there exists a polytope, whose vertices consist of a few columns of the original matrix, covering the entire set of remaining columns. Our algorithm is greedy: it detects, at each iteration, a correct vertex until the entire polytope is identified. We evaluate the proposed algorithm on both synthetic and real hyperspectral data, and show its superior performance compared with other state-of-the-art greedy pursuit algorithms.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178346","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178346","Gradient Vertex Pursuit;Machine learning;greedy pursuit;nonnegative matrix factorization","Algorithm design and analysis;Approximation algorithms;Hyperspectral imaging;Optimization;Pursuit algorithms;Robustness;Signal processing algorithms","acoustic signal processing;learning (artificial intelligence);vertex functions","gradient vertex pursuit;greedy pursuit algorithms;machine learning;nonnegative matrix factorization;polytope;real hyperspectral data;synthetic hyperspectral data","","0","","19","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Estimating expected error rates of random forest classifiers: A comparison of cross-validation and bootstrap","M. Ljumović; M. Klar","University of Montenegro - Faculty of Electrical Engineering Podgorica, Podgorica, Montenegro","2015 4th Mediterranean Conference on Embedded Computing (MECO)","20150810","2015","","","212","215","Statistical learning has recently seen an expansion of applications in different areas of science, finance and industry, as it plays a great role within the fields of statistics, data mining and artificial intelligence. Hence, it intersects with areas of engineering and other disciplines as well. It is used for both regression and classification problems. Solving these problems usually involves building/training a model/classifier and validating its performance for a given task. In this paper we compare two resampling methods for assessment of a random forest classifier: k-fold cross-validation and bootstrap. We use these methods to estimate the generalization error and to create learning curves. Both methods yield similar results on our data. The most important requirement for good generalization error estimates of either method is that the used data sample (i.e. the training dataset) represents the unknown true distribution of the data. This requirement cannot always be met in practice and results of resampling methods have to be interpreted with care if it is violated.","2377-5475;23775475","CD-ROM:978-1-4799-8998-0; Electronic:978-1-4799-1976-5; POD:978-1-4799-1977-2","10.1109/MECO.2015.7181905","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181905","bootstrap;classifier;cross-validation;learning curves;machine learning;resampling methods;statistical learning","Buildings;Embedded computing;Error analysis;Statistical learning;Training;Vegetation","generalisation (artificial intelligence);learning (artificial intelligence);pattern classification;random processes;sampling methods","bootstrap;classification problem;expected error rate estimation;generalization error estimation;k-fold cross-validation;learning curves;random forest classifiers;regression problem;resampling methods;statistical learning","","0","","6","","","14-18 June 2015","","IEEE","IEEE Conference Publications"
"An autonomous model to enforce security policies based on user's behavior","K. Ghazinour; M. Ghayoumi","Department of Computer Science, Kent State University, Ohio, USA","2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)","20150727","2015","","","95","99","To protect user's information, computer systems utilize access control models. These models are supported by a set of policies defined by security administrators in the environment where the organization is active. In previous studies it has been shown that building a user interface that dynamically changes with the security policies defined for each user is a cumbersome task. This work is a further expansion of an improved dynamic model that adjusts users' security policies based on the level of trust that they hold. We use machine learning beside the trust manager component that helps the system to adapt itself, learn from the user's behavior and recognize access patterns based on the similar access requests and not only limit the illegitimate access, but also predict and prevent potential malicious and questionable accesses.","","Electronic:978-1-4799-8679-8; POD:978-1-4799-8680-4","10.1109/ICIS.2015.7166576","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166576","Access Policies;Database;Dynamic Model;Machine Learning;Security Policies;Trust Model","Access control;Business;Computational modeling;Databases;History;User interfaces","learning (artificial intelligence);trusted computing;user interfaces","access control models;access requests;autonomous model;security policy;trust level;trust manager component;user behavior;user information protection;user interface","","0","","14","","","June 28 2015-July 1 2015","","IEEE","IEEE Conference Publications"
"Activity Recognition Based on Streaming Sensor Data for Assisted Living in Smart Homes","B. Chen; Z. Fan; F. Cao","Telecommun. Res. Lab., Toshiba Res. Eur. Ltd., Bristol, UK","2015 International Conference on Intelligent Environments","20150813","2015","","","124","127","This paper proposes an activity recognition method for streaming sensor data in smart homes. Experiments on real datasets have been carried out to show the effectiveness of the proposed approach. It has great potential for applications in e-health and assisted living.","","Electronic:978-1-4673-6654-0; POD:978-1-4673-6655-7","10.1109/IE.2015.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194280","activity recognition;assisted living;machine learning","Accuracy;Assisted living;Data models;Predictive models;Smart homes;Support vector machines;Training","assisted living;home automation","activity recognition;activity recognition method;assisted living;e-health;sensor data streaming;smart homes","","1","","8","","","15-17 July 2015","","IEEE","IEEE Conference Publications"
"Systematic mapping study of missing values techniques in software engineering data","A. Idri; I. Abnane; A. Abran","Software Project Management Research Team, ENSIAS, Mohamed V University, Rabat, Morocco","2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","20150806","2015","","","1","8","Missing Values (MV) present a serious problem facing research in software engineering (SE) which is mainly based on statistical and/or data mining analysis of SE data. The simple method of dealing with MV is to ignore data with missing observations. This leads to losing valuable information and then obtaining biased results. Therefore, various techniques have been developed to deal adequately with MV, especially those based on imputation methods. In this paper, a systematic mapping study was carried out to summarize the existing techniques dealing with MV in SE datasets and to classify the selected studies according to six classification criteria: research type, research approach, MV technique, MV type, data types and MV objective. Publication channels and trends were also identified. As results, 35 papers concerning MV treatments of SE data were selected. This study shows an increasing interest in machine learning (ML) techniques especially the K-nearest neighbor algorithm (KNN) to deal with MV in SE datasets and found that most of the MV techniques are used to serve software development effort estimation techniques.","","Electronic:978-1-4799-8676-7; POD:978-1-4799-8677-4","10.1109/SNPD.2015.7176280","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176280","Machine learning;Missing values;Software engineering data;Systematic mapping study","Conferences;Data mining;Market research;Quality assessment;Software;Software engineering;Systematics","data mining;learning (artificial intelligence);software engineering","K-nearest neighbor algorithm;KNN;MV objective;MV technique;MV type;SE datasets;data mining analysis;data types;machine learning techniques;missing values techniques;research approach;research type;software development effort estimation techniques;software engineering data;systematic mapping study","","0","","40","","","1-3 June 2015","","IEEE","IEEE Conference Publications"
"Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns","M. Hashimoto; M. Terai; T. Maeda; K. Minami","RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","13","23","To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180063","abstract syntax tree differencing;application performance tuning;large-scale scientific computing;machine learning;semantic web","Arrays;Data mining;History;Kernel;Ontologies;Phylogeny;Tuning","data mining;optimisation;program compilers","EBT;combinatorial explosion;compiler options;data mining;evidence based performance tuning;large-scale scientific applications;pattern identification tool;performance tuning history;predicting effective optimization patterns;program parameters;program transformation;scientific applications;source code snapshots;syntactic structure;version control systems","","0","","29","","","16-17 May 2015","","IEEE","IEEE Conference Publications"
"Self Learning Network Traffic Classification","Vandana M; S. Manmadhan","Computer Science & Engineering, NSS College of Engineering, Palakkad Kerala, India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)","20150813","2015","","","1","5","Network management is part of traffic engineering and security. The current solutions - Deep Packet Inspection (DPI) and statistical classification, rely on the availability of a training set. In case of these there is a cumbersome need to regularly update the signatures. Further their visibility is limited to classes the classifier has been trained for. Unsupervised algorithms have been envisioned as a an alternative to automatically identify classes of traffic. To address these issues Self Learning Network Traffic Classification is proposed. It uses unsupervised algorithms along with an adaptive seeding approach to automatically lets classes of traffic to emerge, making them identified and labelled. Unlike traditional classifiers, there is no need of a-priori knowledge of signatures nor a training set to extract the signatures. Instead, Self Learning Network Traffic Classification automatically groups flows into pure (or homogeneous) clusters using simple statistical features. This label assignment (which is still based on some manual intervention) ensures that class labels can be easily discovered. Furthermore, Self Learning Network Traffic Classification uses an iterative seeding approach which will boost its ability to cope with new protocols and applications. Unlike state-of-art classifiers, the biggest advantage of Self Learning Network Traffic Classification is its ability to discover new protocols and applications in an almost automated fashion.","","Electronic:978-1-4799-6818-3; POD:978-1-4799-6819-0","10.1109/ICIIECS.2015.7193038","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193038","Traffic classification;clustering;self-seeding;unsupervised machine learning","Classification algorithms;Clustering algorithms;Filtering;IP networks;Ports (Computers);Protocols;Telecommunication traffic","pattern classification;statistical analysis;traffic engineering computing;unsupervised learning","DPI;adaptive seeding approach;deep packet inspection;network management;protocols;self learning network traffic classification;statistical classification;traffic engineering;unsupervised machine learning","","1","","9","","","19-20 March 2015","","IEEE","IEEE Conference Publications"
"A fast and fully automated approach to segment optic nerves on MRI and its application to radiosurgery","J. Dolz; H. A. Leroy; N. Reyns; L. Massoptier; M. Vermandel","AQUILAB, Loos-les-Lille, France","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1102","1105","Delineating critical structures of the brain is required for advanced radiotherapy technologies to determine whether the dose from the proposed treatment will impair the functionality of those structures. Employing an automatic segmentation computer module in the radiation oncology treatment planning process has the potential to significantly increase the efficiency, cost-effectiveness, and, ultimately, clinical outcome of patients undergoing radiation therapy. Atlas-based segmentation has shown to be a suitable tool for the segmentation of large structures such as the brainstem or the cerebellum. However, smaller structures such as the optic nerves are more difficult to segment. In this work, we present a novel approach to automatically segment the optic nerves, which is based on Support Vector Machines (SVM). Compared to state of the art methods, the presented method obtained a better performance in regards to accuracy, robustness and processing time, being a suitable trade-off between these three factors.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164064","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164064","MRI;machine learning;optic nerves segmentation;radiosurgery;support vector machines","Biomedical optical imaging;Brain;Image segmentation;Optical imaging;Support vector machines;Three-dimensional displays","biomedical MRI;brain;cancer;eye;image segmentation;medical image processing;neurophysiology;planning;radiation therapy;support vector machines","MRI;SVM;advanced radiotherapy technology;atlas-based segmentation;automatic segmentation computer module;brain structure functionality impairment;brainstem segmentation;cerebellum segmentation;clinical outcome;critical brain structure delineation;fast optic nerve segmentation;fully automated optic nerve segmentation;optic nerve segmentation accuracy;optic nerve segmentation robustness;processing time;proposed treatment dose;radiation oncology treatment planning;radiosurgery application;radiotherapy cost effectiveness;radiotherapy efficiency;support vector machine","","1","","16","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Performance evaluation of categorizing technical support requests using advanced K-Means algorithm","M. A. Nadaf; S. S. Patil","Department of CSE, Rajarambapu Institute of Technology, Islampur (Sangli), MS, India","2015 IEEE International Advance Computing Conference (IACC)","20150713","2015","","","409","414","Technical support service providers receive thousands of customer queries daily. Traditionally, such organizations discard the data due to lack of storage capacity. However, value of storing such data is needed for the better results of analysis and to improve the closure rate of the daily customer queries. Data mining is the process of finding important and meaningful information, patterns through the large amount of data. Clustering is used as one of the best concept for data analysis, using machine learning approach with mathematical and statistical methods. Cluster analysis is widely applicable for practical applications in emerging trends in data mining. Analysis of clustering algorithms such as K-Means, Dirichlet, Fuzzy K-Means Canopy algorithms is done by means of the practical approach, in this research work. Performance of algorithm is observed based on the execution or computational time and results are compared with each of these algorithms. This paper proposes the streaming K-Means algorithm which resolves the queries as it arrives and analyses the data. Cosine distance measure plays an important role in clustering dataset. Sum of Square error is measured to check the quality of the cluster.","","CD-ROM:978-1-4799-8046-8; Electronic:978-1-4799-8047-5; POD:978-1-4799-8048-2","10.1109/IADCC.2015.7154740","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154740","Data mining;Machine Learning;MapReduce;Streaming K-Means","Algorithm design and analysis;Clustering algorithms;Data mining;File systems;Machine learning algorithms;Partitioning algorithms;Sparks","data analysis;data mining;learning (artificial intelligence);mathematical analysis;pattern clustering;query processing;statistical analysis;technical support services","Dirichlet;cluster analysis;clustering algorithms;cosine distance measure;customer queries;data analysis;data mining;data storing;fuzzy k-means canopy algorithms;machine learning approach;mathematical methods;performance evaluation;statistical methods;storage capacity;sum of square error;technical support requests;technical support service providers","","0","","15","","","12-13 June 2015","","IEEE","IEEE Conference Publications"
"Developing a framework that utilizes intelligent agents to extract multi-lingual web news","A. Al-Daraiseh; W. Haddoush","Computer Information Systems, King Saud University, Riyadh, Saudi Arabia","2015 2nd World Symposium on Web Applications and Networking (WSWAN)","20150820","2015","","","1","5","This paper proposes a framework for personalized news application that utilizes intelligent agents to aggregate specific articles from web news based on users' preferences. It shows how to extract web news stories in Arabic language, in particular, in addition to English language. Intelligent agents learn user's interests by exploiting data gathered from social media, web browsing history and also explicit and implicit feedback from the user. To suit the various facets of the user's interests, the system adopt multiagent with different genres. Each agent covers a particular task to fulfil the desired functionality of the framework.","","Electronic:978-1-4799-8172-4; POD:978-1-4799-8173-1","10.1109/WSWAN.2015.7210335","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210335","bi-lingual;intelligent agent;machine learning;personalized news","Data mining;Feature extraction;History;Intelligent agents;Internet;Media;Monitoring","Internet;data handling;information retrieval;learning (artificial intelligence);multi-agent systems;natural language processing;social networking (online)","Arabic language;English language;Web browsing history;data gathering;explicit feedback;implicit feedback;intelligent agent utilization;machine learning;multilingual Web news stories extraction;personalized news application;social media;user interest learning","","0","","20","","","21-23 March 2015","","IEEE","IEEE Conference Publications"
"System simulation from operational data","A. Wasicek; E. A. Lee; H. Kim; L. Greenberg; A. Iwai; I. Akkaya","University of California, Berkeley, USA","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20150727","2015","","","1","6","System simulation is a valuable tool to unveil inefficiencies and to test new strategies when implementing and revising systems. Often, simulations are parameterized using offline data and heuristic knowledge. Operational data, i.e., data gained through experimentation and observation, can greatly improve the fidelity between the actual system and the simulation. In a traffic scenario, for example, different road conditions or vehicle types can impact the outcome of the simulation and have to be considered during the modeling stage. This paper proposes using machine learning techniques to generate high fidelity simulation models. A traffic simulation case study exemplifies this approach by generating a model for the SUMO traffic simulator from vehicular telemetry data.","0738-100X;0738100X","Electronic:978-1-4799-8052-9; POD:978-1-4799-8053-6","10.1145/2744769.2747944","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167186","Machine Learning;Model generation;Traffic simulation","Adaptation models;Data models;Roads;Solid modeling;Tutorials;Vehicles","digital simulation;learning (artificial intelligence);road traffic;telemetry;traffic engineering computing","SUMO traffic simulator;heuristic knowledge;high fidelity simulation models;machine learning techniques;modeling stage;offline data;operational data;road conditions;system simulation;traffic scenario;traffic simulation case study;vehicle types;vehicular telemetry data","","0","","24","","","8-12 June 2015","","IEEE","IEEE Conference Publications"
"Review on data uncertainty in face recognition using appearance-based methods","S. G. Khadse; P. S. Mohod","Department of Computer Science and Engineering, G. H. Raisoni Institute of Engineering and Technology for woman, Nagpur, India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)","20150813","2015","","","1","5","The face images should not be the completely accurate for representation and for an observation. To reducing the uncertainty for representation of the face images and to improving the accuracy of face recognition, more observation of the same person face images is required in the face recognition. In the real world face recognition system the uncertainty highly occurred because the limited number of available face images of subject and due to this there is high uncertainty is occurred. In this paper, develop the model which is to improve the accuracy in the face recognition by reducing the data uncertainty. The model is to reduce the uncertainty of face images representation by synthesizing the virtual training samples. Here, the useful training samples are selected, which are comparable to the test sample from the set of all the original training samples and synthesized virtual training sample.","","Electronic:978-1-4799-6818-3; POD:978-1-4799-6819-0","10.1109/ICIIECS.2015.7193054","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193054","Computer vision;face images;face recognition;machine learning;uncertainty","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Mathematical model;Optimization;Robustness;Uncertainty","face recognition;image representation;image sampling","appearance-based method;data uncertainty reduction;face image representation;face recognition;image observation;virtual training sample synthesis","","0","","10","","","19-20 March 2015","","IEEE","IEEE Conference Publications"
"An Integrated System for Mapping Red Clover Ground Cover Using Unmanned Aerial Vehicles: A Case Study in Precision Agriculture","A. M. Abuleil; G. W. Taylor; M. Moussa","Sch. of Eng., Univ. of Guelph, Guelph, ON, Canada","2015 12th Conference on Computer and Robot Vision","20150716","2015","","","277","284","In the field of precision agriculture (PA), Un-manned Aerial Vehicles (UAVs) are creating new opportunities for remotely assessing various characteristics of crops. In this paper, we present two main contributions that were evaluated on a novel application: mapping red clover ground cover (RCGC). First, we develop an integrated system for collecting, pre-processing and analyzing aerial data for the mapping of RCGC at a patch-level. Second, we collected, ground-trusted, and pre-processed a RCGC dataset that we make public for further analysis. We evaluated several different machine learning classifiers for mapping image patches to discrete clover coverage levels, reaching an accuracy of 91%.","","Electronic:978-1-4799-1986-4; POD:978-1-4799-1987-1","10.1109/CRV.2015.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158930","classification;ground cover;machine learning;precision agriculture;red clover;remote sensing","Accuracy;Agriculture;Data collection;Global Positioning System;Hyperspectral sensors;Sensors;Support vector machines","agriculture;autonomous aerial vehicles;image classification;learning (artificial intelligence);path planning;precision engineering;robot vision","PA;RCGC mapping;UAV;aerial data analysis;aerial data collection;aerial data preprocessing;crop characteristics;image patch mapping;machine learning classifiers;precision agriculture;red clover ground cover mapping;unmanned aerial vehicles","","0","","16","","","3-5 June 2015","","IEEE","IEEE Conference Publications"
"Automatic Discovery of Service Name Replacements Using Ledger Data","S. Tuarob; C. S. Tucker; R. Strong; J. Blomberg; A. Chandra; P. Chowdhary; S. Oh","Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","2015 IEEE International Conference on Services Computing","20150820","2015","","","624","631","Recent studies have illustrated historical financial data could be used to predict future revenues and profits. Prediction models would be accurate when long-run data that traces back for multiple years is available. However, changes in service structures often result in alteration of the nomenclatures of the services, making the streams of financial transactions associated with affected services discontinue. Manually inquiring the history of changes can be tedious and unsuccessful especially in large companies. In this paper, we propose a machine learning based algorithm for automatically discovering service name replacements. The proposed methodology draws heterogeneous features from financial data available in most ledger databases, and hence is generalizable. Our proposed methodology is shown to be effective on ground-truth synthesized data generated from real-world IBM service delivery ledger database.","","Electronic:978-1-4673-7281-7; POD:978-1-4673-7282-4","10.1109/SCC.2015.90","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207408","Classification;Machine Learning;Service Name Replacement","Aggregates;Contracts;Data models;Databases;Market research;Time series analysis","data handling;financial data processing;learning (artificial intelligence);profitability","automatic discovery;financial transactions;historical financial data;ledger data;machine learning;real-world IBM service;service name replacements;service structures","","0","","17","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"Information-theoretic characterization of blood panel predictors for brain atrophy and cognitive decline in the elderly","S. K. Madsen; G. V. Steeg; A. Mezher; N. Jahanshad; T. M. Nir; X. Hua; B. A. Gutman; A. Galstyan; P. M. Thompson","Imaging Genetics Center, USC, Marina Del Rey, CA, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","980","984","Cognitive decline in old age is tightly linked with brain atrophy, causing significant burden. It is critical to identify which biomarkers are most predictive of cognitive decline and brain atrophy in the elderly. In 566 older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we used a novel unsupervised machine learning approach to evaluate an extensive list of more than 200 potential brain, blood and cerebrospinal fluid (CSF)-based predictors of cognitive decline. The method, called CorEx, discovers groups of variables with high multivariate mutual information and then constructs latent factors that explain these correlations. The approach produces a hierarchical structure and the predictive power of biological variables and latent factors are compared with regression. We found that a group of variables containing the well-known AD risk gene APOE and CSF tau and amyloid levels were highly correlated. This latent factor was the most predictive of cognitive decline and brain atrophy.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164035","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164035","Brain;Cells & molecules;Genes;Machine learning;Magnetic resonance imaging (MRI)","Alzheimer's disease;Atrophy;Biomarkers;Blood;Correlation;Magnetic resonance imaging","biomedical MRI;blood;brain;diseases;genetics;geriatrics;learning (artificial intelligence);medical image processing;neurophysiology;proteins;regression analysis","APOE gene;Alzheimer disease neuroimaging initiative;Alzheimer disease risk;CSF tau gene;CSF-based predictor;CorEx method;amyloid level;apolipoprotein E;biological variable;blood panel predictor;brain atrophy;cerebrospinal fluid;cognitive decline;elderly;hierarchical structure;high multivariate mutual information;information-theoretic characterization;latent factor;machine learning approach;magnetic resonance imaging;regression analysis","","1","","15","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Predicting Functional Independence Measure Scores During Rehabilitation With Wearable Inertial Sensors","G. Sprint; D. J. Cook; D. L. Weeks; V. Borisov","Department of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Access","20150826","2015","3","","1350","1366","Evaluating patient progress and making discharge decisions regarding inpatient medical rehabilitation rely upon the standard clinical assessments administered by trained clinicians. Wearable inertial sensors can offer more objective measures of patient movement and progress. We undertook a study to investigate the contribution of wearable sensor data to predict discharge functional independence measure (FIM) scores for 20 patients at an inpatient rehabilitation facility. The FIM utilizes a seven-point ordinal scale to measure patient independence while performing several activities of daily living, such as walking, grooming, and bathing. Wearable inertial sensor data were collected from ecological ambulatory tasks at two time points mid-stay during inpatient rehabilitation. Machine learning algorithms were trained with sensor-derived features and clinical information obtained from medical records at admission to the inpatient facility. While models trained only with clinical features predicted discharge scores well, we were able to achieve an even higher level of prediction accuracy when also including the wearable sensor-derived features. Correlations as high as 0.97 for leave-one-out cross validation predicting discharge FIM motor scores are reported.","2169-3536;21693536","","10.1109/ACCESS.2015.2468213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194735","Rehabilitation monitoring;machine learning;prediction;rehabilitation monitoring;signal processing;wearable sensors","Biomedical signal processing;IEEE Standards;Machine learnng algorithms;Medical services;Patient monitoring;Patient rehabilitation;Predictive models;Wearable sensors","biomechanics;body sensor networks;feature extraction;learning (artificial intelligence);medical signal processing;patient rehabilitation","bathing;discharge functional independence measure;grooming;inpatient medical rehabilitation;machine learning algorithms;patient movement;walking;wearable inertial sensors","","2","","41","","20150813","2015","","IEEE","IEEE Journals & Magazines"
"Smart Information Reconstruction via Time-Space-Spectrum Continuum for Cloud Removal in Satellite Images","N. B. Chang; K. Bai; C. F. Chen","Department of Civil, Environmental, and Construction Engineering, University of Central Florida, Orlando, FL, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20150717","2015","8","5","1898","1912","Cloud contamination is a big obstacle when processing satellite images retrieved from visible and infrared spectral ranges for application. Although computational techniques including interpolation and substitution have been applied to recover missing information caused by cloud contamination, these algorithms are subject to many limitations. In this paper, a novel smart information reconstruction (SMIR) method is proposed, in order to reconstruct cloud contaminated pixel values from the time-space-spectrum continuum with the aid of a machine learning tool, namely extreme learning machine (ELM). For the purpose of demonstration, the performance of SMIR is evaluated by reconstructing the missing remote sensing reflectance values derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) on board the Terra satellite over Lake Nicaragua, where is a very cloudy area year round. For comparison, the traditional backpropagation neural network algorithms will also be implemented to reconstruct the missing values. Experimental results show that the ELM outperforms the BP algorithms by an enhanced machine learning capacity with simulated memory effect embedded in MODIS due to linking the complex time-space-spectrum continuum between cloud-free and cloudy pixels. The ELM-based SMIR practice presents a correlation coefficient of 0.88 with root mean squared error of 7.4E - 04sr<sup>-1</sup> between simulated and observed reflectance values. Finding suggests that the SMIR method is effective to reconstruct all the missing information providing visually logical and quantitatively assured images for further image processing and interpretation in environmental applications.","1939-1404;19391404","","10.1109/JSTARS.2015.2400636","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063914","Artificial neural network;cloud removal;computational intelligence;extreme learning machine;machine learning;satellite images","Clouds;Contamination;Image reconstruction;Neural networks;Remote sensing;Satellites;Training","clouds;geophysical image processing;image reconstruction;lakes;learning (artificial intelligence);neural nets;reflectivity;remote sensing","ELM-based SMIR practice;Lake Nicaragua;SMIR method;Terra satellite;backpropagation neural network algorithm;cloud contaminated pixel reconstruction;cloud contamination;cloud removal;cloud-free;cloudy area;cloudy pixel;computational technique;enhanced machine learning capacity;environmental application;extreme learning machine;infrared spectral range;machine learning tool;missing information recovery;missing remote sensing reflectance value;moderate resolution imaging spectroradiometer;root mean squared error;satellite image processing;smart information reconstruction;time-space-spectrum continuum;visible spectral range","","4","","66","","20150319","May 2015","","IEEE","IEEE Journals & Magazines"
"Carotene: A Job Title Classification System for the Online Recruitment Domain","F. Javed; Q. Luo; M. McNair; F. Jacob; M. Zhao; T. S. Kang","Data Sci. R&D, Atlanta, GA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","286","293","In the online job recruitment domain, accurate classification of jobs and resumes to occupation categories is important for matching job seekers with relevant jobs. An example of such a job title classification system is an automatic text document classification system that utilizes machine learning. Machine learning-based document classification techniques for images, text and related entities have been well researched in academia and have also been successfully applied in many industrial settings. In this paper we present Carotene, a machine learning-based semi-supervised job title classification system that is currently in production at CareerBuilder. Carotene leverages a varied collection of classification and clustering tools and techniques to tackle the challenges of designing a scalable classification system for a large taxonomy of job categories. It encompasses these techniques in a cascade classifier architecture. We first present the architecture of Carotene, which consists of a two-stage coarse and fine level classifier cascade. We compare Carotene to an early version that was based on a flat classifier architecture and also compare and contrast Carotene with a third party occupation classification system. The paper concludes by presenting experimental results on real world industrial data using both machine learning metrics and actual user experience surveys.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.61","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184892","job title classification;machine learning;text classification","Accuracy;Computer architecture;Software;Support vector machines;System-on-chip;Taxonomy;Training","classification;learning (artificial intelligence);recruitment;text analysis","Carotene;automatic text document classification system;cascade classifier architecture;job classification;job seekers;job title classification system;machine learning-based document classification techniques;occupation categories;online recruitment domain;related entities;resumes;text entities","","4","","25","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"A Parallel Distributed Weka Framework for Big Data Mining Using Spark","A. K. Koliopoulos; P. Yiapanis; F. Tekiner; G. Nenadic; J. Keane","Sch. of Comput. Sci., Univ. of Manchester, Manchester, UK","2015 IEEE International Congress on Big Data","20150820","2015","","","9","16","Effective Big Data Mining requires scalable and efficient solutions that are also accessible to users of all levels of expertise. Despite this, many current efforts to provide effective knowledge extraction via large-scale Big Data Mining tools focus more on performance than on use and tuning which are complex problems even for experts. Weka is a popular and comprehensive Data Mining workbench with a well-known and intuitive interface, nonetheless it supports only sequential single-node execution. Hence, the size of the datasets and processing tasks that Weka can handle within its existing environment is limited both by the amount of memory in a single node and by sequential execution. This work discusses DistributedWekaSpark, a distributed framework for Weka which maintains its existing user interface. The framework is implemented on top of Spark, a Hadoop-related distributed framework with fast in-memory processing capabilities and support for iterative computations. By combining Weka's usability and Spark's processing power, DistributedWekaSpark provides a usable prototype distributed Big Data Mining workbench that achieves near-linear scaling in executing various real-world scale workloads - 91.4% weak scaling efficiency on average and up to 4x faster on average than Hadoop.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207196","Big Data;Data Mining;Distributed Systems;Machine Learning;Spark;Weka","Algorithm design and analysis;Big data;Computational modeling;Data mining;Load modeling;Object oriented modeling;Sparks","Big Data;data mining;parallel processing;user interfaces","DistributedWekaSpark;Hadoop-related distributed framework;Spark processing power;Weka usability;datasets size;distributed Big Data mining;fast in-memory processing capabilities;iterative computations;knowledge extraction;large-scale Big Data mining tools;parallel distributed Weka framework;processing tasks;sequential single-node execution;user interface","","5","","29","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"Layout optimization and template pattern verification for directed self-assembly (DSA)","Z. Xiao; Daifeng Guo; M. D. F. Wong; H. Yi; M. C. Tung; H. S. P. Wong","University of Illinois at Urbana-Champaign, 61820, USA","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20150727","2015","","","1","6","Recently, block copolymer directed self-assembly (DSA) has demonstrated great advantages in patterning contacts/vias for the 7 nm technology node and beyond. The high throughput and low process cost of DSA makes it the most promising candidate in patterning tight pitched dense patterns for the next generation lithography. Since DSA is very sensitive to the shapes and distributions of the guiding templates, it is necessary to develop new EDA algorithms and tools to address the patterning rules and constraints of the process. This paper presents a set of DSA-aware optimization techniques targeting the most urgent problems for DSA technology, including layout optimization and template pattern verification.","0738-100X;0738100X","Electronic:978-1-4799-8052-9; POD:978-1-4799-8053-6","10.1145/2744769.2747934","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167384","Directed Self-Assembly;Layout Optimization;Machine Learning;Template Verification","Layout;Libraries;Lithography;Optimization;Routing;Shape;Wires","nanolithography;nanopatterning;optimisation;polymer blends;self-assembly","DSA technology;DSA-aware optimization techniques;EDA algorithms;block copolymer;directed self-assembly;guiding templates;layout optimization;next generation lithography;patterning rules;pitched dense patterns;size 7 nm;template pattern verification","","0","","28","","","8-12 June 2015","","IEEE","IEEE Conference Publications"
"Applying Analytics to Improve Hardware and Software Maintenance Support Services","Z. Xin; F. Li; Q. C. Li; S. R. Sarkar","IBM Res. - China, Beijing, China","2015 IEEE International Conference on Services Computing","20150820","2015","","","355","362","Organizations providing large scale software and hardware maintenance support services typically capture detailed metrics of each service request (SR) for a customer. Examples of such metrics include the time taken to resolve the problem, success of the resolution, escalations across levels of support, field engineer site visit statistics, and parts replacement data -- the latter two for hardware maintenance only. For some SRs, targeted customer surveys may be conducted to elicit feedback about how effectively the end-to-end problem resolution process was performed. Application of analytics to such data to enable continuous improvement of the operational efficiencies of providing maintenance services is an open area of research. This paper describes the authors' experience with several analytics projects in this domain. Improvement of maintenance support services can lead to faster and better problem resolution, leading to reduced down time and an increase in the overall resiliency of a computing environment.","","Electronic:978-1-4673-7281-7; POD:978-1-4673-7282-4","10.1109/SCC.2015.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207374","analytics;customer satisfaction;machine learning;maintenance;service;technical support;trends","Analytical models;Data models;Hardware;Maintenance engineering;Measurement;Predictive models;Training data","software maintenance;software metrics","SR;analytics projects;computing environment resiliency improvement;continuous improvement;down time reduction;elicit feedback;end-to-end problem resolution process;hardware maintenance support service improvement;large-scale software;operational efficiencies;service request;software maintenance support service improvement","","0","","11","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"Fast encoding for personalized views extracted from beyond high definition content","N. Van Kets; J. De Praeter; G. Van Wallendael; J. De Cock; R. Van de Walle","Multimedia Lab, Ghent University - iMinds, Ghent, Belgium, Gaston Crommenlaan 8 box 201, 9050 Ledeberg-Ghent, Belgium","2015 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting","20150806","2015","","","1","7","Broadcast providers are looking for new opportunities to increase user experience and user interaction on their content. Their main goal is to attract and preserve viewer attention to create a big and stable audience. This could be achieved with a second screen application that lets the users select their own viewpoint in an extremely high resolution video to direct their own first screen. By allowing the users to create their own personalized video stream, they become involved with the content creation itself. However, encoding a personalized view for each user is computationally complex. This paper describes a machine learning approach to speed up the encoding of each personal view. Simulation results of zoom, pan and tilt scenarios show bit rate increases between 2% and 9% for complexity reductions between 69% and 79% compared to full encoding.","2155-5044;21555044","Electronic:978-1-4799-5865-8; POD:978-1-4799-5866-5","10.1109/BMSB.2015.7177225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177225","Future technologies and services of broadcasting;High Efficiency Video Coding (HEVC);machine learning;video coding and processing;video interaction","Cameras;Complexity theory;Encoding;High definition video;Spatial resolution;Transforms","learning (artificial intelligence);video coding;video streaming","broadcast providers;complexity reductions;content creation;fast encoding;high definition content;machine learning;personalized video stream;personalized views;second screen application;user experience;user interaction","","0","","12","","","17-19 June 2015","","IEEE","IEEE Conference Publications"
"A Step Towards the Automated Diagnosis of Parkinson's Disease: Analyzing Handwriting Movements","C. R. Pereira; D. R. Pereira; F. A. d. Silva; C. Hook; S. A. T. Weber; L. A. M. Pereira; J. P. Papa","UFSCAR, Fed. Univ., Sao Carlos, Brazil","2015 IEEE 28th International Symposium on Computer-Based Medical Systems","20150727","2015","","","171","176","Parkinson's disease (PD) has affected millions of people world-wide, being its major problem the loss of movements and, consequently, the ability of working and locomotion. Although we can find several works that attempt at dealing with this problem out there, most of them make use of datasets composed by a few subjects only. In this work, we present some results toward the automated diagnosis of PD by means of computer vision-based techniques in a dataset composed by dozens of patients, which is one of the main contributions of this work. The dataset is part of a joint research project that aims at extracting both visual and signal-based information from healthy and PD patients in order to go forward the early diagnosis of PD patients. The dataset is composed by handwriting clinical exams that are analyzed by means of image processing and machine learning techniques, being the preliminary results encouraging and promising. Additionally, a new quantitative feature to measure the amount of tremor of an individual's handwritten trace called Mean Relative Tremor is also presented.","1063-7125;10637125","Electronic:978-1-4673-6775-2; POD:978-1-4673-6776-9","10.1109/CBMS.2015.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167480","Parkinson's disease;machine learning;movement disorders","Accuracy;Feature extraction;Handheld computers;Niobium;Parkinson's disease;Spirals","biomechanics;diseases;feature extraction;learning (artificial intelligence);medical disorders;medical image processing","automated Parkinson disease diagnosis;computer vision-based techniques;feature extraction;handwriting movement analysis;image processing techniques;locomotion;machine learning techniques;mean relative tremor","","1","","16","","","22-25 June 2015","","IEEE","IEEE Conference Publications"
"Catlinks - a category clustering algorithm based on multi-class regression","Rui Liu; Lixin Ding; Lan Xie","State Key Lab of Software Engineering Wuhan University, 430000, China","2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems","20150806","2014","","","323","326","This paper proposed CatLinks, a category clustering algorithm based on multi-class regression. In recommender systems for e-commerce web sites, users' experience of recommendations highly relies on the diversity of purchase suggestions. Taking inexpensive training data as products' literal information and their categories, CatLinks extracts latent features of categories and construct presentation of them as vectors. With vector presentation categories can be clustered by similarity measure and aggregation methods such as KNN or K-Means. Algorithm of CatLinks is based on training of a multi-class category predictor of products. After the predictor is trained, its weight matrix is taken as feature vectors of categories. With similarity of categories, recommender system can suggest users to purchase products from extended categories, when their interest on a certain category is discovered. Through our experiments on Alibaba's product and order dataset, CatLinks is proved a novel method to predict category co-occurrence of user's joint orders.","2376-5933;23765933","Electronic:978-1-4799-4719-5; POD:978-1-4673-6954-1","10.1109/CCIS.2014.7175752","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175752","category clustering;machine learning;multi-class classification;recommender system;stochastic gradient descent","","Web sites;electronic commerce;feature extraction;pattern clustering;purchasing;recommender systems;regression analysis","CatLinks;K-Means;KNN;aggregation methods;category clustering algorithm;e-commerce Web sites;latent feature extraction;multiclass category predictor;multiclass regression;product literal information;purchase suggestions;recommender system;recommender systems;user recommendation experience;vector presentation categories;weight matrix","","0","","12","","","27-29 Nov. 2014","","IEEE","IEEE Conference Publications"
"Learning-Based Object Identification and Segmentation Using Dual-Energy CT Images for Security","L. Martin; A. Tuysuzoglu; W. C. Karl; P. Ishwar","Department of Electrical and Computer Engineering, Boston University, Boston, MA, USA","IEEE Transactions on Image Processing","20150811","2015","24","11","4069","4081","In recent years, baggage screening at airports has included the use of dual-energy X-ray computed tomography (DECT), an advanced technology for nondestructive evaluation. The main challenge remains to reliably find and identify threat objects in the bag from DECT data. This task is particularly hard due to the wide variety of objects, the high clutter, and the presence of metal, which causes streaks and shading in the scanner images. Image noise and artifacts are generally much more severe than in medical CT and can lead to splitting of objects and inaccurate object labeling. The conventional approach performs object segmentation and material identification in two decoupled processes. Dual-energy information is typically not used for the segmentation, and object localization is not explicitly used to stabilize the material parameter estimates. We propose a novel learning-based framework for joint segmentation and identification of objects directly from volumetric DECT images, which is robust to streaks, noise and variability due to clutter. We focus on segmenting and identifying a small set of objects of interest with characteristics that are learned from training images, and consider everything else as background. We include data weighting to mitigate metal artifacts and incorporate an object boundary field to reduce object splitting. The overall formulation is posed as a multilabel discrete optimization problem and solved using an efficient graph-cut algorithm. We test the method on real data and show its potential for producing accurate labels of the objects of interest without splits in the presence of metal and clutter.","1057-7149;10577149","","10.1109/TIP.2015.2456507","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7159062","Baggage screening;Dual-energy X-ray computed tomography;Homeland security;Machine learning;Object labeling;Threat detection;baggage screening;homeland security;machine learning;object labeling;threat detection","Attenuation;Computed tomography;Image segmentation;Labeling;Metals;Noise;X-ray imaging","computerised tomography;graph theory;image segmentation;learning (artificial intelligence);national security;object recognition;optimisation;parameter estimation","baggage screening;data weighting;decoupled processes;dual-energy CT images;dual-energy X-ray computed tomography;dual-energy information;graph-cut algorithm;image artifacts;image noise;learning-based object identification;learning-based object segmentation;material identification;material parameter estimate stability;medical CT;metal artifact mitigation;multilabel discrete optimization problem;nondestructive evaluation;object boundary field;object labeling;object localization;object splitting reduction;scanner images;threat object identification;training images;volumetric DECT images","","1","","40","","20150715","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"Unsupervised Hierarchical Modeling of Driving Behavior and Prediction of Contextual Changing Points","T. Taniguchi; S. Nagasaka; K. Hitomi; K. Takenaka; T. Bando","Coll. of Inf. Sci. & Eng., Ritsumeikan Univ., Kusatsu, Japan","IEEE Transactions on Intelligent Transportation Systems","20150731","2015","16","4","1746","1760","An unsupervised learning method, called double articulation analyzer with temporal prediction (DAA-TP), is proposed on the basis of the original DAA model. The method will enable future advanced driving assistance systems to determine driving context and predict possible scenarios of driving behavior by segmenting and modeling incoming driving-behavior time series data. In previous studies, we applied the DAA model to driving-behavior data and argued that contextual changing points can be estimated as changing points of chunks. A sequence prediction method, which predicts the next hidden state sequence, was also proposed in a previous study. However, the original DAA model does not model the duration of chunks of driving behavior and is not able to do a temporal prediction of the scenarios. Our DAA-TP method explicitly models the duration of chunks of driving behavior on the assumption that driving-behavior data have a two-layered hierarchical structure, i.e., double articulation structure. For this purpose, the hierarchical Dirichlet process hidden semi-Markov model is used for explicitly modeling the duration of segments of driving-behavior data. A Poisson distribution is also used to model the duration distribution of driving-behavior segments. The duration distribution of chunks of driving-behavior data is also theoretically calculated using the reproductive property of the Poisson distribution. We also propose a calculation method for obtaining the probability distribution of the remaining duration of current driving words as a mixture of Poisson distribution with a theoretical approximation for unobserved driving words. This method can calculate the posterior probability distribution of the next termination time of chunks by explicitly modeling all probable chunking results for observed data. The DAA-TP was applied to a synthetic data set having a double articulation structure to evaluate its model consistency. To evaluate the effectiveness of DAA-TP, we appl- ed it to a driving-behavior data set recorded at actual factory circuits. The DAA-TP could predict the next termination time of chunks more accurately than the compared methods. We also report the qualitative results for understanding the potential capability of DAA-TP.","1524-9050;15249050","","10.1109/TITS.2014.2376525","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990595","Driving data;long-term prediction;machine learning;nonparametric Bayes","Context modeling;Data models;Hidden Markov models;Predictive models;Probability distribution;Time series analysis;Vehicles","Markov processes;Poisson distribution;driver information systems;unsupervised learning","DAA-TP method;Poisson distribution;advanced driving assistance systems;chunk termination time;double articulation analyzer with temporal prediction;double articulation structure;driving-behavior segment duration distribution;driving-behavior time series;hierarchical Dirichlet process hidden semiMarkov model;probability distribution;sequence prediction method;synthetic data set;two-layered hierarchical structure;unsupervised hierarchical contextual changing point prediction modeling;unsupervised hierarchical driving behavior modeling;unsupervised learning method","","7","","28","","20141218","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"A bio-inspired logical process for saliency detections in cognitive crowd monitoring","S. Chiappino; A. Mazzu; A. Mazzu; L. Marcenaro; C. S. Regazzoni","DITEN, University of Genova, Via Opera Pia 11A 16145 - Italy","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","2110","2114","It is well known from physiological studies that the level of human attention for adult individuals rapidly decreases after five to twenty minutes (Green and Sandia, 1999). Attention retention for a surveillance operator represents a crucial aspect in Video Surveillance applications and could have a significant impact in identifying relevance, especially in crowded situations. In this field, advanced mechanisms for selection and extraction of saliency information can improve the performances of autonomous video surveillance systems and increase the effectiveness of human operator support. In particular, crowd monitoring represents a central aspect in many practical applications for managing and preventing emergencies due to panic and overcrowding. In this paper, an adaptive inductive reasoning mechanism for saliency extraction and information reconstruction for a distributed camera sensors network is presented. The proposed system, by means of Self Organizing Maps (SOMs), can learn the correlation of the observed data and then recover the whole information from a subset of available sensors. Experimental results show how the proposed system can reconstruct the information about the non-observed parts starting from relevant data acquired from observed areas of the environment.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178343","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178343","Self Organizing Maps;cognitive crowd monitoring;human reasoning;machine learning;saliency detection","Cognition;Correlation;Cost function;Data mining;Sensors;Surveillance","cameras;distributed sensors;inference mechanisms;self-organising feature maps;video surveillance","adaptive inductive reasoning mechanism;autonomous video surveillance systems;bio-inspired logical process;cognitive crowd monitoring;crowded situations;distributed camera sensors network;human operator support;information reconstruction;saliency detections;saliency extraction;saliency information;self organizing maps;surveillance operator;video surveillance applications","","0","","19","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Behavioral Modeling for Churn Prediction: Early Indicators and Accurate Predictors of Custom Defection and Loyalty","M. R. Khan; J. Manoj; A. Singh; J. Blumenstock","Inf. Sch., Univ. of Washington, Seattle, WA, USA","2015 IEEE International Congress on Big Data","20150820","2015","","","677","680","Churn prediction, or the task of identifying customers who are likely to discontinue use of a service, is an important and lucrative concern of firms in many different industries. As these firms collect an increasing amount of large-scale, heterogeneous data on the characteristics and behaviors of customers, new methods become possible for predicting churn. In this paper, we present a unified analytic framework for detecting the early warning signs of churn, and assigning a ""Churn Score"" to each customer that indicates the likelihood that the particular individual will churn within a predefined amount of time. This framework employs a brute force approach to feature engineering, then winnows the set of relevant attributes via feature selection, before feeding the final feature-set into a suite of supervised learning algorithms. Using several terabytes of data from a large mobile phone network, our method identifies several intuitive - and a few surprising - early warning signs of churn, and our best model predicts whether a subscriber will churn with 89.4% accuracy.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207291","Churn;call detail records;data science;machine learning;supervised learning","Accuracy;Measurement;Mobile communication;Mobile handsets;Prediction algorithms;Predictive models;Supervised learning","consumer behaviour;feature selection;learning (artificial intelligence)","behavioral modeling;brute force approach;churn prediction;churn score;custom defection;customer identification;feature selection;mobile phone network;supervised learning algorithms","","1","","14","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"SAETA: A Smart Coaching Assistant for Professional Volleyball Training","J. Vales-Alonso; D. Chaves-Diéguez; P. López-Matencio; J. J. Alcaraz; F. J. Parrado-García; F. J. González-Castaño","Department of Information Technologies and Communications, Technical University of Cartagena, Cartagena, Spain","IEEE Transactions on Systems, Man, and Cybernetics: Systems","20150715","2015","45","8","1138","1150","This paper introduces a smart assistant for professional volleyball training based on machine-learning techniques (SAETA). SAETA addresses two main aspects of elite sports coaching: 1) technical-tactical effort control, which aims at controlling exercise effort and fatigue levels and 2) exercise quality training, which complements the former by analyzing the execution of player movements. SAETA relies on a sensing infrastructure that monitors both players and their environment, and produces real-time data that is analyzed by different modules of a decision engine. Technical-tactical effort control is based on a dynamic programming model, which selects the best activity and rest durations in interval training, with the goal of maximizing effort while preventing fatigue. Exercise quality control consists of two stages. In the first stage, movements are detected by means of a k-nearest neighbors classifier and in the second stage, movement intensity is classified according to recent statistical data from the player being analyzed. These analyses are reported to coaches and players in real-time. SAETA has been developed in close collaboration with the Universidad Católica San Antonio de Murcia volleyball team, which competes in the Spanish women's premier league. Data gathered during training sessions has provided a knowledge base for the algorithms developed, and has been used for the validation of results.","2168-2216;21682216","","10.1109/TSMC.2015.2391258","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029116","Ambient intelligence (AmI);machine learning;sports;training","Biomedical monitoring;Fatigue;Monitoring;Real-time systems;Sensors;Training;Wireless sensor networks","ambient intelligence;computer based training;dynamic programming;learning (artificial intelligence);pattern classification;sport","SAETA;ambient intelligence;autonomous system for athletes personalized training based on machine learning and AmI techniques;decision engine;dynamic programming model;exercise quality control;exercise quality training;k-nearest neighbors classifier;machine-learning techniques;movement intensity classification;professional volleyball training;sensing infrastructure;smart coaching assistant;technical-tactical effort control","","1","","54","","20150202","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Automated Feature Design for Numeric Sequence Classification by Genetic Programming","D. Y. Harvey; M. D. Todd","Department of Structural Engineering, University of California, San Diego, CA, USA","IEEE Transactions on Evolutionary Computation","20150728","2015","19","4","474","489","Pattern recognition methods rely on maximum-information, minimum-dimension feature sets to reliably perform classification and regression tasks. Many methods exist to reduce feature set dimensionality and construct improved features from an initial set; however, there are few general approaches for the design of features from numeric sequences. Any information lost in preprocessing or feature measurement cannot be recreated during pattern recognition. General approaches are needed to extend pattern recognition to include feature design and selection for numeric sequences, such as time series, within the learning process itself. This paper proposes a novel genetic programming (GP) approach to automated feature design called Autofead. In this method, a GP variant evolves a population of candidate features built from a library of sequence-handling functions. Numerical optimization methods, included through a hybrid approach, ensure that the fitness of candidate algorithms is measured using optimal parameter values. Autofead represents the first automated feature design system for numeric sequences to leverage the power and efficiency of both numerical optimization and standard pattern recognition algorithms. Potential applications include the monitoring of electrocardiogram signals for indications of heart failure, network traffic analysis for intrusion detection systems, vibration measurement for bearing condition determination in rotating machinery, and credit card activity for fraud detection.","1089-778X;1089778X","","10.1109/TEVC.2014.2341451","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6861439","Feature design;genetic programming;machine learning;pattern recognition;sequence classification;time series classification;time series data mining","Algorithm design and analysis;Classification algorithms;Genetic programming;Pattern recognition;Standards;Time series analysis;Vegetation","data reduction;feature selection;genetic algorithms;learning (artificial intelligence);pattern classification;regression analysis;time series","Autofead;GP approach;automated feature design system;bearing condition determination;candidate algorithms;credit card activity;electrocardiogram signal monitoring;feature measurement;feature selection;feature set dimensionality reduction;fraud detection;genetic programming;heart failure;information lost;intrusion detection systems;learning process;maximum-informationfeature sets;minimum-dimension feature sets;network traffic analysis;numeric sequence classification;numerical optimization;numerical optimization methods;optimal parameter values;pattern recognition methods;regression tasks;rotating machinery;sequence-handling functions;time series","","5","","32","","20140721","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Randomized Subspace Learning for Proline Cis-Trans Isomerization Prediction","O. Y. Al-Jarrah; P. D. Yoo; K. Taha; S. Muhaidat; A. Shami; N. Zaki","ECE Dept., Khalifa University, Abu Dhabi","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20150805","2015","12","4","763","769","Proline residues are common source of kinetic complications during folding. The X-Pro peptide bond is the only peptide bond for which the stability of the cis and trans conformations is comparable. The cis-trans isomerization (CTI) of X-Pro peptide bonds is a widely recognized rate-limiting factor, which can not only induces additional slow phases in protein folding but also modifies the millisecond and sub-millisecond dynamics of the protein. An accurate computational prediction of proline CTI is of great importance for the understanding of protein folding, splicing, cell signaling, and transmembrane active transport in both the human body and animals. In our earlier work, we successfully developed a biophysically motivated proline CTI predictor utilizing a novel tree-based consensus model with a powerful metalearning technique and achieved 86.58 percent Q2 accuracy and 0.74 Mcc, which is a better result than the results (70-73 percent Q2 accuracies) reported in the literature on the well-referenced benchmark dataset. In this paper, we describe experiments with novel randomized subspace learning and bootstrap seeding techniques as an extension to our earlier work, the consensus models as well as entropy-based learning methods, to obtain better accuracy through a precise and robust learning scheme for proline CTI prediction.","1545-5963;15455963","","10.1109/TCBB.2014.2369040","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951423","Proline cis-trans isomerization;ensemble methods;machine learning;proline cis-trans isomerization;subspace learning","Bioinformatics;Computational biology;Peptides;Proteins;Support vector machines","biology computing;biomembrane transport;entropy;isomerisation;learning (artificial intelligence);molecular biophysics;molecular configurations;proteins","Q2 accuracy;X-Pro peptide bond;accurate computational prediction;benchmark dataset;biophysically motivated proline CTI predictor;bootstrap seeding techniques;cell signaling;cis conformation stability;entropy-based learning methods;kinetic complications;metalearning technique;millisecond dynamics;proline CTI;proline cis-trans isomerization prediction;protein folding;protein splicing;randomized subspace learning;recognized rate-limiting factor;submillisecond dynamics;trans conformation stability;transmembrane active transport;tree-based consensus model","0","0","","31","","20141110","July-Aug. 1 2015","","IEEE","IEEE Journals & Magazines"
"Bankruptcy Prediction of Construction Businesses: Towards a Big Data Analytics Approach","A. Hafiz; O. Lukumon; B. Muhammad; A. Olugbenga; O. Hakeem; A. Saheed","Univ. of the West of England (UWE), Bristol, UK","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","347","352","Bankruptcy prediction models (BPMs) are needed by financiers like banks in order to check the credit worthiness of companies. A very robust model needs a very large amount of data with periodic updates (i.e. appending new data). Such size of data cannot be processed directly by the tools used in building BPMs, however Big Data Analytics offers the opportunity to analyse such data. With data sources like DataStream, FAME, Company House, etc. that hold large financial data of existing and failed firms, it is possible to extract huge financial data into Hadoop database (e.g. HBase), whilst allowing periodic appending of data from the data sources, and carry out a Big Data analysis using a machine learning tool on Apache Mahout. Lifelong machine learning can also be employed in order to avoid repeated intensive training of the model using all the data in the Hadoop database. A framework is thus proposed for developing a Big Data Analytics based BPM.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184901","Bankruptcy prediction models;Big data analytics;Construction business failure;Financial models;Machine learning","Bankruptcy;Big data;Companies;Data models;Predictive models;Robustness;Support vector machines","Big Data;construction industry;data analysis;financial data processing;learning (artificial intelligence)","Apache Mahout;BPM;Big Data analytics approach;Hadoop database;bankruptcy prediction model;construction business;credit worthiness;financial data;lifelong machine learning","","0","","41","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"Prediction of online game performance degradation under network impairments","C. Y. Chiang; A. Cichocki; S. Erramilli; K. McInerney; D. Shur; S. Loeb","Applied Communication Sciences, Basking Ridge, NJ, USA","2015 12th Annual IEEE Consumer Communications and Networking Conference (CCNC)","20150716","2015","","","720","725","It is known that network impairments cause degradation in the online playing experience. Awareness of this degradation can enable game servers to take adaptive action that can mitigate or alleviate the game degradation quickly before it causes a player to leave the game in frustration. In this paper, we focus on a first person shooter game and determine the impact of network impairments on game performance using experimentation with player bots. We analyze game metrics such as the affected player's score, accuracy and effectiveness in shooting and taking evasive action. We show the use of statistical and machine learning techniques to determine the set of game metrics that can be used to discriminate between game states in near real-time. Our results indicate that the game state classifiers were very accurate in detecting high levels of impairments and were also reasonably accurate down to the time scale of 20-second intervals. These prediction techniques can be incorporated into gaming middleware to enable the mitigation of network-caused impairments.","2331-9852;23319852","CD-ROM:978-1-4799-6389-8; Electronic:978-1-4799-6390-4; POD:978-1-4799-6391-1","10.1109/CCNC.2015.7158067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158067","game state prediction;machine learning;network impairments;online games","Accuracy;Degradation;Delays;Games;Servers;Training","computer games;learning (artificial intelligence);performance evaluation;statistical analysis","first person shooter game;game degradation;game metrics;gaming middleware;machine learning techniques;network-caused impairments;online game performance degradation prediction;online playing experience;player bots;statistical techniques","","0","1","12","","","9-12 Jan. 2015","","IEEE","IEEE Conference Publications"
"Multi-task rank learning for image quality assessment","L. Xu; J. Li; W. Lin; Y. Zhang; L. Ma; Y. Fang; Y. Zhang; Y. Yan","Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","1339","1343","In practice, multiple types of distortions are associated with an image quality degradation process. The existing machine learning (ML) based image quality assessment (IQA) approaches generally established a unified model for all distortion types, or each model is trained independently for each distortion type by using single-task learning, which lead to the poor generalization ability of the models as applied to practical image processing. There are often the underlying cross relatedness amongst these single-task learnings in IQA, which is ignored by the previous approaches. To solve this problem, we propose a multi-task learning framework to train IQA models simultaneously across individual tasks each of which concerns one distortion type. These relatedness can be therefore exploited to improve the generalization ability of IQA models from single-task learning. In addition, pairwise image quality rank instead of image quality rating is optimized in learning task. By mapping image quality rank to image quality rating, a novel no-reference (NR) IQA approach can be derived. The experimental results confirm that the proposed Multi-task Rank Learning based IQA (MRLIQ) approach is prominent among all state-of-the-art NR-IQA approaches.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178188","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178188","MOS;Rank learning;image quality assessment;machine learning;pairwise comparison","Databases;Distortion;Image quality;Optimization;Support vector machines;Training;Transform coding","distortion;image coding;learning (artificial intelligence);optimisation","JPEG 2000;MRLIQ;NR-IQA approach;distortion type;generalization ability improvement;image quality assessment;image quality degradation process;image quality rank mapping;image quality rating;machine learning;multitask rank learning based IQA approach;pairwise image quality rank optimisation;single task learning;unified model","","1","","19","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Utilizing ECG-Based Heartbeat Classification for Hypertrophic Cardiomyopathy Identification","Q. A. Rahman; L. G. Tereshchenko; M. Kongkatong; T. Abraham; M. R. Abraham; H. Shatkay","Computational Biology and Machine Learning Lab, School of Computing, Queen&#x0027;s University, Kingston, Canada","IEEE Transactions on NanoBioscience","20150810","2015","14","5","505","512","Hypertrophic cardiomyopathy (HCM) is a cardiovascular disease where the heart muscle is partially thickened and blood flow is (potentially fatally) obstructed. A test based on electrocardiograms (ECG) that record the heart electrical activity can help in early detection of HCM patients. This paper presents a cardiovascular-patient classifier we developed to identify HCM patients using standard 10-second, 12-lead ECG signals. Patients are classified as having HCM if the majority of their recorded heartbeats are recognized as characteristic of HCM. Thus, the classifier's underlying task is to recognize individual heartbeats segmented from 12-lead ECG signals as HCM beats, where heartbeats from non-HCM cardiovascular patients are used as controls. We extracted 504 morphological and temporal features-both commonly used and newly-developed ones-from ECG signals for heartbeat classification. To assess classification performance, we trained and tested a random forest classifier and a support vector machine classifier using 5-fold cross validation. We also compared the performance of these two classifiers to that obtained by a logistic regression classifier, and the first two methods performed better than logistic regression. The patient-classification precision of random forests and of support vector machine classifiers is close to 0.85. Recall (sensitivity) and specificity are approximately 0.90. We also conducted feature selection experiments by gradually removing the least informative features; the results show that a relatively small subset of 264 highly informative features can achieve performance measures comparable to those achieved by using the complete set of features.","1536-1241;15361241","","10.1109/TNB.2015.2426213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7094280","Electrocardiogram;feature selection;hypertrophic cardiomyopathy;machine learning;patient classification","Electrocardiography;Feature extraction;Heart beat;Logistics;Support vector machines;Training","blood;cardiovascular system;diseases;electrocardiography;feature extraction;feature selection;haemodynamics;medical signal processing;muscle;regression analysis;signal classification;support vector machines","12-lead ECG signal segmentation;5-fold cross validation;ECG-based heartbeat classification;HCM patient detection;blood flow;cardiovascular disease;cardiovascular-patient classifier;feature selection;heart electrical activity;heart muscle;hypertrophic cardiomyopathy identification;least informative features;logistic regression classifier;morphological feature extraction;random forests;standard 10-second 12-lead ECG signals;support vector machine classifier;temporal feature extraction","1","3","","36","","20150424","July 2015","","IEEE","IEEE Journals & Magazines"
"Bio-medical Application on Predicting Systolic Blood Pressure Using Neural Networks","T. H. Wu; E. W. Y. Kwong; G. K. H. Pang","Dept. of Electr. & Electron. Eng., Univ. of Hong Kong, Hong Kong, China","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","456","461","This paper presents a new study based on artificial neural network, which is a typical technique for processing big data, for the prediction of systolic blood pressure by correlated factors (gender, serum cholesterol, fasting blood sugar and electrocardiography signal). Two neural network algorithms, back-propagation neural network and radial basis function network, are used to construct and validate the bio-medical prediction system. The database of raw data is divided into two parts: 80% for training the neural network and the remaining 20% for testing the performance. The experimental result shows that artificial neural networks are suitable for modeling and predicting systolic blood pressure. This novel method of predicting systolic blood pressure contributes to giving early warnings to adults who may not take regular blood pressure measurements. Also, as it is known that an isolated blood pressure measurement is sometimes not very accurate due to the daily fluctuation, our predictor can provide another reference value to the medical staff.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184916","Systolic blood pressure;big data application;bio-medical;hypertension;machine learning","Artificial neural networks;Biomedical monitoring;Blood pressure;Electrocardiography;Heart rate;Training","Big Data;backpropagation;blood pressure measurement;medical computing;radial basis function networks","artificial neural network;back-propagation neural network;big data processing;biomedical application;biomedical prediction system;blood pressure measurement;correlated factors;electrocardiography signal;fasting blood sugar;gender;neural network training;performance testing;radial basis function network;serum cholesterol;systolic blood pressure prediction","","0","","20","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"DC appliance classification and identification using k-Nearest Neighbours technique on features extracted within the 1st second of current waveforms","Y. T. Quek; W. L. Woo; T. Logenthrian","School of Electrical and Electronic Engineering, Newcastle University, United Kingdom","2015 IEEE 15th International Conference on Environment and Electrical Engineering (EEEIC)","20150723","2015","","","554","560","The commonly used identification techniques for appliances in a household are usually performed on the AC power supply side. However, as more household appliances and gadgets are now being DC powered, it is more accurate to perform the measurement and identification on the DC demand side. In addition, the AC identification method is not applicable for the DC household-grid. This paper discusses the application of a computational intelligence technique, k-Nearest Neighbours, to classify and identify DC appliances in a low voltage DC household through their 1st second of DC demand-side waveforms, sampled at 500Hz. Voltage and current waveforms were collected from an experiment conducted using this technique and it has been observed from the data collected that DC appliances generate unique current waveforms, similar to signatures, during the 1st second of operation. This time window can be spilt further into an inrush current stage and a steady-state stage. Two primary features and three secondary features of the waveforms were extracted and employed as attributes in the kNN technique, which was successfully used to classify and identify three appliances: a Peltier technology fridge, LED lights and a DC motor fan.","","CD-ROM:978-1-4799-7992-9; Electronic:978-1-4799-7993-6; POD:978-1-4799-7994-3","10.1109/EEEIC.2015.7165222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165222","Appliance recognition;Direct Current;Load classification;Machine Learning;kNN","DC motors;Feature extraction;Home appliances;Light emitting diodes;Steady-state;Surges;Training","demand side management;domestic appliances;light emitting diodes;power supply quality","AC identification method;AC power supply side;DC appliance classification;DC appliance identification;DC demand side;DC demand-side waveforms;DC household-grid;DC motor fan;LED lights;Peltier technology fridge;computational intelligence;current waveforms;frequency 500 Hz;household appliances;inrush current stage;k-nearest neighbours technique;steady-state stage;time window;voltage waveforms","","4","","11","","","10-13 June 2015","","IEEE","IEEE Conference Publications"
"Malware Function Classification Using APIs in Initial Behavior","N. Kawaguchi; K. Omote","Sch. of Inf. Sci., Japan Adv. Inst. of Sci. & Technol., Ishikawa, Japan","2015 10th Asia Joint Conference on Information Security","20150713","2015","","","138","144","Malware proliferation has become a serious threat to the Internet in recent years. Most of the current malware are subspecies of existing malware that have been automatically generated by illegal tools. To conduct an efficient analysis of malware, estimating their functions in advance is effective when we give priority to analyze. However, estimating malware functions has been difficult due to the increasing sophistication of malware. Although various approaches for malware detection and classification have been considered, the classification accuracy is still low. In this paper, we propose a new classification method which estimates malware's functions from APIs observed by dynamic analysis on a host. We examining whether the proposed method can correctly classify unknown malware based on function by machine learning. The results show that the our new method can classify each malware's function with an average accuracy of 83.4%.","","Electronic:978-1-4799-1989-5; POD:978-1-4799-1990-1; USB:978-1-4799-1988-8","10.1109/AsiaJCIS.2015.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153948","machine learning;malware classification","Accuracy;Data mining;Feature extraction;Machine learning algorithms;Malware;Software;Support vector machines","Internet;invasive software;learning (artificial intelligence);pattern classification","API;Internet;dynamic analysis;efficient malware analysis;illegal tools;initial behavior;machine learning;malware detection;malware function classification;malware proliferation","","1","","13","","","24-26 May 2015","","IEEE","IEEE Conference Publications"
"Graph Based Constrained Semi-Supervised Learning Framework via Label Propagation over Adaptive Neighborhood","Z. Zhang; M. Zhao; T. W. S. Chow","School of Computer Science and Technology, Soochow University, Suzhou, China","IEEE Transactions on Knowledge and Data Engineering","20150804","2015","27","9","2362","2376","A new graph based constrained semi-supervised learning (G-CSSL) framework is proposed. Pairwise constraints (PC) are used to specify the types (intra- or inter-class) of points with labels. Since the number of labeled data is typically small in SSL setting, the core idea of this framework is to create and enrich the PC sets using the propagated soft labels from both labeled and unlabeled data by special label propagation (SLP), and hence obtaining more supervised information for delivering enhanced performance. We also propose a Two-stage Sparse Coding, termed TSC, for achieving adaptive neighborhood for SLP. The first stage aims at correcting the possible corruptions in data and training an informative dictionary, and the second stage focuses on sparse coding. To deliver enhanced inter-class separation and intra-class compactness, we also present a mixed soft-similarity measure to evaluate the similarity/dissimilarity of constrained pairs using the sparse codes and outputted probabilistic values by SLP. Simulations on the synthetic and real datasets demonstrated the validity of our algorithms for data representation and image recognition, compared with other related state-of-the-art graph based semi-supervised techniques.","1041-4347;10414347","","10.1109/TKDE.2013.182","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682892","Constrained semi-supervised learning;Feature evaluation and selection;Machine learning;Multidimensional;Pattern analysis;adaptive neighborhood;label propagation;soft-similarity measure;sparse coding;subspace learning","Dictionaries;Encoding;Kernel;Noise;Semisupervised learning;Sparse matrices;Vectors","data handling;data structures;encoding;image recognition;learning (artificial intelligence)","PC sets;adaptive neighborhood;data representation;graph based constrained semi-supervised learning framework;graph based semi-supervised techniques;image recognition;inter-class separation;intra-class compactness;labeled data;outputted probabilistic values;pairwise constraints;soft labels;sparse codes;special label propagation;supervised information;two-stage sparse coding","","12","","50","","20131212","Sept. 1 2015","","IEEE","IEEE Journals & Magazines"
"Improved Threshold Selection by Using Calibrated Probabilities for Random Forest Classifiers","F. Baumann; J. Chen; K. Vogt; B. Rosenhahn","Inst. fur Informationsverarbeitung, Leibniz Univ. Hannover, Hannover, Germany","2015 12th Conference on Computer and Robot Vision","20150716","2015","","","155","160","Random Forest is a well-known ensemble learning method that achieves high recognition accuracies while preserving a fast training procedure. To construct a Random Forest classifier, several decision trees are arranged in a forest while a majority voting leads to the final decision. In order to split each node of a decision tree into two children, several possible variables are randomly selected while a splitting criterion is computed for each of them. Using this pool of possible splits, the Random Forest algorithm selects the best variable according to the splitting criterion. Often, this splitting is not reliable leading to a reduced recognition accuracy. In this paper, we propose to introduce an additional condition for selecting the best variable leading to an improvement of the recognition accuracy, especially for a smaller number of trees. We enhance the standard threshold selection by a quality estimation that is computed using a calibration method. The proposed method is evaluated on datasets for machine learning as well as object recognition.","","Electronic:978-1-4799-1986-4; POD:978-1-4799-1987-1","10.1109/CRV.2015.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158334","Machine Learning;Object Recognition;Random Forest;Splitting","Accuracy;Decision trees;Entropy;Reliability;Standards;Training;Vegetation","decision trees;image segmentation;learning (artificial intelligence);object recognition;probability","calibrated probabilities;calibration method;decision trees;learning method;machine learning;object recognition;quality estimation;random forest algorithm;random forest classifiers;splitting criterion;threshold selection","","0","","17","","","3-5 June 2015","","IEEE","IEEE Conference Publications"
"Edge-based depth gradient refinement for 2D to 3D learned prior conversion","J. L. Herrera; C. R. del-Blanco; N. Garcıa","Grupo de Tratamiento de Imagenes. ETSI Telecomunicacion. Universidad Politecnica de Madrid","2015 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)","20150817","2015","","","1","4","2D-to-3D conversion is an important task for reducing the current gap between the number of 3D displays and the available 3D content. Here, we present an automatic 2D-to-3D image conversion approach based on machine learning principles. Stemming from the hypothesis that images with a similar structure have likely a similar 3D structure, the depth of a query color image is estimated using a color plus depth image dataset. Clusters with common scene structure are computed offline. Then, a matching process is performed to select the cluster centroid which is the most similar to the query image. A prior depth map is computed fusing the depth maps of the images in this cluster. Then, an edge-based post-processing stage is applied to the prior depth map estimation to enhance the final scene depth estimation. Promising results are obtained in two commonly used databases achieving a similar performance to other much complex state-of-the-art approaches.","2161-2021;21612021","Electronic:978-1-4673-8090-4; POD:978-1-4673-8091-1","10.1109/3DTV.2015.7169364","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169364","2D-to-3D conversion;clustering;depth maps;depth prior;machine learning","Clustering algorithms;Color;Databases;Estimation;Image edge detection;Measurement;Three-dimensional displays","gradient methods;image colour analysis;image fusion;image matching;learning (artificial intelligence);natural scenes;pattern clustering;three-dimensional displays","2D to 3D learned prior conversion;3D display;automatic 2D-to-3D image conversion approach;edge-based depth gradient refinement;image fusion;image matching process;machine learning principlee;query color plus depth image;scene depth map estimation","","1","","13","","","8-10 July 2015","","IEEE","IEEE Conference Publications"
"Performance evaluation of information retrieval models in bug localization on the method level","M. Alduailij; M. Al-Duailej","Department of Computer Science, Western Michigan University, Kalamazoo, MI 49008","2015 International Conference on Collaboration Technologies and Systems (CTS)","20150820","2015","","","305","313","This study uses statistical inference to compare the performance of three text models used for bug localization in collaboration systems: Vector Space Model (VSM), Latent Semantic Indexing (LSI), and Latent Dirichlet Analysis (LDA) on the method level. After the three models are compared we confirm that VSM is the superior model. We then, point out which external factors i.e. methods lengths, queries lengths, methods documentation comments, products names and components names mentioned in bug reports affect VSM performance. We conclude that VSM performance is positively correlated with most of the tested factors. We believe our results can be helpful to: (i) text models developers, to understand the strengths and limitations of VSM for future development; (ii) bug localization programmers using classical VSM, to understand improved ways to prepare methods extracted from big data collaboration systems and (iii) bug reporters, to follow the most efficient methods presented in this work in reporting bugs to enhance the information retrieval process.","","CD-ROM:978-1-4673-7646-4; Electronic:978-1-4673-7648-8; POD:978-1-4673-7649-5","10.1109/CTS.2015.7210439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210439","discovery, collection, and extraction of information in big data sources;machine learning methods applied to big data analytics;natural language processing methodologies;performance and benchmarking for big data processing and analytics;semantic content extraction and analytics languages and techniques;text categorization and topic recognition","Big data;Computer bugs;Information retrieval;Large scale integration;Object oriented modeling;Sociology;Statistics","Big Data;inference mechanisms;program debugging;query processing;statistical analysis;text analysis","LDA;LSI;VSM;VSM performance evaluation;big data collaboration systems;bug localization programmers;bug reports;collaboration systems;documentation comments;information retrieval models;latent Dirichlet analysis;latent semantic indexing;method level bug localization;statistical inference;text model developer;vector space model","","0","","36","","","1-5 June 2015","","IEEE","IEEE Conference Publications"
"Maximum entropy property of discrete-time stable spline kernel","T. Ardeshiri; T. Chen","Dept. of Electr. Eng., Linkoping Univ., Linkoping, Sweden","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","3676","3680","In this paper, the maximum entropy property of the discrete-time first-order stable spline kernel is studied. The advantages of studying this property in discrete-time domain instead of continuous-time domain are outlined. One of such advantages is that the differential entropy rate is well-defined for discrete-time stochastic processes. By formulating the maximum entropy problem for discrete-time stochastic processes we provide a simple and self-contained proof to show what maximum entropy property the discrete-time first-order stable spline kernel has.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178657","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178657","Gaussian process;Machine learning;impulse response estimation;maximum entropy (MaxEnt)","Entropy;Gaussian processes;Indexes;Kernel;Splines (mathematics);White noise","maximum entropy methods;stochastic processes","continuous-time domain;discrete-time domain;discrete-time first-order stable spline kernel;discrete-time stochastic processes;maximum entropy property","","0","","","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Harmonising RES-E support schemes using design elements","K. K. Iychettira; P. Linares; R. A. Hakvoort","IIT - Institute for Research in Technology, Comillas Pontifical University, Madrid, Spain","2015 12th International Conference on the European Energy Market (EEM)","20150824","2015","","","1","5","In this paper, we present a new lens to study harmonization of RES-E support schemes. We collect data from existing RES-E support scheme designs, break up RES-E policies into their design elements, and relate each design element or policy characteristic to policy objectives. Based on these relations, which are complex, we use the decision tree learning method in order to understand exactly how relevant each design element is to a certain policy objective. With this knowledge it is possible to understand which attributes of an RES-E scheme must be harmonized or co-ordinated at the EU level and which ones could be left to the discretion of the member states.","2165-4077;21654077","Electronic:978-1-4673-6692-2; POD:978-1-4673-6693-9; USB:978-1-4673-6691-5","10.1109/EEM.2015.7216731","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7216731","Decision trees;design elements;harmonization;machine learning;renewable energy support schemes","Decision trees;Distortion;Electricity supply industry;Europe;Feeds;Investment;Training","decision trees;government policies;power markets","RES-E policies;RES-E support schemes;decision tree learning method","","0","","13","","","19-22 May 2015","","IEEE","IEEE Conference Publications"
"Shilling attack detection in Collaborative Recommender Systems using a Meta Learning strategy","W. Bhebe; O. P. Kogeda","Department of Computer Science, Tshwane University of Technology, Private Bag X680, Pretoria 0001, South Africa","2015 International Conference on Emerging Trends in Networks and Computer Communications (ETNCC)","20150813","2015","","","56","61","Collaborative Recommender Systems suggest items to a user based on other users past behaviour (items they once bought, viewed or selected and/or ratings they gave to those items). They are very effective in generating meaningful recommendations to a group of users for products or items that might interest them. However, since Collaborative filtering techniques depend on outside sources of information they are susceptible to profile injection attacks popularly known as shilling attacks. Shilling is a process in which syndicating users can connive to promote or demote a certain item. These mischievous users can consciously inject shilling profiles in an effort to bias the recommender system to their advantage. In this paper we seek to understand the degree to which shilling attacks can harm recommender systems and how these attacks can be detected. Firstly, we evaluate the vulnerabilities of collaborative filtering techniques in providing reliable recommendations. We study various attack strategies that manipulators use to attack recommender systems. Secondly we investigate the most suitable features that can be used to adequately identify shilling attacks. We propose the combiner strategy that combines multiple classifiers in an effort to detect shilling attacks. The diversity measure is used to determine the most suitable combination of classifiers. In this paper, we made use k-Nearest Neighbour, Support Vector Machines and Bayesian Networks as the initial base classifiers. The Naïve Bayes was used as a Meta Classifier. The proposed Meta-Learning classifier gave an overall performance of 99% and was found to be more superior to Neural Networks and k-Nearest Neighbor.","","DVD:978-1-4799-7705-5; Electronic:978-1-4799-7707-9; POD:978-1-4799-7708-6","10.1109/ETNCC.2015.7184808","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184808","Collaborative filtering;Machine Learning;Meta Learning;Recommender Systems;Shilling attacks","Classification algorithms;Collaboration;Motion pictures;Prediction algorithms;Recommender systems;Training","belief networks;collaborative filtering;pattern classification;recommender systems;security of data;support vector machines","Bayesian network;collaborative filtering technique;collaborative recommender system;k-nearest neighbour;metalearning classifier;shilling attack detection;support vector machine","","0","","17","","","17-20 May 2015","","IEEE","IEEE Conference Publications"
"Automatic acoustic analysis for biodiversity preservation: A multi-environmental approach","K. López-de-Ipiña; M. Iturrate; J. B. Alonso; B. Rodríguez-Herrera","Engineering School, 20018, Donostia-San Sebastian Spain, Universidad del Pa&#x00ED;s Vasco/Euskal Herriko Unibertsitatea","2015 4th International Work Conference on Bioinspired Intelligence (IWOBI)","20150716","2015","","","43","48","Biodiversity preservation is essential for environmental health, which is becoming a valuable indicator for quality of life for human being. In this sense, automatic and bioinspired intelligent system can provide powerful tools for monitoring, identification or tracking of species. Automatic acoustic analysis is a non-invasive methodology, easy to use and useful even in complex conditions and inaccessible environments. This work is focused on the development of an automatic system for analysis of environmental agents and their interaction and it is based on human hearing abilities and several Machine Learning paradigms. This preliminary system is evaluated over a multi-environmental database.","","Electronic:978-1-4673-7846-8; POD:978-1-4673-7847-5","10.1109/IWOBI.2015.7160142","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160142","Automatic acoustic analysis;Biodiversity preservation;Machine Learning;multi-environmental analysis","Acoustics;Biodiversity;Birds;Databases;Feature extraction;Insects;Speech","acoustic signal processing;hearing;learning (artificial intelligence)","automatic acoustic analysis;automatic intelligent system;biodiversity preservation;bioinspired intelligent system;human hearing ability;machine learning paradigm;multienvironmental approach","","0","","9","","","10-12 June 2015","","IEEE","IEEE Conference Publications"
"Proof-reading guidance in cell tracking by sampling from tracking-by-assignment models","M. Schiegg; B. Heuer; C. Haubold; S. Wolf; U. Koethe; F. A. Hamprecht","University of Heidelberg, HCI/IWR, Speyerer Str. 6, 69115 Heidelberg, Germany","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","394","398","Automated cell tracking methods are still error-prone. On very large data sets, uncertainty measures are thus needed to guide the expert to the most ambiguous events so these can be corrected with minimal effort. We present two easy-to-use methods to sample multiple proposal solutions from a tracking-by-assignment graphical model and experimentally evaluate the benefits of the uncertainty measures derived. Expert time for proof-reading is reduced greatly compared to random selection of predicted events.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163895","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163895","Cell tracking;machine learning;probabilistic graphical models;uncertainty","Biomedical measurement;Gaussian processes;Graphical models;Labeling;Measurement uncertainty;Proposals;Uncertainty","cellular biophysics;graph theory;image sampling;learning (artificial intelligence);medical image processing;object tracking;random processes","automated cell tracking methods;cell sampling;multiple proposal solutions;predicted events;proof-reading guidance;random selection;tracking-by-assignment graphical model;tracking-by-assignment models;uncertainty measures","","1","","16","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Learning-based detection of flow diverters in cerebral images","Y. J. Zhu","Electrical & Computer Engineering, Temple University, Philadelphia, PA, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1122","1125","We propose a machine learning-based method to automatically detect flow diverters in cerebral C-arm CT images. An appearance detector is learned to generate hypotheses of a flow diverter's location in a volumetric image. A probabilistic framework incorporating a local appearance and shape model is developed to trace the flow diverter. Promising results have been obtained on clinical data. The proposed method provides a potential solution to the automation of cerebral aneurysm treatment workflow and in particular the post-operative assessment of flow diverter placement.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164069","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164069","brain aneurysm;flow diverter detection;machine learning;stenting","Aneurysm;Computed tomography;Detectors;Estimation;Image segmentation;Shape;Training","brain;computerised tomography;learning (artificial intelligence);medical disorders;medical image processing;patient treatment;stents","cerebral C-arm CT images;cerebral aneurysm treatment workflow;computerised tomography;flow diverters;machine learning-based detection;probabilistic framework;shape model","","0","","10","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"A symmetric deformation-based similarity measure for shape analysis","S. Kolouri; D. Slepcev; G. K. Rohde","Center for Bioimage Informatics, Biomedical Engineering Department, Carnegie Mellon University, Pittsburgh, PA, 15213","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","314","318","Statistical modeling of cellular/subcellular shapes is required for quantifying shape variations and understanding complex cell behaviors. Such statistical models rely on the choice of a proper similarity measure. In this paper we introduce a symmetric deformation-based similarity measure for cellular shape analysis. The proposed method is based on finding an optimal diffeomorphic mapping between shape images that minimizes a physically meaningful energy function. We compare our proposed method to the large deformation dif-feomorphic metric mapping (LDDMM) and the standard Euclidean metric on a nuclei dataset and show that the proposed method outperforms the others in capturing the variations in the dataset.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163876","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163876","Shape analysis;image registration;machine learning","Biomedical measurement;Energy measurement;Euclidean distance;Optimization;Shape;Shape measurement","biological techniques;biology computing;biomechanics;cellular biophysics;deformation;image matching;statistical analysis","Euclidean metrics;cell behavior;cellular shape analysis;cellular-subcellular shape;energy function;large deformation diffeomorphic metric mapping;nuclei dataset;optimal diffeomorphic mapping;shape images;shape variation quantification;statistical model;symmetric deformation-based similarity measurement","","0","","16","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Informational modeling of tissue-like materials using ultrasound","C. Hoerig; J. Ghaboussi; M. Insana","Department of Bioengineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, U.S.A","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","239","242","The correlation between disease pathology and tissue stiffness can be exploited to detect and potentially diagnose abnormal tissue states. Elastography is an imaging modality that attempts to image tissue stiffness by measuring local displacements caused by an applied force and calculating a strain map. Some elasticity imaging techniques attempt to assign a material parameter, such as Young's or shear modulus, to the imaged region in an effort to increase specificity. Unfortunately, the inversion techniques require many simplifying assumptions which lead to errors in the parameter estimates. One possible solution to increase accuracy in estimation is to first build an empirical model of the tissue using measured force-displacement data, thus eliminating the need for a priori assumptions. We propose the use of informational models for this purpose.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163858","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163858","Elasticity Imaging;Machine Learning;Neural Networks","Artificial neural networks;Force measurement;Iron;Load modeling;Strain;Training;Young's modulus","Young's modulus;biomechanics;biomedical ultrasonics;diseases;displacement measurement;elasticity;physiological models;shear modulus","Young modulus;disease pathology;elasticity imaging techniques;elastography;force-displacement data measurement;inversion techniques;shear modulus;strain map calculation;tissue stiffness imaging;ultrasound","","1","","6","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Predictive Auto-scaling Techniques for Clouds Subjected to Requests with Service Level Agreements","A. Biswas; S. Majumdar; B. Nandy; A. El-Haraki","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada","2015 IEEE World Congress on Services","20150817","2015","","","311","318","This paper focuses research focuses on automatic provisioning of cloud resources performed by an intermediary enterprise that provides a virtual private cloud for a single client enterprise by using resources from a public cloud. This paper concerns auto-scaling techniques for dynamically controlling the number of resources used by the client enterprise. We focus on proactive auto-scaling that is based on predictions of future workload based on the past workload. The primary goal of the auto-scaling techniques is to achieve a profit for the intermediary enterprise while maintaining a desired grade of service for the client enterprise. The technique supports both on demand requests and requests with service level agreements (SLAs). This paper presents an auto-scaling algorithm and includes a discussion of system design and implementation experience for a prototype system that implements the technique. A detailed performance analysis based on measurements made on the prototype is presented.","2378-3818;23783818","Electronic:978-1-4673-7275-6; POD:978-1-4673-7276-3","10.1109/SERVICES.2015.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7196542","auto-scaling;dynamic resource provisioning;machine learning;resource allocation;resource management on clouds;scheduling with SLAs","Cloud computing;Machine learning algorithms;Maximum likelihood estimation;Measurement;Prediction algorithms;Prototypes;Support vector machines","cloud computing;contracts","SLA;client enterprise;predictive auto-scaling technique;service level agreement;virtual private cloud","","0","","23","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"Analytics: Key to Go from Generating Big Data to Deriving Business Value","D. Arora; P. Malik","Dept. of Electr. & Comput. Eng., Univ. of Victoria, Victoria, TX, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","20150813","2015","","","446","452","The potential to extract actionable insights from Big Data has gained increased attention of researchers in academia as well as several industrial sectors. The field has become interesting and problems look even more exciting to solve ever since organizations have been trying to tame large volumes of complex and fast arriving Big Data streams through newer computing paradigms. However, extracting meaningful and actionable information from Big Data is a challenging and daunting task. The ability to generate value from large volumes of data is an art which combined with analytical skills needs to be mastered in order to gain competitive advantage in business. The ability of organizations to leverage the emerging technologies and integrate Big Data into their enterprise architectures effectively depends on the maturity level of the technology and business teams, capabilities they develop as well as the strategies they adopt. In this paper, through selected use cases, we demonstrate how statistical analyses, machine learning algorithms, optimization and text mining algorithms can be applied to extract meaningful insights from the data available through social media, online commerce, telecommunication industry, smart utility meters and used for variety of business benefits, including improving security. The nature of applied analytical techniques largely depends on the underlying nature of the problem so a one-size-fits-all solution hardly exists. Deriving information from Big Data is also subject to challenges associated with data security and privacy. These and other challenges are discussed in context of the selected problems to illustrate the potential of Big Data analytics.","","Electronic:978-1-4799-8128-1; POD:978-1-4799-8129-8","10.1109/BigDataService.2015.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184914","algorithms;big data;machine learning;review","Algorithm design and analysis;Big data;Companies;Machine learning algorithms;Security;Sentiment analysis","Big Data;business data processing;data integration;data mining;data privacy;learning (artificial intelligence);optimisation;statistical analysis;text analysis","Big Data integration;Big Data streams;analytical skills;analytical techniques;business benehts;business teams;business value;computing paradigms;data privacy;data security;enterprise architectures;information extraction;large-data volumes;machine learning algorithm;maturity level;one-size-hts-all solution;online commerce;optimization algorithm;security improvement;smart utility meters;social media;statistical analysis;telecommunication industry;text mining algorithm","","1","","76","","","March 30 2015-April 2 2015","","IEEE","IEEE Conference Publications"
"Designing Medical Interactive Systems Via Assessment of Human Mental Workload","L. Longo","Sch. of Comput., Dublin Inst. of Technol., Dublin, Ireland","2015 IEEE 28th International Symposium on Computer-Based Medical Systems","20150727","2015","","","364","365","In clinical settings, Human-computer systems need to be designed in a way that medical errors are reduced and patient care is enhanced. Inspection methods are usually employed in HCI to assess usability of interactive systems. However, they do not consider the state of the operator while executing a task, the surrounding environment and the task demands. It is argued that assessing performance of operators is fundamental for designing optimal systems with which healthcare can be effectively delivered. The aim of our solution is to assess performance of operators employing the notion of Mental Workload (MWL) this being a construct believed to strongly correlate with performance. The proposal is to develop a model for MWL assessment using supervised machine learning. This model will be evaluated via user studies involving clinicians and operators interacting with a set of medical systems. Assessments of MWL will be compared and validated with objective indexes of performance such as error rate and task execution time.","1063-7125;10637125","Electronic:978-1-4673-6775-2; POD:978-1-4673-6776-9","10.1109/CBMS.2015.67","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167521","Human Mental Workload;Human-Computer Interaction;Interactive Systems;Machine Learning;Medical applications","Context;Ergonomics;Human computer interaction;Inspection;Interactive systems;Performance evaluation;Usability","brain-computer interfaces;health care;interactive systems;learning (artificial intelligence);medical computing;neurophysiology","healthcare;human MWL assessment;human-computer system;inspection method;machine learning;medical error rate;medical interactive system;mental workload;patient care;task execution time","","2","","7","","","22-25 June 2015","","IEEE","IEEE Conference Publications"
"A Hidden Markov Model for Vehicle Detection and Counting","N. Miller; M. A. Thomas; J. A. Eichel; A. Mishra","Miovision Technol. Inc., Kitchener, ON, Canada","2015 12th Conference on Computer and Robot Vision","20150716","2015","","","269","276","To reduce roadway congestion and improve traffic safety, accurate traffic metrics, such as number of vehicles travelling through lane-ways, are required. Unfortunately most existing infrastructure, such as loop-detectors and many video detectors, do not feasibly provide accurate vehicle counts. Consequently, a novel method is proposed which models vehicle motion using hidden Markov models (HMM). The proposed method represents a specified small region of the roadway as 'empty', 'vehicle entering', 'vehicle inside', and 'vehicle exiting', and then applies a modified Viterbi algorithm to the HMM sequential estimation framework to initialize and track vehicles. Vehicle observations are obtained using an Adaboost trained Haar-like feature detector. When tested on 88 hours of video, from three distinct locations, the proposed method proved to be robust to changes in lighting conditions, moving shadows, and camera motion, and consistently out-performed Multiple Target Tracking (MTT) and Virtual Detection Line(VDL) implementations. The median vehicle count error of the proposed method is lower than MTT and VDL by 28%, and 70% respectively. As future work, this algorithm will be implemented to provide the traffic industry with improved automated vehicle counting, with the intent to eventually provide real-time counts.","","Electronic:978-1-4799-1986-4; POD:978-1-4799-1987-1","10.1109/CRV.2015.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158929","Haar-like features;Intelligent Transportation Systems;hidden Markov models;machine learning;vehicle detection;vehicle tracking","Detectors;Hidden Markov models;Real-time systems;Target tracking;Vehicle detection;Vehicles","Haar transforms;feature extraction;hidden Markov models;image sensors;object detection;road vehicles;traffic engineering computing","Adaboost trained Haar-like feature detector;HMM sequential estimation framework;MTT;VDL implementations;automated vehicle counting;camera motion;hidden Markov model;lighting conditions;median vehicle count error;moving shadows;multiple target tracking;roadway congestion;traffic metrics;traffic safety;vehicle counting;vehicle counts;vehicle detection;vehicle entering;vehicle exiting;vehicle inside;vehicle motion;vehicle tracking;vehicle travel;virtual detection line","","1","","16","","","3-5 June 2015","","IEEE","IEEE Conference Publications"
"Classification of water for production using parameters in real time","J. T. C. Mariño; O. R. Pacheco; M. Á. G. López","Instituto de Ingenier&#x00ED;a Electr&#x00F3;nica y Telem&#x00E1;tica de Aveiro - IEETA, Universidad de Aveiro, Aveiro, Portugal","2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","20150730","2015","","","1","5","In this paper, a new classification method for production water is proposed, based on so real-time measured parameters. The classification method consists of three steps: 1) An initial classification of the Water Quality Index is computed using the method proposed by KUMAR; 2) Feature selection based on random forest (specifically based on the method varSelRF); and 3) Training of classifiers using different configurations of heuristic decision trees. A total of 4 datasets (5090 instances of 8 features each) representative of water samples from Portugal, Canada, Mexico, and Romania were used for method validation. The dataset was group in two families of different classes: binary (good and regular water) and multiclass (good, regular and bad water). Final classification accuracy reached 94.85% for the binary family and 91.73% for the multiclass family. The contribution consists of a continuous monitoring system to detect (in real time) dramatic changes in water quality and provide tools for historical studies behaviour in strategic points.","2166-0727;21660727","Electronic:978-9-8998-4345-5; POD:978-1-4799-8330-8","10.1109/CISTI.2015.7170477","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170477","Data Mining;Hydroinformatic;Machine Learning;Water Quality","Decision trees;Guidelines;Indexes;Monitoring;Real-time systems;Water pollution;Water resources","data mining;decision trees;geophysics computing;learning (artificial intelligence);pattern classification;water quality","Canada;KUMAR;Mexico;Portugal;Romania;binary family;classifier training;continuous monitoring system;data mining;feature selection;heuristic decision trees;initial water quality index classification;machine learning;multiclass family;production water classification method;random forest;real-time measured parameters;varSelRF method","","0","","17","","","17-20 June 2015","","IEEE","IEEE Conference Publications"
"A learning-based approach to direction of arrival estimation in noisy and reverberant environments","X. Xiao; S. Zhao; X. Zhong; D. L. Jones; E. S. Chng; H. Li","Temasek Lab@NTU, Nanyang Technological University, Singapore","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","2814","2818","This paper presents a learning-based approach to the task of direction of arrival estimation (DOA) from microphone array input. Traditional signal processing methods such as the classic least square (LS) method rely on strong assumptions on signal models and accurate estimations of time delay of arrival (TDOA) . They only work well in relatively clean conditions, but suffer from noise and reverberation distortions. In this paper, we propose a learning-based approach that can learn from a large amount of simulated noisy and reverberant microphone array inputs for robust DOA estimation. Specifically, we extract features from the generalised cross correlation (GCC) vectors and use a multilayer perceptron neural network to learn the nonlinear mapping from such features to the DOA. One advantage of the learning based method is that as more and more training data becomes available, the DOA estimation will become more and more accurate. Experimental results on simulated data show that the proposed learning based method produces much better results than the state-of-the-art LS method. The testing results on real data recorded in meeting rooms show improved root-mean-square error (RMSE) compared to the LS method.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178484","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178484","direction of arrival;least squares;machine learning;microphone arrays;neural networks","Arrays;Direction-of-arrival estimation;Estimation;Robustness;Speech;Training;Training data","acoustic signal processing;direction-of-arrival estimation;feature extraction;learning (artificial intelligence);microphone arrays;multilayer perceptrons;reverberation;vectors","GCC;direction of arrival estimation;feature extraction;generalised cross correlation vectors;learning-based approach;multilayer perceptron neural network;noisy environment;nonlinear mapping;reverberant environment;reverberant microphone array;robust DOA estimation","","4","","27","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"An Ultrasonographic Risk Score For Detecting Symptomatic Carotid Atherosclerotic Plaques","D. Afonso; J. Seabra; L. M. Pedro; J. F. e. Fernandes; J. M. Sanches","Department of Bioengineering, Institute for Systems and Robotics, Instituto Superior T&#233;cnico, Technical University of Lisbon, Lisbon, Portugal","IEEE Journal of Biomedical and Health Informatics","20150723","2015","19","4","1505","1513","This paper proposes a risk score computed from ultrasound data that correlates to plaque activity. It has the twofold purpose of detecting symptomatic plaques and estimating the likelihood of the asymptomatic lesion to become symptomatic. The proposed ultrasonographic activity index (UAI) relies on the plaque active profile, which is a combination of the most discriminate ultrasound parameter associated with symptoms. These features are extracted by the automatic algorithm and also by the physician from the ultrasound images and from some transformations on it, such as monogenic decomposition, which is a novelty in this clinical problem. This information is used to compute a risk score from the conditional probabilities of either symptomatic or asymptomatic groups. Symptom detection performance is evaluated on a transversal dataset of 146 plaques, where UAI obtained 83.5% accuracy, 84.1% sensitivity, and 83.7% specificity. Performance is also assessed on a longitudinal study of 112 plaques, where UAI shows a significant improvement over the gold standard degree of stenosis, demonstrating higher power at predicting which asymptomatic plaques developed symptoms in an average follow-up of ten months. Results suggest that this score could have a positive impact on early stroke prevention and treatment planning.","2168-2194;21682194","","10.1109/JBHI.2014.2359236","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6905710","Atherosclerotic carotid disease;computer-aided detection and diagnosis;machine learning;pattern recognition and classification;probabilistic and statistical methods;receiver–operator curve (ROC) analysis;risk score;ultrasound;vessels","Biomedical imaging;Feature extraction;Indexes;Medical services;Speckle;Ultrasonic imaging;Vectors","biomedical ultrasonics;feature extraction;medical image processing","UAI detection;accuracy;asymptomatic lesion;early stroke prevention;feature extraction;likelihood estimation;monogenic decomposition;plaque activity;sensitivity;specificity;symptomatic carotid atherosclerotic plaques;symptomatic lesion;symptomatic plaques;treatment planning;ultrasonographic risk score;ultrasound data;ultrasound images","0","2","","34","","20140919","July 2015","","IEEE","IEEE Journals & Magazines"
"Towards Making Systems Forget with Machine Unlearning","Y. Cao; J. Yang","","2015 IEEE Symposium on Security and Privacy","20150720","2015","","","463","480","Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly. This paper focuses on making learning systems forget, the process of which we call machine unlearning, or simply unlearning. We present a general, efficient unlearning approach by transforming learning algorithms used by a system into a summation form. To forget a training data sample, our approach simply updates a small number of summations -- asymptotically faster than retraining from scratch. Our approach is general, because the summation form is from the statistical query learning in which many machine learning algorithms can be implemented. Our approach also applies to all stages of machine learning, including feature selection and modeling. Our evaluation, on four diverse learning systems and real-world workloads, shows that our approach is general, effective, fast, and easy to use.","1081-6011;10816011","Electronic:978-1-4673-6949-7; POD:978-1-4673-6950-3","10.1109/SP.2015.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163042","Adversarial Machine Learning;Forgetting System;Machine Unlearning","Computational modeling;Data models;Data privacy;Feature extraction;Learning systems;Machine learning algorithms;Training data","data privacy;learning (artificial intelligence);recommender systems;security of data","complex data propagation network;data lineage;feature modeling;feature selection;forgetting systems;machine learning algorithms;machine unlearning;privacy risks;recommendation engine;security perspective;statistical query learning;summation form;usability perspective","","1","","80","","","17-21 May 2015","","IEEE","IEEE Conference Publications"
"On the Pitfalls of Using Arbiter-PUFs as Building Blocks","G. T. Becker","Horst G&#246;rtz Institute for IT Security, Ruhr University Bochum, Bochum, Germany","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20150724","2015","34","8","1295","1307","Physical unclonable functions (PUFs) have emerged as a promising solution for securing resource-constrained embedded devices such as RFID tokens. PUFs use the inherent physical differences of every chip to either securely authenticate the chip or generate cryptographic keys without the need of nonvolatile memory. However, PUFs have shown to be vulnerable to model building attacks if the attacker has access to challenge and response pairs. In these model building attacks, machine learning is used to determine the internal parameters of the PUF to build an accurate software model. Nevertheless, PUFs are still a promising building block and several protocols and designs have been proposed that are believed to be resistant against machine learning attacks. In this paper, we take a closer look at two such protocols, one based on reverse fuzzy extractors and one based on pattern matching. We show that it is possible to attack these protocols using machine learning despite the fact that an attacker does not have access to direct challenge and response pairs. The introduced attacks demonstrate that even highly obfuscated responses can be used to attack PUF protocols. Hence, this paper shows that even protocols in which it would be computationally infeasible to compute enough challenge and response pairs for a direct machine learning attack can be attacked using machine learning.","0278-0070;02780070","","10.1109/TCAD.2015.2427259","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096998","Evolution Strategies;Evolution strategies (ES);Machine Learning;Physical Unclonable Functions;Reverse Fuzzy Extractor;machine learning;physical unclonable functions (PUFs);reverse fuzzy extractor","Authentication;Buildings;Computational modeling;Delays;Error correction codes;Protocols","asynchronous circuits;cryptography;fuzzy set theory;learning (artificial intelligence);pattern matching;protocols;radiofrequency identification;random-access storage","PUF protocol;RFID token;arbiter-PUF;building block;chip authentication;cryptographic key;machine learning;model building attack;nonvolatile memory;pattern matching;physical unclonable function;radiofrequency identification;resource-constrained embedded device;response pair;reverse fuzzy extractor","","5","","21","","20150428","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Identifying the Culprits Behind Network Congestion","A. Bhatele; A. R. Titus; J. J. Thiagarajan; N. Jain; T. Gamblin; P. T. Bremer; M. Schulz; L. V. Kale","Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA","2015 IEEE International Parallel and Distributed Processing Symposium","20150720","2015","","","113","122","Network congestion is one of the primary causes of performance degradation, performance variability and poor scaling in communication-heavy parallel applications. However, the causes and mechanisms of network congestion on modern interconnection networks are not well understood. We need new approaches to analyze, model and predict this critical behaviour in order to improve the performance of large-scale parallel applications. This paper applies supervised learning algorithms, such as forests of extremely randomized trees and gradient boosted regression trees, to perform regression analysis on communication data and application execution time. Using data derived from multiple executions, we create models to predict the execution time of communication-heavy parallel applications. This analysis also identifies the features and associated hardware components that have the most impact on network congestion and intern, on execution time. The ideas presented in this paper have wide applicability: predicting the execution time on a different number of nodes, or different input datasets, or even for an unknown code, identifying the best configuration parameters for an application, and finding the root causes of network congestion on different architectures.","1530-2075;15302075","Electronic:978-1-4799-8649-1; POD:978-1-4799-8650-7","10.1109/IPDPS.2015.92","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161501","congestion;interconnection network;machine learning;modeling;performance prediction;root cause","Data models;Hardware;Prediction algorithms;Predictive models;Regression tree analysis;Three-dimensional displays;Vegetation","learning (artificial intelligence);parallel processing;regression analysis;trees (mathematics)","communication-heavy parallel application;extremely randomized tree;gradient boosted regression tree;network congestion;regression analysis;supervised learning algorithm","","2","","20","","","25-29 May 2015","","IEEE","IEEE Conference Publications"
"Big Data Analytics Framework for System Health Monitoring","B. Xu; S. A. Kumar","","2015 IEEE International Congress on Big Data","20150820","2015","","","401","408","In this paper, we present our Machine Learning (ML) based big data analytics framework that we tested to improve the quality and performance of Auxiliary Power Units (APU) health monitoring services. We are motivated to develop and apply practical and useful big data analytics technologies for industrial applications in aerospace and aviation. Key contributions of our work include the development and use of our ML algorithms that have been tested and used to analyze multiple data sources and to provide useful insights and increase the ability to predict (1) APU wear from 39%to 56% and (2) APU shutdown events from 19% to 60%. Such system health monitoring can be integrated with the widely used condition based maintenance (CBM) services. Users can use this cloud based analytic toolset and access the big data through any devices (PCs, Tablets, smart phones) anytime and anywhere.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207250","Big Data Analytics;CBM;Machine Learning Algorithms;Map-Reduce;System health Monitoring","Aircraft;Big data;Data mining;Data models;Distributed databases;Maintenance engineering;Monitoring","Big Data;cloud computing;condition monitoring;data analysis;learning (artificial intelligence);maintenance engineering;power engineering computing","APU;CBM services;ML;aerospace;auxiliary power units;aviation;big data analytics framework;cloud based analytic toolset;condition based maintenance services;industrial applications;machine learning;system health monitoring","","0","","33","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"A Hybrid Color Plane Approach Towards Color Based Object Detection and Modeling of a Real-Time Gesture Based Intelligent Virtual Aid Using Artificial Neural Network","P. Das; A. Mukherjee; A. Dey; D. Kundu; S. Ghosh; S. D. Gupta","West Bengal Univ. of Technol., Kolkata, India","2015 International Conference on Computing Communication Control and Automation","20150716","2015","","","868","874","Augmented and Virtual Reality has become a highly researched subfield in the Computer Vision domain. Among various researched topics, object detection and tracking is a major field of interest, with various algorithms being developed for it. Various algorithms exist that employs different approaches to solving the problem of object detection. The approach that we take is a mixture of investigating and evaluating the previous approaches and coming up with a novel algorithm that is able to detect and track objects in real-time that is not tightly bound to specific colours. In order to implement a dynamic algorithm, we look into various colour planes and then take in multiple dimensions from a given colour plane and use a combination of the results obtained from the individual dimensions to get a result that is capable of detecting various range of colours. After the objects are satisfactorily detected, we proceed to track them and implement gestures that allow the users to elect a single colour to be tracked without any direct interactions, using a novel algorithm that we developed over the course of the research. After the election, the algorithm is expanded further to be able to integrate the ability to detect various gestures and patterns. The system is also capable of basic intelligence and is able to modify its phases as necessary based on the user's interaction with the system. As such most of the system is user action dependent and free of any physical interaction. All the while, the algorithm is designed to be able to do all the processing in real-time.","","Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0","10.1109/ICCUBEA.2015.173","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155971","Artificial Neural Networks;Colour Planes;Gesture Recognition;Machine Learning;Object Detection;Pattern Recognition;Virtual Reality","Cameras;Classification algorithms;Color;Image color analysis;Object detection;Pattern recognition;Real-time systems","augmented reality;computer vision;gesture recognition;image colour analysis;neural nets;object detection;object tracking","artificial neural network;augmented reality;basic intelligence;color based object detection;computer vision;dynamic algorithm;hybrid color plane approach;object tracking;real-time gesture based intelligent virtual aid;user interaction;virtual reality","","0","","15","","","26-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"A case of precision-tunable STT-RAM memory design for approximate neural network","Y. Wang; L. Song; Y. Han; Y. Cheng; H. Li; X. Li","State Key Lab of Computer Architecture, Institute of Computing technology, Chinese Academy of Sciences","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","20150730","2015","","","1534","1537","Multi-level STT-RAM cell is able to boost the memory density at the expense of read/write reliability. However, the induced data integrity issue in STT-RAM memory can be effectively masked by a wide spectrum of applications with intrinsic forgiveness, which belong to the specific domain such as multimedia, synthesis and mining. In this work, we leverage the reconfigurable capability of MLC STT-RAM to provide variable-precision data storage for popular machine learning architectures. The targeted STT-RAM memory design is able to transform between multiple work modes and adaptable to meet the varying quality constraint of approximate applications. Particularly, we demonstrate the concept of precision-tunable STT-RAM memory with the emerging Convolution Neural Network accelerators and elaborate on the data mapping policy in STT-RAM memory to achieve the best energy-efficiency.","0271-4302;02714302","Electronic:978-1-4799-8391-9; POD:978-1-4799-8392-6","10.1109/ISCAS.2015.7168938","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168938","Machine Learning;Neural Network;STT-RAM","Accuracy;Buffer storage;Computer architecture;Random access memory;Reliability;Resistance;Switches","CMOS memory circuits;learning (artificial intelligence);logic design;neural nets;random-access storage","approximate neural network;convolution neural network accelerators;data integrity issue;data mapping policy;machine learning architectures;memory density;precision-tunable STT-RAM memory design","","0","","11","","","24-27 May 2015","","IEEE","IEEE Conference Publications"
"Reacting to different types of concept drift with adaptive and incremental one-class classifiers","B. Krawczyk; M. Woźniak","Department of Systems and Computer Networks, Wroc&#x0142;aw University of Technology, Poland","2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)","20150806","2015","","","30","35","Modern computer systems generate massive amounts of data in real-time. We have come to the age of big data, where the amount of information exceeds the perceptive abilities of any human being. Frequently the massive data collections arrive over time, in the form of a data stream. Not only the volume and velocity of data poses a challenge for machine learning systems, but also its variability. Such an environment may have non-stationary properties, i.e. change its characteristic over time. This phenomenon is known as concept drift, and is considered as one of the main challenges for moder learning systems. In this paper, we propose to investigate different methods for handling concept drift with adaptive soft one-class classifiers. One-class classification is a promising direction in data stream analytics, as it allows for a novelty detection, data description and learning with limited access to class labels. We describe an adaptive model of Weighted One-Class Support Vector Machine, augmented with mechanisms for incremental learning and forgetting. These allow for our models to swiftly adapt to changes in data, without any need for a dedicated drift detector. We carry out an experimental analysis of the behavior of our method with different forgetting rates for various types of concept drift. Additionally, we compare our classifier with state-of-the-art one-class methods for streaming data. We observe, that our adaptive soft one-class model can efficiently handle different types of concept drifts, while delivering a highly satisfactory accuracy for streaming data.","","Electronic:978-1-4799-8322-3; POD:978-1-4799-8323-0","10.1109/CYBConf.2015.7175902","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175902","big data;concept drift;data streams;forgetting;incremental learning;machine learning;one-class classification","Accuracy;Adaptation models;Benchmark testing;Data models;Memory management;Support vector machines;Training","Big Data;data analysis;learning (artificial intelligence);pattern classification;support vector machines","adaptive soft one-class classifiers;big data;concept drift;data stream analytics;incremental learning;incremental one-class classifiers;machine learning systems;weighted one-class support vector machine","","1","","12","","","24-26 June 2015","","IEEE","IEEE Conference Publications"
