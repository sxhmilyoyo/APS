"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6022751,6107359,6106534,6107342,6107874,5995280,6106947,6106551,6103780,6104730,6102485,6103421,6102462,6103673,6104601,6103241,6101282,6104732,6104734,5672612,6103137,6103528,6103156,6103146,6104736,6103506,6103455,6103210,6098705,6100120,6100102,6100298,6100782,6098495,6044741,6100119,6100057,6098707,6095947,6096689,6096643,6096651,6094046,5734815,5756649,6088166,6090905,6088094,6093579,6093850,6088840,6092574,6093355,6093353,6093576,6088209,6093405,6093575,6088887,6093316,6093344,6093449,6092329,6092319,6092326,6089624,6093798,6089130,6088165,6088117,6088153,6092712,6093329,6093582,6092243,6092050,6093600,6093285,6088199,6085973,6087091,6085999,6084194,5981411,6026222,6079774,6080778,6079657,6080820,6080788,6079868,6082335,6080758,6079667,6079661,6080816,6078263,6078266,6076337,6078304",2017/05/04 22:29:27
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"An Unsupervised Approach for Identifying Spammers in Social Networks","M. Bouguessa","Dept. d'Inf. et d'Ing. Gatineau, Univ. du Quebec en Outaouais, Gatineau, QC, Canada","2011 IEEE 23rd International Conference on Tools with Artificial Intelligence","20111215","2011","","","832","840","This paper proposes an unsupervised method for automatic identification of spammers in a social network. In our approach, we first investigate the link structure of the network in order to derive a legitimacy score for each node. Then we model these scores as a mixture of beta distributions. The number of components in the mixture is determined by the integrated classification likelihood Bayesian information criterion, while the parameters of each component are estimated using the expectation-maximization algorithm. This method allows us to automatically discriminate between spam senders and legitimate users. Experimental results show the suitability of the proposed approach and compare its performance to that of a previous fully-supervised method. We also illustrate our approach through a test application to Yahoo! Answers, a large question-answering web service that is particularly rich in the amount and types of content and social interactions represented.","1082-3409;10823409","Electronic:978-0-7695-4956-7; POD:978-1-4577-2068-0","10.1109/ICTAI.2011.130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103421","beta mixture model;social networks;spammers identification;unsupervised learning","Clustering algorithms;Electronic mail;Feature extraction;Measurement;Partitioning algorithms;Shape;Social network services","Bayes methods;Web services;expectation-maximisation algorithm;question answering (information retrieval);social networking (online);unsolicited e-mail;unsupervised learning","Yahoo! Answers;beta distributions;content interactions;expectation-maximization algorithm;integrated classification likelihood Bayesian information criterion;legitimacy score;legitimate users;question-answering Web service;social interactions;social network analysis;spam senders;spammer automatic identification;unsupervised method","","5","","27","","","7-9 Nov. 2011","","IEEE","IEEE Conference Publications"
"Influence of search engines on customer decision process","M. Zgódka","Warsaw School of Economics - Szko&#x0142;a G&#x0142;&#x00F3;wna Handlowa, w Warszawie Al. Niepodleg&#x0142;o&#x015B;ci 162, 02-554 Warsaw, Poland","2011 Federated Conference on Computer Science and Information Systems (FedCSIS)","20111114","2011","","","341","344","This article summarizes customer decision process focusing on information search. It explains the role and use communication channels that are used during information search, particularly the internet. It describes the internet and the role of search engines during information search. It explains the use of search engines and provides better understanding of ways in which search engines support customer decision process such as reduction of information search cost, higher involvement in the search process and increased ability to search for information. It also identifies some possible disadvantages like information irrelevancy or invisible web. Paper aims to identify the influence of search engines on information search phase of customer decision process.","","Electronic:978-83-60810-39-2; POD:978-1-4577-0041-5; USB:978-83-60810-35-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078304","","Browsers;Consumer behavior;Google;Indexes;Internet;Media;Search engines","Internet;consumer behaviour;decision making;information retrieval;search engines","Internet;communication channels;customer decision process;information irrelevancy;information search cost;invisible Web;search engines;search process involvement","","0","","34","","","18-21 Sept. 2011","","IEEE","IEEE Conference Publications"
"An Encrypted Index Mechanism in Ciphertext Retrieval System","L. Peng; R. Li; H. Wang; X. Gu; K. Wen; Z. Lu","Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China","2011 Eighth Web Information Systems and Applications Conference","20111201","2011","","","131","136","The cipher text-based full-text retrieval technology is proposed to satisfy the pressing demands for high security in massive information sharing applications. Based on the analysis of general technology of full-text indexing, we propose an improved index structure that supports cipher text indexing and retrieval. Then we present approaches to construct, maintain and retrieve cipher text indexes. Meanwhile, we implement a cipher text-based full-text retrieval system, which allows full-text search on the encrypted documents in multiple formats without decryption. Finally, we carry out the experiments and evaluate the performance of cipher text index construction and retrieval.","","Electronic:978-0-7695-4555-4; POD:978-1-4577-1812-0","10.1109/WISA.2011.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093579","","Encryption;Indexing;Libraries;Sorting","cryptography;indexing;information retrieval;text analysis","cipher text index construction;cipher text indexing;cipher text-based full-text retrieval technology;document encryption;encrypted index mechanism;full-text indexing;full-text retrieval system;full-text searching;index structure;information sharing application;performance evaluation","","1","","10","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"ICT + PBL = holistic learning solution: UTeM's experience","F. Shahbodin; M. Yusoff; C. K. N. C. K. Mohd","Interactive Media Department, Faculty of Information and Communication, Technology, Universiti Teknikal Malaysia Melaka (UTeM), Hang Tuah Jaya, 76100, Durian Tunggal, Melaka, Malaysia","2011 Sixth International Conference on Digital Information Management","20111201","2011","","","322","326","This paper highlights how ICT could be integrated in the process of teaching and learning in the Problem Based Learning (PBL) environment. The main focus is integrating the ICT components such as multimedia and internet technologies as a tool for PBL learning environment, and utilizing the PBL approach for the delivering instructions in the teaching and learning process at UTeM. This paper also shares findings on the effectiveness of PBLAssess which have been developed in this study. Fifty-six respondents (second year students) enrolled for the Human Computer Interaction course are selected for this study. Two research instruments are developed for the purpose of evaluating students' performances and preferences which include a set of questionnaire and prototype known as PBLAssess. Further some of the current work on integrating ICT and PBL learning environment are also shared. Understanding both the current state of art for PBL and future prospects are the key issues in setting an agenda for future research and development in PBL.","Pending","Electronic:978-1-4577-1539-6; POD:978-1-4577-1538-9","10.1109/ICDIM.2011.6093355","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093355","ICT;Internet;Learning Process;Multimedia;PBL","Collaboration;Education;Internet;Multimedia communication;Problem-solving;Prototypes;User interfaces","Internet;human computer interaction;learning (artificial intelligence);multimedia computing;question answering (information retrieval);teaching","ICT;Internet technology;PBL learning environment;UTeM;holistic learning solution;human computer interaction;learning process;multimedia technology;problem based learning;questionnaire set;research instrument;teaching process","","0","","12","","","26-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"A modified gray-level difference algorithm for analysing Gaussian Blurred texture images","R. Zhang; X. Qian; D. Ye","Graduate School at ShenZhen, Tsinghua University, China","2011 4th International Congress on Image and Signal Processing","20111212","2011","2","","833","837","A modified texture feature extraction method based on gray-level difference (GLD) algorithm is presented in this paper. Briefly, the proposed method is composed of four steps: 1) the variance of the image's texture part is estimated and compared to the preset threshold; 2) the Wiener filter was applied to remove Gaussian blur noise iteratively if the variance is lower than the threshold; 3) until the variance is higher than the threshold and then iteration stops; 4) the conventional GLD algorithm is used to extract the texture information from the processed texture image. To measure the discrimination performance of the new algorithm, Experiments are operated and the results indicate that the new method is better than the conventional GLD algorithm while extracting textural information from image with noise, especially when contaminated by Gaussian blur noise.","","Electronic:978-1-4244-9306-7; POD:978-1-4244-9304-3","10.1109/CISP.2011.6100298","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100298","Gray-Level Difference (GLD);confusion matrix;texture feature;wiener filter","Entropy;Feature extraction;Image restoration;Noise;Robustness;Schedules;Wiener filter","Gaussian noise;feature extraction;image denoising;image texture;information retrieval;iterative methods","Gaussian blur noise;Gaussian blurred texture image;Wiener filter;modified gray level difference algorithm;modified texture feature extraction method;preset threshold;texture information extraction","","0","","5","","","15-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"Learning contextual relevance of audio segments using discriminative models over AUD sequences","S. Chaudhuri; B. Raj","Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA - 15213, USA","2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","20111117","2011","","","197","200","Effective retrieval of multimodal data involves performing accurate segmentation and analysis of such data. With easy access to a number of audio and video sharing platforms online, user-generated content with considerably less than ideal recording conditions has increased rapidly. One major issue with such content is the presence of semantically irrelevant segments in such recordings. This leads to the presence of considerable contextual noise in such recordings that makes analysis difficult. In this paper, we present a discriminative large-margin based approach that uses annotated data to understand which parts of the audio are relevant (while noting that the notion of relevance could be extremely subjective and potentially challenging to define), and can automatically extract such segments from new audio.","1931-1168;19311168","Electronic:978-1-4577-0693-6; POD:978-1-4577-0692-9; USB:978-1-4577-0691-2","10.1109/ASPAA.2011.6082335","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082335","AUDs;audio segment selection;large margin discriminative training","Acoustics;Context;Data mining;Feature extraction;Sports equipment;Training;Training data","audio signal processing;content-based retrieval;data analysis;image segmentation;information retrieval;learning (artificial intelligence);sequences;speech recognition","AUD sequences;audio segment selection;audio sharing;contextual noise;data analysis;data segmentation;discriminative models;learning contextual relevance;multimodal data retrieval;semantic;video sharing","","1","","10","","","16-19 Oct. 2011","","IEEE","IEEE Conference Publications"
"Analysis of large digital collections with interactive visualization","W. Xu; M. Esteva; S. D. Jain; V. Jain","The University of Texas at Austin, USA","2011 IEEE Conference on Visual Analytics Science and Technology (VAST)","20111215","2011","","","241","250","To make decisions about the long-term preservation and access of large digital collections, archivists gather information such as the collections' contents, their organizational structure, and their file format composition. To date, the process of analyzing a collection - from data gathering to exploratory analysis and final conclusions - has largely been conducted using pen and paper methods. To help archivists analyze large-scale digital collections for archival purposes, we developed an interactive visual analytics application. The application narrows down different kinds of information about the collection, and presents them as meaningful data views. Multiple views and analysis features can be linked or unlinked on demand to enable researchers to compare and contrast different analyses, and to identify trends. We describe and present two user scenarios to show how the application allowed archivists to learn about a collection with accuracy, facilitated decision-making, and helped them arrive at conclusions.","","Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5","10.1109/VAST.2011.6102462","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102462","Digital collections;archival analysis;data curation;visual analytics","Analytical models;Data mining;Data visualization;Image color analysis;Layout;Rendering (computer graphics);Visualization","data analysis;data visualisation;decision making;information retrieval systems;records management","collection contents;data gathering;decision-making;exploratory analysis;file format composition;interactive visual analytics application;interactive visualization;large digital collection access;large digital collection analysis;long-term preservation;organizational structure","","3","1","34","","","23-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Impact of Low-Cost, On-demand, Information Access in a Remote Ghanaian Village","C. Schmidt; T. Gorman; A. Bayor; M. S. Gary","Literacy Bridge, Seattle, WA, USA","2011 IEEE Global Humanitarian Technology Conference","20111215","2011","","","419","425","Emerging technologies provide information topeople living in rural poverty. However, using information toaffect health or farming practices requires overcoming uniquechallenges including illiteracy and lack of electricity. Weexamine the effects of a low-cost audio computer (""TalkingBook"")-a handheld device enabling users to create, listen to,and copy recordings-for improving learning and knowledgesharing in such environments. In northern Ghana, we studiedthe impact of giving rural villagers on-demand access toguidance created by local experts. Our evaluation showsTalking Books significantly impact learning, behavior change,and crop yields in a village with low literacy rates and noelectricity.","","Electronic:978-1-61284-635-4; POD:978-1-61284-634-7","10.1109/GHTC.2011.88","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103673","Knowledge transfer;agriculture production;illiteracy;information dissemination;low-cost technology","Agriculture;Animals;Educational institutions;Interviews;Lead;Production;Training","agricultural engineering;agriculture;information retrieval;knowledge management;notebook computers","copy recordings;farming;handheld device;illiteracy;knowledge sharing;learning;on-demand information access;remote Ghanaian Village;rural villagers","","0","","27","","","Oct. 30 2011-Nov. 1 2011","","IEEE","IEEE Conference Publications"
"A Feasible Process For Mining Corpus From Web","C. Wang; D. Zheng; T. Zhao; J. Guo","MOE-MS Key Laboratory of Natural Language Processing and Speech Harbin Institute of Technology Harbin, China","Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology","20111117","2011","9","","","","Mining bilingual parallel sentence pair from Web data is the most effective way to get large-scale of bilingual corpus. In this paper, we put forward both the set of method and the series of process for extracting parallel sentence pair from nonspecific web date source. considering 1.1 billion page as the web data input, with a sequence of steps we get several sentences pair which has 81% recall and 85% precision, on this basis we bring up a parameter for measure quality of sentence pair. After filter sentence pair by this parameter, we get 850 thousand unique sentence pairs. On filtering by this parameter, the precision increase to 95%, meanwhile the recall only decrease by 1%.","","DVD:978-1-61284-086-4; Electronic:978-1-61284-088-8; POD:978-1-61284-087-1","10.1109/EMEIT.2011.6080758","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080758","","Accuracy;Data mining;Dictionaries;HTML;Patents;Radio access networks;Web pages","Internet;data mining;information retrieval;text analysis","Web data;bilingual corpus;bilingual parallel sentence pair mining;nonspecific Web date source;parallel sentence pair extraction","","0","","22","","","12-14 Aug. 2011","","IEEE","IEEE Conference Publications"
"Automatic Web Content Extraction for Generating Tag Clouds from Thai Web Sites","W. Thanadechteemapat; C. C. Fung","Sch. of Inf. Technol., Murdoch Univ., Murdoch, WA, Australia","2011 IEEE 8th International Conference on e-Business Engineering","20111215","2011","","","85","89","This paper proposes a novel Web content extraction approach based on heuristic rules and the XPath utility in XML. The main objective is to address the problem of Web visualization by generating tag clouds from Thai Web sites in order to provide an overview of the key words in the Web pages. This paper also proposes a detailed method to assess the Web content extraction technique on a single Web page by using the length of the extracted content. There are three main steps in the proposed technique: Web page elements and features extraction, Block detection, and Content extraction selection. The empirical results have shown this technique produces high accuracies.","","Electronic:978-0-7695-4518-9; POD:978-1-4577-1404-7","10.1109/ICEBE.2011.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104601","Tag clouds;Web Content Extraction;XPath","Accuracy;Feature extraction;Noise;Tag clouds;Visualization;Web pages","Web sites;XML;cloud computing;information retrieval","Thai Web sites;Web visualization;XML;XPath utility;automatic Web content extraction;block detection;content extraction selection;features extraction;heuristic rules;tag clouds","","3","","11","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"The Galaxy Track Browser: Transforming the genome browser from visualization tool to analysis tool","J. Goecks; K. Li; D. Clements; T. G. Team; J. Taylor","The Galaxy Project Emory University","2011 IEEE Symposium on Biological Data Visualization (BioVis).","20111205","2011","","","39","46","The proliferation of next-generation sequencing (NGS) technologies and analysis tools present new challenges to genome browsers. These challenges include supporting very large datasets, integrating analysis tools with data visualization to help reason about and improve analyses, and sharing or publishing fully interactive visualizations. The Galaxy Track Browser (GTB) is a Web-based genome browser integrated into the Galaxy platform that addresses these challenges. GTB is the first Web-based genome browser to provide a full multi-resolution data model; this model supports efficient data retrieval from very large datasets. GTB leverages the Galaxy platform to combine data visualization and data analysis; users can specify parameter values and run tools to produce new data, all within GTB. GTB also provides interactive filters that dynamically show and hide data and can be used to identify data for further investigation. GTB is available on every Galaxy server, and visualizations can be created for both standard and custom genome builds. Fully interactive GTB visualizations can be shared with colleagues and published on the Web using a simple graphical user interface.","","Electronic:978-1-4673-0004-9; POD:978-1-4673-0003-2","10.1109/BioVis.2011.6094046","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094046","Galaxy Track Browser;genome browser;genomics;visual analytics","Assembly;Bioinformatics;Browsers;Data models;Data visualization;Genomics;Servers","biology computing;data visualisation;graphical user interfaces;information retrieval","GTB;Galaxy platform;NGS;Web based genome browser;analysis tool;data retrieval;data visualization;galaxy track browser;graphical user interface;interactive visualizations;next generation sequencing","","0","","32","","","23-24 Oct. 2011","","IEEE","IEEE Conference Publications"
"The effect of user search behaviour on web information gathering tasks","A. Alhenshiri; H. Badesh","Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada","2011 Sixth International Conference on Digital Information Management","20111201","2011","","","61","66","Research has identified high level tasks in activities users perform on the web. Amongst those tasks is information gathering. This type of task is complex, exploratory, and it usually requires more user effort than other tasks. During the task of information gathering, users may follow one or more types of search behaviour. This paper investigates the effect of the user search behaviour on the task of information gathering on the web. Activities users perform on the web during information gathering and their correlation with the type of behaviour followed by the user were examined. The research draws recommendations for further studies that may concern developing tools for supporting web information gathering tasks according to the user search behaviour.","Pending","Electronic:978-1-4577-1539-6; POD:978-1-4577-1538-9","10.1109/ICDIM.2011.6093353","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093353","Information;Web;behaviour;gathering tasks;organization;orienteering;teleporting","Analysis of variance;Browsers;Correlation;Educational institutions;Interviews;Web pages;Web search","Internet;information retrieval;user interfaces","Web information gathering task;user search behaviour","","0","","14","","","26-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"Monitoring access in advanced time zones to direct prefetching and so smooth access loads","F. Rafiq; T. Moors","School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, Australia","2011 Australasian Telecommunication Networks and Applications Conference (ATNAC)","20111208","2011","","","1","6","Network use tends to follow other human activities in terms of peaking during the day and evening, and being light in the early hours of the morning. This can lead to links that are bottlenecks during peak periods having excess capacity during the night. This paper shows how trends in access by users in advanced (eastern) times zones can be used to predict future access by users in delayed (western) time zones, and so identify which objects should be pre-fetched during the (western) night so that they are ready for access when users in the delayed time zone waken. This can smooth access loads on bottleneck links, and may be particularly useful to prevent trans-Pacific submarine cables becoming a bottleneck in connecting the Australian National Broadband Network to the rest of the Internet.","Pending","Electronic:978-1-4577-1712-3; POD:978-1-4577-1711-6","10.1109/ATNAC.2011.6096643","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096643","National Broadband Network (NBN);caching;network load;prefetching;submarine cables;time zones;web trends","Australia;Educational institutions;Google;Monitoring;Prefetching;Servers;Underwater cables","Internet;broadband networks;cache storage;information retrieval;storage management;submarine cables","Australian National Broadband Network;Internet;advanced times zones;delayed time zones;direct prefetching;smooth access loads;transPacific submarine cables;user access","","0","","23","","","9-11 Nov. 2011","","IEEE","IEEE Conference Publications"
"Restorative encoding memory integrative neural device: “REMIND”","R. E. Hampson; V. Marmaralis; D. C. Shin; G. A. Gerhardt; D. Song; R. H. M. Chan; A. J. Sweatt; J. Granacki; T. W. Berger; S. A. Deadwyler","Department of Physiology of Wake Forest University Health Sciences, Winston-Salem, NC 27157","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20111201","2011","","","3338","3341","Construction and application of a neural prosthesis device that enhances existing and replaces lost memory capacity in humans is the focus of research described here in rodents. A unique approach for the analysis and application of neural population firing has been developed to decipher the pattern in which information is successfully encoded by the hippocampus where mnemonic accuracy is critical. A nonlinear dynamic multi-input multi-output (MIMO) model is utilized to extract memory relevant firing patterns in CA3 and CA1 and to predict online what the consequences of the encoded firing patterns reflect for subsequent information retrieval for successful performance of delayed-nonmatch-to-sample (DNMS) memory task in rodents. The MIMO model has been tested successfully in a number of different contexts, each of which produced improved performance by a) utilizing online predicted codes to regulate task difficulty, b) employing electrical stimulation of CA1 output areas in the same pattern as successful cell firing, c) employing electrical stimulation to recover cell firing compromised by pharmacological agents and d) transferring and improving performance in naïve animals using the same stimulation patterns that are effective in fully trained animals. The results in rodents formed the basis for extension of the MIMO model to nonhuman primates in the same type of memory task that is now being tested in the last step prior to its application in humans.","1094-687X;1094687X","Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8","10.1109/IEMBS.2011.6090905","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090905","Delayed Memory Task;Ensemble Activity;Hippocampal Prosthesis;Memory enhancement and recovery;Nonlinear Model;Stimulation patterns","Animals;Encoding;Firing;Hippocampus;MIMO;Prosthetics;Strontium","MIMO systems;bioelectric potentials;brain;cellular biophysics;drugs;information retrieval;neurophysiology;nonlinear dynamical systems;prosthetics","DNMS memory task;REMIND;cell firing;delayed-nonmatch-to-sample memory task;electrical stimulation;hippocampus;information retrieval;memory capacity;memory enhancement;memory relevant firing pattern;mnemonic accuracy;neural population firing;neural prosthesis device;nonlinear dynamic MIMO model;nonlinear dynamic multi-input multi-output model;pharmacological agent;restorative encoding memory integrative neural device","Animals;Electric Stimulation;Humans;Information Storage and Retrieval;Memory;Models, Theoretical;Rodentia","0","","52","","","Aug. 30 2011-Sept. 3 2011","","IEEE","IEEE Conference Publications"
"A Study of Category Expansion for Related Entity Finding","J. Zhang; Y. Qu","Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China","2011 Fourth International Symposium on Computational Intelligence and Design","20111117","2011","1","","185","188","Entity is an important information carrier in Web pages. Searchers often want a ranked list of relevant entities directly rather a list of documents. So the research of related entity finding (REF) is very meaningful. In this paper we investigate the most important task of REF: Entity Ranking. To address the issue of wrong entity type in entity ranking: some retrieved entities don't belong to the target entity type. We make use of category expansion to deal with the issue of wrong entity type polluting entity ranking. We use Wikipedia and Dbpedia as data sources in the experiment. We found category expansion based on original type achieves a better result in recall and precision proved by experiment.","","Electronic:978-0-7695-4500-4; POD:978-1-4577-1085-8","10.1109/ISCID.2011.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079667","entity;entity ranking;related entity finding;type filtering","Educational institutions;Electronic publishing;Encyclopedias;Filtering;Internet;Maximum likelihood estimation","Internet;Web sites;encyclopaedias;information retrieval","Dbpedia;Web pages;Wikipedia;category expansion;entity ranking;information carrier;related entity finding","","0","","11","","","28-30 Oct. 2011","","IEEE","IEEE Conference Publications"
"Research and Implementation of Data Access Framework Based on Design Patterns","X. Sun; C. Yao","Coll. of Sci. & Technol., Ningbo Univ., Ningbo, China","2011 Fourth International Symposium on Computational Intelligence and Design","20111117","2011","1","","163","166","Designing software to connect a relational database in an object-oriented business system is a tedious task. This paper proposed a data access framework based on design patterns, analyzed the Template Pattern, Proxy Pattern, and Factory patterns in the application of data access framework layer, encapsulated the underlying data access through the variable mechanism independently which kept independence of the object model And the relational database. Similarly, system increased greatly the efficiency of data access through buffer mechanisms.","","Electronic:978-0-7695-4500-4; POD:978-1-4577-1085-8","10.1109/ISCID.2011.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079661","data persistent;design patterns;object-relational mapping","Business;Data models;Production facilities;Relational databases;Rendering (computer graphics);XML","data encapsulation;information retrieval;object-oriented methods;object-oriented programming;relational databases;software architecture","data access framework;data encapsulation;factory pattern;object-oriented business system;proxy pattern;relational database;software design pattern;template pattern","","1","","7","","","28-30 Oct. 2011","","IEEE","IEEE Conference Publications"
"Japanese Painting Study Tool: A System for Creating Nihonga Portraits","M. Watanabe; N. Tosa; T. Kawahara","Grad. Sch. of Inf., Kyoto Univ., Kyoto, Japan","2011 Second International Conference on Culture and Computing","20111215","2011","","","159","160","Nihonga is a type of Japanese traditional drawing, and its delicate colors and textures are attractive. However, it is difficult for Japanese people to create nihonga, because it is necessary to prepare many kinds of expensive art supplies and to master complex classical techniques. Therefore, we developed a system that allows even people who do not have special skill of drawing to create nihonga portraits. Nihonga has contour, little shading, and simple expressions. Because we use these features of nihonga, the user can create a portrait only by combining some images as if they were making a collage, and system processing and interface are very simple. Furthermore, it also allows those who are expert painters to use paintings completed using the system as a reference before creating or repairing actual nihonga portraits. Also, this system provides a new method of using digital archives.","","Electronic:978-0-7695-4546-2; POD:978-1-4577-1593-8","10.1109/Culture-Computing.2011.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103241","Cultural computing;Digital archive;Japanese painting;Nihonga","Art;Cultural differences;Educational institutions;Global communication;Image color analysis;Painting;Paints","art;image colour analysis;image texture;information retrieval systems","Japanese painting;Japanese traditional drawing;Nihonga portraits;collage;delicate colors;digital archives;system processing;textures","","0","","6","","","20-22 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Rules and Statistical Learning Based Method for Chinese Patent Information Extraction","F. Guangpu; C. Xu; P. Zhiyong","Comput. Sch., Wuhan Univ., Wuhan, China","2011 Eighth Web Information Systems and Applications Conference","20111201","2011","","","114","118","Patent documents, as a kind of open scientific literature protected by law, the abstracts of which often highly summarize the main information. Information extraction work and analysis of the abstracts can contribute to better protection of intellectual property rights and promotion of enterprise technological innovation. This paper focus on patent abstracts and view information extraction of patent documents as a short text categorization problem, a method based on the combination of rules and statistical learning is used to annotate and extract the information of patent features, composition and usage. Experiments show that our method can not only extract the above three types of information in the patent abstracts, but also has higher accuracy when compared to the rules based method or SVM, which is an efficient and commonly used statistical learning classification algorithm.","","Electronic:978-0-7695-4555-4; POD:978-1-4577-1812-0","10.1109/WISA.2011.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093576","information extraction;patent document;rules-based method;statistic learning","Abstracts;Classification algorithms;Data mining;Feature extraction;Libraries;Patents;Support vector machines","industrial property;information retrieval;learning (artificial intelligence);patents;pattern classification;statistical analysis;support vector machines;text analysis","Chinese patent information extraction;enterprise technological innovation;intellectual property right;patent abstract;patent document;rules based method;short text categorization problem;statistical learning based method;statistical learning classification algorithm;support vector machines","","0","","11","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"Introduction to the product-entity recognition task","Fang Luo; Qizhi Qiu; QianXing Xiong","Department of Computer Science and Technology, Wuhan University of Technology, 430063, China","2011 3rd Symposium on Web Society","20111215","2011","","","122","126","The task of named entity recognition is to locate and classify words and phrases in text into predefined categories, which is an important first step for many of these larger information management goals. A framework for Product-entity recognition in Chinese was presented using Conditional Random Fields with a variety of traditional and novel features in this paper. Experimental results show that this approach can achieve an overall F-measure around 89.96, which seems to achieve the current state-of-the-art performance. However, due to the imperfect of Domain Ontology and the complication of reviews texts, the recognition for product named entity may not be better than the research of the traditional named entity recognition.","2158-6985;21586985","Electronic:978-1-4577-0211-2; POD:978-1-4577-0212-9","10.1109/SWS.2011.6101282","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6101282","","Computational modeling;Labeling;Pattern matching;Semantics","information management;information retrieval;ontologies (artificial intelligence)","Chinese;F-measure;conditional random fields;domain ontology;information management goals;named entity recognition;product-entity recognition task","","0","","16","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Pre-selection of Recruitment Candidates Using Case Based Reasoning","F. Siraj; N. Mustafa; M. F. Haris; S. R. M. Yusof; M. A. Salahuddin; M. R. Hasan","Coll. of Arts &amp; Sci., Univ. Utara Malaysia, Sintok Kedah, Malaysia","2011 Third International Conference on Computational Intelligence, Modelling & Simulation","20111114","2011","","","84","90","The cost of manually preselecting potential candidates have risen and employers are searching for methods to automate the pre-selection of candidates to fill in the vacancies at their organizations. Job portal services have proved to be the most successful and popular information services on the internet. Being more selective through selection especially from large pool of applicants will be able to decrease the possibility of hiring the poor performing individuals. The proposed system enables the employer to select the right candidate quicker and by using internet for recruitment has the advantage of faster cycle time, cheaper, and more convenient for both the employers and the job seekers. However, most job portal search engines rely on exact-match retrieval constraints. For this type of matching, a rule based approach was normally utilized. To this end, a prototype called JOBMatching<sup>©</sup> using CBR engine for matching purposes has been developed, validated and evaluated. JOBMatching<sup>©</sup> system comprises of databases of graduates as well as a match engine that incorporates Case Based Reasoning to increase its competitive advantage. CBR recommends the best candidate suitable with the job requirement using similarity measurement. Based on the feedback, JOBMatching<sup>©</sup> system facilitates the pre-selection of candidates for employment.","2166-8523;21668523","Electronic:978-0-7695-4562-2; POD:978-1-4577-1797-0","10.1109/CIMSim.2011.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076337","","Computational intelligence;Computational modeling","Internet;case-based reasoning;competitive intelligence;information retrieval;pattern matching;portals;recruitment;search engines","CBR engine;Internet;JOBMatching;case based reasoning;competitive advantage;exact-match retrieval constraint;information service;job portal service;job seeker;organization vacancy;poor-performing individual;recruitment candidate;rule based approach;similarity measurement","","0","","20","","","20-22 Sept. 2011","","IEEE","IEEE Conference Publications"
"Determining the level of knowledge in managing information among potential Bumiputera entrepreneurs in Malaysia","N. A. Kassim; S. Z. Buyong","Faculty of Information Management, UiTM Shah Alam, Selangor, Malaysia","2011 IEEE Symposium on Business, Engineering and Industrial Applications (ISBEIA)","20111201","2011","","","373","378","The paper aims to provide findings of an exploratory study that examines the level of knowledge in managing information among Malaysian Bumiputera potential entrepreneurs who desire to do business for a living. The study adopted a quantitative approach using questionnaire as the survey instrument. The sample comprised of Bumiputera potential entrepreneurs who had attended entrepreneurial development courses at Universiti Teknologi MARA. A self-developed questionnaire was distributed to 400 respondents of whom, 288 (72%) responded. Findings revealed that respondents perceive themselves to be knowledgeable in all the four components of information gathering, but are more knowledgeable in compiling information than in searching information, retrieving information, and identifying information needs. They consider gathering business information to be very important and perceive identifying information needs and searching for information relatively more important than retrieving information and compiling information. They also perceive themselves to be knowledgeable in managing business information. They are relatively more knowledgeable in using information than in storing information, disseminating information, and organising information. They consider managing business information to be very important and perceived using information to be very important followed by storing information, organising information and disseminating information. The level of knowledge in gathering and managing business information is found to be the same regardless of education level and age. The level of knowledge in gathering information and that in managing information are positively and fairly strongly correlated. The level of importance in gathering information and level of importance in managing information are positively and fairly strongly correlated. The findings of the study will be useful to supporting agencies related to entrepreneurship in Malaysia.","","Electronic:978-1-4577-1549-5; POD:978-1-4577-1548-8","10.1109/ISBEIA.2011.6088840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088840","Bumiputera entrepreneurs;Malaysia;managing business information;managing information","Correlation;Educational institutions;Government;Innovation management;Qualifications;Reliability","educational courses;information management;information retrieval;knowledge management","Bumiputera potential entrepreneurs;Malaysia;business information gathering;business information management;entrepreneurial development courses;information dissemination;information needs identification;information retrieval;information searching;self-developed questionnaire","","0","","7","","","25-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"ECG Information System based on AHA/ACC/HRS electrocardiography diagnostic statement list","H. Ding; C. Zhao; Y. Zhang; X. Lu; H. Duan","College of Biomedical Engineering and Instrument Science, Zhejiang University, The Key Laboratory of Biomedical Engineering, Ministry of Education, China, Hangzhou, Zhejiang Province, P.R. China, 310027","2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)","20111212","2011","4","","1929","1932","In 2007, the American Heart Association (AHA), the American College of Cardiology (ACC), and the Heart Rhythm Society (HRS) initiated a diagnostic statement list to create a common and easily applied terminology. There may be significant enhancements for ECG Information System with the help of the terminology, such as editing a comprehensive and normalized interpretation, classifying the ECGs automatically while interpreting, and the further using. Nowadays, there is no ECG Information System that could embody the features of the terminology. This paper designs an ECG Information System based on the terminology and presents 4 parts that improve the system: (1) Edit a normative diagnostic interpretation with the terminology. (2) Express the text and coded interpretation in XML format which is helpful for further usage. (3) Statistics of ECGs contributes to perform research with specific ECG classifications. (4) Retrieval is targeted and precise with the terminology, and the comparison is more meaningful. The system in this paper can be further enhanced, but the greatest barrier for the terminology is that it is still new for ECG manufacturers to accept it.","1948-2914;19482914","Electronic:978-1-4244-9352-4; POD:978-1-4244-9351-7","10.1109/BMEI.2011.6098705","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6098705","ECG Information System;interpretation;primary statements;terminology","Cardiology;Computers;Electrocardiography;Heart beat;Information systems;Terminology;XML","XML;classification;codes;electrocardiography;information retrieval;medical information systems;nomenclature;statistics","ACC;AHA;American College of Cardiology;American Heart Association;ECG information system;HRS;Heart Rhythm Society;XML format;classification;coded interpretation;diagnostic statement list;normative diagnostic interpretation;retrieval;statistics;terminology","","0","","6","","","15-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"A system for dynamic playlist generation driven by multimodal control signals and descriptors","L. Chiarandini; M. Zanoni; A. Sarti","Yahoo! Research Barcelona Barcelona, Catalunya (Spain)","2011 IEEE 13th International Workshop on Multimedia Signal Processing","20111201","2011","","","1","6","This work describes a general approach to multimedia playlist generation and description and an application of the approach to music information retrieval. The example of system that we implemented updates a musical playlist on the fly based on prior information (musical preferences); current descriptors of the song that is being played; and fine-grained and semantically rich descriptors (descriptors of user's gestures, of environment conditions, etc.). The system incorporates a learning system that infers the user's preferences. Subjective tests have been conducted on usability and quality of the recommendation system.","","Electronic:978-1-4577-1434-4; POD:978-1-4577-1432-0; USB:978-1-4577-1433-7","10.1109/MMSP.2011.6093850","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093850","","Brightness;Collaboration;Feature extraction;Generators;Measurement;Mood;Multimedia communication","information retrieval;learning systems;multimedia systems;vocabulary","descriptors;dynamic playlist generation;learning system;multimedia playlist generation;multimodal control signals;music information retrieval;musical playlist;musical preferences","","3","","17","","","17-19 Oct. 2011","","IEEE","IEEE Conference Publications"
"Building a large scale climate data system in support of HPC environment","F. Wang; J. Harney; G. Shipman; D. Williams; L. Cinquini","Oak Ridge National Laboratory, Tennessee 37831, USA","2011 7th International Conference on Next Generation Web Services Practices","20111201","2011","","","380","385","The Earth System Grid Federation (ESG) is a large scale, multi-institutional, interdisciplinary project that aims to provide climate scientists and impact policy makers worldwide a web-based and client-based platform to publish, disseminate, compare and analyze ever increasing climate related data. This paper describes our practical experiences on the design, development and operation of such a system. In particular, we focus on the support of the data lifecycle from a high performance computing (HPC) perspective that is critical to the end-to-end scientific discovery process. We discuss three subjects that interconnect the consumer and producer of scientific datasets: (1) the motivations, complexities and solutions of deep storage access and sharing in a tightly controlled environment; (2) the importance of scalable and flexible data publication/population; and (3) high performance indexing and search of data with geospatial properties. These perceived corner issues collectively contributed to the overall user experience and proved to be as important as any other architectural design considerations. Although the requirements and challenges are rooted and discussed from a climate science domain context, we believe the architectural problems, ideas and solutions discussed in this paper are generally useful and applicable in a larger scope.","","Electronic:978-1-4577-1127-5; POD:978-1-4577-1125-1","10.1109/NWeSP.2011.6088209","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088209","","Access control;Catalogs;Context;Data models;Distributed databases;Logic gates;Meteorology","Internet;climatology;geophysics computing;indexing;information dissemination;information retrieval;storage management","ESG;HPC environment;Web-based platform;architectural design considerations;architectural problems;client-based platform;climate related data;climate science domain context;climate scientists;data lifecycle;data population;deep storage access;earth system grid federation;end-to-end scientific discovery process;flexible data publication;geospatial property;high performance computing;high performance indexing;large scale climate data system;multiinstitutional interdisciplinary project;policy makers;scalable data publication;scientific datasets","","0","","15","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"Velocity retrieval of moving object from a single channel high resolution SAR data","J. W. Park; J. S. Won","Department of Earth System Sciences, Yonsei University, 134 Shinchon-dong, Seodaemun-gu, Seoul 120-749, Korea","2011 3rd International Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","20111128","2011","","","1","4","Two-dimensional velocity retrieval methods from a single channel high resolution SAR data through a joint time-frequency analysis and a fractional Fourier transform (FrFT) are presented. Two-dimensional velocity can be measured by estimation of the Doppler center frequency and Doppler frequency rate for the range and azimuth velocity component, respectively. The Doppler spectrum along a Doppler phase history line in the time-frequency domain was reconstructed and projected onto the frequency and time dimension. The peak of the frequency-axis projected spectrum corresponds to the Doppler center frequency, while that of the time-axis indicates an azimuth time of closest approach. The Doppler frequency rate is also measured by the slope deviation of the Doppler spectrum. Simulation using TerraSAR-X parameters indicated that the velocity errors were less than 1 m/sec or 5% for moving objects with a velocity higher than 3 m/sec. While the measurement of Doppler center frequency was reliable over the entire velocity range, errors in Doppler frequency rate became large if the velocity was lower than 3 m/sec. An experiment using TerraSAR-X and truck-mounted corner reflectors validated the measurement accuracy of the approach. Absolute and percent errors of the range velocity were 1.4 km/h and 2.8%, respectively, while the azimuth velocity measurement was comparatively accurate under an assumption of zero acceleration. To apply the method to single-look complex data, the full Doppler bandwidth must be preserved. Application of a fractional Fourier transform (FrFT) to the same data is also presented. The FrFT approach significantly improves computational efficiency and is superior to the WVD approach in estimation of Doppler frequency rate.","","Electronic:978-89-93246-17-9; POD:978-1-4577-1351-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6087091","Doppler parameter;Joint time-frequency analysis;Moving object;TerraSAR-X","Azimuth;Doppler effect;History;Spaceborne radar;Time frequency analysis;Velocity measurement","Doppler radar;Fourier transforms;frequency measurement;information retrieval;radar resolution;synthetic aperture radar;time-frequency analysis;velocity measurement","Doppler bandwidth;Doppler center frequency measurement;Doppler frequency rate;TerraSAR-X parameter;WVD approach;azimuth velocity measurement;fractional Fourier transform;frequency-axis projected spectrum;joint time-frequency analysis;measurement accuracy;moving object velocity retrieval;single channel high resolution SAR data;single-look complex data;truck-mounted corner reflector","","0","","6","","","26-30 Sept. 2011","","IEEE","IEEE Conference Publications"
"Design of multi-feature class models for Speech Recognition Security systems with under-resourced languages","N. Barroso; K. L. de Ipiña; C. Hernández; A. Ezeiza","Irunweb Enterprise Auzolan 2B - 2 Irun, 23303 Basque Country","2011 Carnahan Conference on Security Technology","20111208","2011","","","1","6","One of the goals of Speech Recognition Security (SRS) systems is to have appropriately tools to recognize speech password spoken based on elements such as words, sub-word or speakers. The main goal of the present work is to design robust ASR systems based on alternative ways to the classical evaluation rates, which often depend on the vocabulary of the task and on the language resources available. The drawback of this approach is that it is not straightforward that a system with a slightly lower WER during tests will adapt properly to new utterances, and this is much more sensible when the baseline system has a big error rate since there are many features that could be improved. This tends to be the case of under-resourced languages, since the lack of resources has a great impact in the performance of the system and not all the standard methods are suitable to any kind of language or task. The novel approach is to choose balanced multi-features of the acoustic models and the sub-word units based on rates related to entropy, mutual information and similitude. Selected models are integrated in an ontology-driven Audio Information Retrieval system that suits the requirements of under-resourced languages.","1071-6572;10716572","Electronic:978-1-4577-0903-6; POD:978-1-4577-0902-9","10.1109/CCST.2011.6095947","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095947","Security Systems;Under-resourced languages;fuzzy validation indexes;multi-feature class modelling","Acoustics;Error analysis;Hidden Markov models;Indexes;Robustness;Speech recognition;Training","audio signal processing;entropy;information retrieval;ontologies (artificial intelligence);security of data;speaker recognition","ASR systems;WER;acoustic models;baseline system;entropy;multifeature class models;mutual information;ontology driven audio information retrieval system;speech password spoken recognition;speech recognition security system;subword units;under resourced language","","0","","14","","","18-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"An integrated approach for information extraction","Y. Xia; Y. Yang; F. Ge; S. Zhang; H. Yu","Fujitsu Research & Development Center Co., LTD., 13F Tower A, Ocean International Center, No.56 Dong Si Huan Zhong Rd, Chaoyang District, Beijing, China, 100025","The 5th International Conference on New Trends in Information Science and Service Science","20111201","2011","1","","122","127","This paper proposes an integrated approach to automatic information extraction for Forums, Blogs and News web sites using wrapper. This paper presents a tree alignment and transfer learning method to generate the wrapper. The tree alignment algorithm is adopted to find the best matching structure of the input web pages. A kind of linear regression method is employed to get the weight of different tag-matching. For wrapper maintenance, this paper presents a method using a log likelihood ratio test for detecting the change points on the similarity series which gotten from the wrapper and input web pages. Experimental results show that the method achieves high accuracy and has steady performance.","","Electronic:978-89-88678-50-3; POD:978-1-4577-0665-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093405","","Accuracy;Blogs;Data mining;Estimation;Linear regression;Maintenance engineering;Web pages","Web sites;information retrieval;learning (artificial intelligence);regression analysis;trees (mathematics)","Web pages;automatic information extraction;blogs;forums;integrated approach;linear regression method;log likelihood ratio test;news Web sites;tag matching;transfer learning method;tree alignment method;wrapper maintenance","","0","","32","","","24-26 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Template-Based Tibetan Web Text Information Extraction Method","X. Chuncheng; W. Yu","Nat. Language Resource Monitoring & Res. Center, Minzu Univ. of China, Beijing, China","2011 4th International Conference on Intelligent Networks and Intelligent Systems","20111215","2011","","","218","221","In order to build a large Tibetan corpus, the researcher proposes a simple and effective method of text information extraction over Tibetan Web pages. Most web pages too much noise information unrelated to the content of the text, which makes it difficult to collect the required text information accurately and completely. After analyzing the characteristics of the seven major Tibetan Web sites, whose way of providing information is a combining use of the records in the database and the inherent dynamic web templates, the researcher presents in this article a web-based template text information extraction method. Experiments show that the method can identify and extract text information through a regular expression that filters the noise information, thus it might play a significant role in the Tibetan corpus construction with much feasibility and applicability.","","Electronic:978-0-7695-4543-1; POD:978-1-4577-1626-3","10.1109/ICINIS.2011.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104732","Text Information Extraction;Tibetan Information Processing;Tibetan language websites;Web Templates","Accuracy;Data mining;Educational institutions;Noise;Training;Web pages","Web sites;information retrieval;natural languages;text analysis","Tibetan Web pages;Web sites;dynamic Web templates;template-based Tibetan Web text information extraction","","0","","10","","","1-3 Nov. 2011","","IEEE","IEEE Conference Publications"
"Clustering of Web Search Results Based on Combination of Links and In-Snippets","N. Yang; Y. Liu; G. Yang","Sch. of Inf., Renmin Univ. of China, Beijing, China","2011 Eighth Web Information Systems and Applications Conference","20111201","2011","","","108","113","Search engine is a common tool to retrieve the information in the Web. But the current status of returned results is still far from satisfaction. Users have to be confronted with searching for a long result list to get the information really wanted. Many works focused on the post processing search results to facilitate users to examine the results. One of the common ways of post processing search result is clustering. Term-based clustering appears as first way to cluster the results. But this method is suffering from the poor quality while the processed pages have little text. Link-based clustering can conquer this problem. But the quality of clusters heavily depends on the number of in-links and out-links in common. In this paper, we propose that the short text attached to in-link is valuable information and it is helpful to reach high clustering quality. To distinguish them with general snippet, we name it as in-snippet. Based on the in-snippet, we propose a new clustering method that combines the links and the in-snippets together. In our method, similarity between pages consists of two parts : link similarity and term similarity. We designed related algorithm to implement clustering. In order to prevent bias from human judgments, the experiment datasets are collected from Open Directory Project(DMOZ). Due to DMOZ is human-edited directory, the datasets from DMOZ has higher quality and larger scale. We use entropy and f-measure to evaluate the quality of the final clusters. By being compared with the link-based and the pure term-based algorithms, our method outperforms others in clustering quality.","","Electronic:978-0-7695-4555-4; POD:978-1-4577-1812-0","10.1109/WISA.2011.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093575","Clustering;Link analysis;Search engine result","Algorithm design and analysis;Clustering algorithms;Educational institutions;Entropy;Search engines;Vectors;Web pages","Internet;information retrieval;pattern clustering;search engines","DMOZ;Web search;entropy;f-measure;in-snippet;link similarity;link-based clustering;search engine;term similarity;term-based clustering","","2","","15","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"Domain Ontology Usage Analysis Framework","J. Ashraf; M. Hadzic","Digital Ecosyst. & Bus. Intell. (DEBII), Curtin Univ., Perth, WA, Australia","2011 Seventh International Conference on Semantics, Knowledge and Grids","20111201","2011","","","75","82","The Semantic Web (also known as Web of Data) is growing fast and becoming a decentralized knowledge platform for publishing and sharing information. The web ontologies promote the establishment of a shared understanding between data providers and data consumers, allowing for automated information processing and effective and efficient information retrieval. The majority of existing research efforts is focused around ontology engineering, ontology evaluation and ontology evolution. This work goes a step further and evaluates the ontology usage. In this paper, we present an Ontology USage Analysis Framework (OUSAF) and a set of metrics used to measure the ontology usage. The implementation of the proposed framework is illustrated using the example of Good Relations ontology (GRO). GRO has been well adopted by the semantic ecommerce community, and the OUSAF approach has been used to analyse GRO usage in the dataset comprised of RDF data collected from the web.","","Electronic:978-0-7695-4515-8; POD:978-1-4577-1323-1","10.1109/SKG.2011.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088094","GoodRelations;Ontology Usage;Semantic web","Knowledge based systems;Measurement;Ontologies;Resource description framework;Semantics;Vocabulary","information retrieval;ontologies (artificial intelligence);semantic Web","Web ontologies;World Wide Web;automated information processing;data consumer;decentralized knowledge platform;domain ontology usage analysis framework;good relations ontology;information retrieval;ontology engineering;ontology evaluation;ontology evolution;semantic Web;semantic ecommerce community","","0","","23","","","24-26 Oct. 2011","","IEEE","IEEE Conference Publications"
"A comparative study on extraction method of non-geometry information in engineering drawing","M. F. M. Amran; R. Sulaiman; S. Kahar; S. Marjudi; K. A. Abdullah; Z. Adnan","Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Selangor, Malaysia","2011 IEEE Symposium on Business, Engineering and Industrial Applications (ISBEIA)","20111201","2011","","","5","9","In manufacturing industry, engineering drawing is used to facilitate the process of designing a product. In real situation, the engineering drawing is usually available in a large quantity and not in order. This is due to many drawings are produced in the process of designing a product. Effective access of engineering drawings from various numbers of engineering drawings is very challenging. Engineer can reduce the cost of designing a new product by retrieving the original drawings of components from the database in the CAD software. This paper presents a review on extraction of non-geometry information in engineering drawing. It explores the past studies on the existing extraction methods discovered by numerous researchers in the field of engineering drawing. In this research, the information that will be extracted is non-geometry information such as length, thickness, radius, diameter and labels. The comparative example of extraction in engineering drawing of previous research has been carried out.","","Electronic:978-1-4577-1549-5; POD:978-1-4577-1548-8","10.1109/ISBEIA.2011.6088887","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088887","computer aided design;engineering drawing;information extraction;non-geometry information","Data mining;Databases;Design automation;Engineering drawings;Feature extraction;Geometry;Software","CAD;information retrieval;manufacturing industries;product design;production engineering computing;technical drawing","CAD software;engineering drawing;manufacturing industry;nongeometry information extraction method;product design","","0","","17","","","25-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"Online ngram-enhanced topic model for academic retrieval","H. Wang; B. Lang","State Key Laboratory of Software Development Environment, Beihang University, Beijing, 100191,P. R. China","2011 Sixth International Conference on Digital Information Management","20111201","2011","","","137","142","Applying topic model to text mining has achieved a great success. However, state-of-art topic modeling methods still have potential to improve in academic retrieval field. In this paper, we propose an online unified topic model, which is ngram-enhanced. Our model discovers topics with unigrams as well as topical bigrams and is updated by an online inference algorithm with the new incoming data streams. On this basis, we combine our model into the query likelihood model and develop an integrated academic searching system. Experiment results on ACM collection show that our proposed methods outperform the existing approaches on document modeling and searching accuracy. Especially, we prove the efficiency of our system on academic retrieval problem.","Pending","Electronic:978-1-4577-1539-6; POD:978-1-4577-1538-9","10.1109/ICDIM.2011.6093316","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093316","","Computational modeling;Data models;Inference algorithms;Mathematical model;Neodymium;Object oriented modeling;Predictive models","data mining;inference mechanisms;information retrieval;maximum likelihood estimation;text analysis","academic retrieval;document modeling;document search;integrated academic searching system;ngram enhanced topic model;online inference algorithm;query likelihood model;text mining;topical bigram;unigram","","0","","17","","","26-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"Base state amendments spatio-temporal data model with dynamic selection of base state","X. Song; Y. Wang; G. Wu; S. Wang","School of Information & Control engineering, Shenyang Jianzhu University, China","2011 4th International Congress on Image and Signal Processing","20111212","2011","5","","2349","2353","The traditional Base State with Amendments Spatio-temporal data model often select the operation of base state on sequential. The method is relatively easy to implement, but there are obvious deficiencies. This paper introduces the concepts of operation of the base state and Operand, and proposes a base state amendments spatio-temporal data model with dynamic selection of base state. The retrieval time choose the operation of base state which having smaller operand in the adjacent two base states. The problem of selecting a distant base state causing a large number of redundant operations because of using sequential retrieval method can be solved by this approach, thereby enhancing the retrieval efficiency.","","Electronic:978-1-4244-9306-7; POD:978-1-4244-9304-3","10.1109/CISP.2011.6100782","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100782","base state with amendment;operation base state;spatial-temporal data model;the operand of time","Complexity theory;Data models;Equations;Indexes;Mathematical model;Timing","geographic information systems;information retrieval","base state amendments spatio-temporal data model;dynamic base state selection;redundant operations;sequential retrieval method","","0","","10","","","15-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"Heart sound acquisition based on PDA and bluetooth","T. Xian-ting; Z. Zhi-dong","School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China","2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)","20111212","2011","2","","773","776","This article introduces a heart sound data acquisition system based on PDA and Bluetooth. The system mainly includes PDA (Personal Digital Assistant) and the data acquisition terminal equipment. The data acquisition terminal equipment is controlled by dsPIC chip and has the following functions: heart sound acquisition, heart sound transmission and heart sound auscultation. The data communication between PDA and data acquisition terminal equipment is achieved via Bluetooth virtual serial port. The PDA can receive heart sound data, display heart sound waveform real-time, preserve heart sound signal and playback heart sound signal. The software and hardware are also given in detail.","1948-2914;19482914","Electronic:978-1-4244-9352-4; POD:978-1-4244-9351-7","10.1109/BMEI.2011.6098495","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6098495","Auscultation;LabVIEW;PDA;bluetooth;heart sound","Bluetooth;Data acquisition;Heart;Instruments;Low pass filters;Personal digital assistants","Bluetooth;biomedical communication;cardiology;data acquisition;data communication;information retrieval systems;information storage;medical administrative data processing;medical signal detection;notebook computers","Bluetooth;PDA;data acquisition terminal equipment;data communication;dsPIC chip;heart sound acquisition;heart sound auscultation;heart sound data;heart sound signal preservation;heart sound transmission;heart sound waveform display;personal digital assistant;signal playback;virtual serial port","","0","","6","","","15-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"Expanding identifiers to normalize source code vocabulary","D. Lawrie; D. Binkley","Loyola University Maryland, Baltimore, 21210-2699, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","20111117","2011","","","113","122","Maintaining modern software requires significant tool support. Effective tools exploit a variety of information and techniques to aid a software maintainer. One area of recent interest in tool development exploits the natural language information found in source code. Such Information Retrieval (IR) based tools compliment traditional static analysis tools and have tackled problems, such as feature location, that otherwise require considerable human effort. To reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. Unfortunately, there is a significant proportion of invented vocabulary in source code. Vocabulary normalization aligns the vocabulary found in the source code with that found in other software artifacts. Most existing work related to normalization has focused on splitting an identifier into its constituent parts. The next step is to expand each part into a (dictionary) word that matches the vocabulary used in other software artifacts. Building on a successful approach to splitting identifiers, an implementation of an expansion algorithm is presented. Experiments on two systems find that up to 66% of identifiers are correctly expanded, which is within about 20% of the current system's best-case performance. Not only is this performance comparable to previous techniques, but the result is achieved in the absence of special purpose rules and not limited to restricted syntactic contexts. Results from these experiments also show the impact that varying levels of documentation (including both internal documentation such as the requirements and design, and external, or user-level, documentation) have on the algorithm's performance.","1063-6773;10636773","Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9","10.1109/ICSM.2011.6080778","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080778","natural language processing;program comprehension;source code analysis tools","Context;Dictionaries;Documentation;Natural languages;Software;Vocabulary","information retrieval;natural language processing;program diagnostics;software maintenance;source coding;vocabulary","IR-based techniques;feature location;identifier expansion;information retrieval based tools;natural language information;software artifacts;software maintenance;source code vocabulary normalization;splitting identifiers;static analysis tools","","16","","31","","","25-30 Sept. 2011","","IEEE","IEEE Conference Publications"
"New EPICS Channel Archiver Based on MDSplus Data System","G. Manduchi; A. Luchetta; C. Taliercio; A. Soppelsa; A. Barbalace","Consorzio RFX, Associazione Euratom-ENEA sulla fusione, Padova, Italy","IEEE Transactions on Nuclear Science","20111212","2011","58","6","3158","3161","The EPICS Channel Archiver is used to store data ex ported by EPICS I/O Controllers. The Channel Archiver acts as a Channel Access Client and stores recorded data, acquired via periodic scan or monitored, into indexed binary files. MDSplus is a data management system used in several Nuclear Fusion experiments to handle experimental and configuration data. A data access Application Programming Interface for local and remote data access is available for several languages, namely C, C++, Fortran, Java, Python, MATLAB and IDL, and a set of visualization tools is available for data browsing and display. The paper presents a new implementation of the EPICS Channel Archiver which uses MDSplus for data storage. In this way, it is possible to take advantage of the availability of the local and remote data access layers of MDSplus, widely used in the fusion community to handle large sets of data. A performance comparison between the original implementation and the new one is provided. In particular, the storage space requirements and the data access speed are considered.","0018-9499;00189499","","10.1109/TNS.2011.2167349","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6044741","Data access;data acquisition;data storage;system monitoring","Data acquisition;Data storage systems;Data visualization;Protocols","data acquisition;information retrieval;nuclear engineering computing","EPICS IO controllers;EPICS channel archiver;MDSplus data system;channel access client;data access speed;data management system;data storage;indexed binary files;nuclear fusion experiments;remote data access layers","","4","","10","","20111013","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"LTIMEX: Representing the local semantics of temporal expressions","P. Mazur; R. Dale","Institute of Applied Informatics, Wroc&#x0142;aw University of Technology, Wyb. Wyspia&#x0144;skiego 27, 50-370 Wroc&#x0142;aw, Poland","2011 Federated Conference on Computer Science and Information Systems (FedCSIS)","20111114","2011","","","201","208","Semantic information retrieval requires that we have a means of capturing the semantics of documents; and a potentially useful feature of the semantics of many documents is the temporal information they contain. In particular, the temporal expressions contained in documents provide important information about the time course of the events those documents describe. Unfortunately, temporal expressions are often context-dependent, requiring the application of information about the context in order to work out their true values. We describe a representational formalism for temporal information that captures what we call the local semantics of such expressions; this permits a modularity whereby the context-independent contribution to meaning can be computed independently of the global context of interpretation, which may not be immediately or easily available. Our representation, LTIMEX, is intended as an extension to widely-used TIMEX2 and TimeML representations.","","Electronic:978-83-60810-39-2; POD:978-1-4577-0041-5; USB:978-83-60810-35-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078263","","Calendars;Context;Electronic mail;Encoding;Particle separators;Semantics;Tagging","document handling;information retrieval","LTIMEX;TIMEX2;TimeML representations;context-independent contribution;document semantics;local semantics representation;semantic information retrieval;temporal expressions;temporal information representational formalism","","0","","15","","","18-21 Sept. 2011","","IEEE","IEEE Conference Publications"
"Expert finding and query answering for Collaborative Inter-Organizational system by using Rule Responder","T. Rui; S. Fong; S. Sarasvady","Department of Computer and Information Science, University of Macau, Macau SAR","2011 Sixth International Conference on Digital Information Management","20111201","2011","","","162","167","Collaborative inter-organizational system (C-IOS) is defined as information technology-based systems that engage multiple business partners for achieving some common value-added goals. In the past, many papers from the literature addressed a large number of techniques on collaborative agents. The techniques range from basic information exchange to sophisticated negotiation. Specifically, for C-IOS's two important tasks namely Experts Finding (EF) and Query Answering (QA) are required in collaboration. These two specific tasks facilitate supply-chain mediation and may be subsequent procurement negotiation. EF concerns about finding or match-making the right personnel in an organization as a committee member for fulfilling a part of the job. QA screens initially whether the resources and commitments are potentially available. The two tasks supposedly would have executed prior to any further collaboration, and the communication is cross organizations. This paper contributes a design of C-IOS that supports EF and QA for inter-organizational collaboration. The underlying technical framework is by Rule Responder which is a powerful tool for creating virtual organizations as multi-agent systems that support collaborative teams on the Semantic Web. A use case of hosting an academic conference among different organizations is illustrated with our proposed concepts in this paper.","Pending","Electronic:978-1-4577-1539-6; POD:978-1-4577-1538-9","10.1109/ICDIM.2011.6093344","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093344","Collaborative Inter-organizations;Rule Responders","Collaboration;Engines;Organizations;Random access memory;Resource description framework;Standards organizations","business data processing;groupware;multi-agent systems;personnel;procurement;question answering (information retrieval);semantic Web;supply chains","collaborative agents;collaborative inter-organizational system;collaborative teams;common value-added goals;experts finding;information technology-based system;multiagent system;personnel match-making;procurement negotiation;query answering;rule responder;semantic Web;supply-chain mediation;virtual organization","","0","","11","","","26-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"TECS: A Web Text Extraction Tool Based on Semantic Similarity Calculation","W. Yu; L. Qiu","Sch. of Inf. & Eng., Minzu Univ. of China, Beijing, China","2011 4th International Conference on Intelligent Networks and Intelligent Systems","20111215","2011","","","225","228","The knowledge-based information extraction approach has been proved effectively in the recognition and classification of Web texts. However, manual creation of knowledge base is time consuming and error prone, even for a small application domain. For improving the automatic construction ability of knowledge base in the process of IE, this paper presents a web text extraction tool based on case similarity calculation (named TECS). TECS facilitates the process of constructing case knowledge base and strengthens the knowledge representation ability of single case by calculating case similarity and merging the cases which have similar semantic relationships.","","Electronic:978-0-7695-4543-1; POD:978-1-4577-1626-3","10.1109/ICINIS.2011.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104734","sementic similarity calculation;text extraction;web text mining","Data mining;Educational institutions;Knowledge based systems;Knowledge representation;Pattern matching;Resource description framework;Semantics","Internet;feature extraction;information retrieval;knowledge representation;pattern classification;text analysis","TECS;Web text extraction tool;case knowledge base;case similarity calculation;knowledge representation;knowledge-based information extraction approach;semantic relationship;semantic similarity calculation;text extraction tool based on case similarity","","0","","13","","","1-3 Nov. 2011","","IEEE","IEEE Conference Publications"
"How to Find Relevant Data for Effort Estimation?","E. Kocaguneli; T. Menzies","Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA","2011 International Symposium on Empirical Software Engineering and Measurement","20111201","2011","","","255","264","Background: Building effort estimators requires the training data. How can we find that data? It is tempting to cross the boundaries of development type, location, language, application and hardware to use existing datasets of other organizations. However, prior results caution that using such cross data may not be useful. Aim: We test two conjectures: (1) instance selection can automatically prune irrelevant instances and (2) retrieval from the remaining examples is useful for effort estimation, regardless of their source. Method: We selected 8 cross-within divisions (21 pairs of within-cross subsets) out of 19 datasets and evaluated these divisions under different analogy-based estimation (ABE) methods. Results: Between the within & cross experiments, there were few statistically significant differences in (i) the performance of effort estimators, or (ii) the amount of instances retrieved for estimation. Conclusion: For the purposes of effort estimation, there is little practical difference between cross and within data. After applying instance selection, the remaining examples (be they from within or from cross source divisions) can be used for effort estimation.","1949-3770;19493770","Electronic:978-0-7695-4604-9; POD:978-1-4577-2203-5","10.1109/ESEM.2011.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092574","cross resource;k-NN;software cost estimation;within resource","Artificial neural networks;Buildings;Data models;Estimation;Organizations;Training;Training data","data handling;estimation theory;information retrieval","ABE;analogy based estimation;cross source divisions;effort estimation;information retrieval;instance selection;relevant data","","12","","31","","","22-23 Sept. 2011","","IEEE","IEEE Conference Publications"
"Study on Data Management of Fundamental Model in Control Center for Smart Grid Operation","J. Liu; X. Li; D. Liu; H. Liu; P. Mao","East China Electric Power Test & Research Institute, Shanghai, China","IEEE Transactions on Smart Grid","20111121","2011","2","4","573","579","The control center in smart grids requires the collaboration among different specialized groups to ensure the safety, reliability, greenness and efficiency of power system. However, each specialized group is interested in different aspects of the network model. In order to improve the teamwork and timeliness, the fundamental models should be managed in a unified way. This paper summarizes the requirements for data management of fundamental models, presents a concept of three-mode models, and designs the architecture of generic model access services. With the generic model access services, the differences of real-world model databases will be shielded, and the future virtual model can be managed as versions. The upper applications can obtain the fundamental model information through the data broker component and business model data through the business data organizing component.","1949-3053;19493053","","10.1109/TSG.2011.2160571","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981411","Data management;generic model access services;intelligent control center;three-mode models","Analytical models;Data models;Load modeling;Real time systems;Relational databases","database management systems;information retrieval;power system control;power system reliability;smart power grids","business model data;control center;data broker component;data management;generic model access service;power system efficiency;power system reliability;power system safety;real-world model database;smart grid operation;three-mode model;virtual model","","15","","12","","20110811","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Interactive visualization and search system for speech corpora","S. Itahashi; T. Kajiyama; K. Yamakawa; Y. Ishimoto; T. Matsui","National Institute of Informatics, Tokyo, Japan","2011 International Conference on Speech Database and Assessments (Oriental COCOSDA)","20111128","2011","","","157","161","We have already reported a corpus similarity visualization method based on the corpus attribute using multidimensional scaling that makes it easy for users to utilize various speech corpora. In this paper, we present a revised visualization method that is based on a ring structure like a planisphere. By using only a mouse, a user can choose appropriate search keys for each of the multiple attributes and can easily filter information by adjusting the keys. Retrieved results are displayed inside the rings, and the user can filter and browse them in real time. This will facilitate efficient searching of the specific corpus that fits user's needs.","","Electronic:978-1-4577-0931-9; POD:978-1-4577-0930-2","10.1109/ICSDA.2011.6085999","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085999","corpus;information;retrieval;specification","Data mining;Information filters;Intelligent systems;Microphones;Speech;Structural rings","data visualisation;information retrieval;interactive systems","corpus attribute;corpus similarity visualization method;interactive visualization;multidimensional scaling;ring structure;search system;speech corpora","","1","","11","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"On the Voice-Activated Question Answering","P. Rosso; L. F. Hurtado; E. Segarra; E. Sanchis","Departament de Sistemes Inform&#x00E0;tics i Computaci&#x00F3;, Universitat Polit&#x00E8;cnica de Val&#x00E8;ncia, Spain","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","20111215","2012","42","1","75","85","Question answering (QA) is probably one of the most challenging tasks in the field of natural language processing. It requires search engines that are capable of extracting concise, precise fragments of text that contain an answer to a question posed by the user. The incorporation of voice interfaces to the QA systems adds a more natural and very appealing perspective for these systems. This paper provides a comprehensive description of current state-of-the-art voice-activated QA systems. Finally, the scenarios that will emerge from the introduction of speech recognition in QA will be discussed.","1094-6977;10946977","","10.1109/TSMCC.2010.2089620","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672612","Automatic speech recognition (ASR);information extraction;language model;named entity recognition;question answering (QA);speech input","Acoustics;Data mining;Databases;Hidden Markov models;Speech;Speech recognition;Vocabulary","natural language processing;question answering (information retrieval)","QA;comprehensive description;natural language processing;voice activated question answering;voice interfaces","","0","","66","","20101220","Jan. 2012","","IEEE","IEEE Journals & Magazines"
"A simple architecture for the fine-grained documentation of endangered languages: The LACITO multimedia archive","B. Michailovsky; A. Michaud; S. Guillaume","LACITO-CNRS, France","2011 International Conference on Speech Database and Assessments (Oriental COCOSDA)","20111128","2011","","","14","23","The LACITO multimedia archive [1] provides free access to documents of connected, spontaneous speech, mostly in “rare” or endangered languages, recorded in their cultural context and transcribed in consultation with native speakers. Its goal is to contribute to the documentation and study of a precious human heritage: the world's languages. It has a special strength in languages of Asia and the Pacific. The LACITO archive was built with little personnel and less funding. It has been devised, developed and maintained over two decades by two researchers assisted by one engineer. Its simple architecture is based on current standards: Unicode character coding and XML markup; and Dublin Core/Open Language Archives Community recommendations for metadata. The data can be consulted online with any standard browser. The technical simplicity of the tools developed at LACITO makes them suitable for the creation of similar databases at other institutions. (For instance, tools from this archive were successfully adapted in the creation of the Formosan Languages archive [2].)","","Electronic:978-1-4577-0931-9; POD:978-1-4577-0930-2","10.1109/ICSDA.2011.6085973","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085973","Multimedia corpora;endangered languages;interlinear glossing;language documentation;long-term preservation;online databases;spontaneous speech","Communities;Databases;Documentation;Pragmatics;Speech;Standards;XML","document handling;information retrieval;information retrieval systems;linguistics;multimedia databases;speech coding","Asia and the Pacific;Dublin Core recommendation;Formosan languages archive;LACITO multimedia archive;Open Language Archives Community recommendation;XML markup;cultural context;document free access;endangered languages;fine-grained documentation;human heritage;unicode character coding","","0","","26","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Assessing the data processing innovations for inventory management with patent analysis","C. w. Shen; C. C. Cheng","Department of Business Administration, National Central University, No.300, Jhongda Rd., Jhongli City, Taoyuan County 32001, Taiwan","The 5th International Conference on New Trends in Information Science and Service Science","20111201","2011","2","","324","327","To understand the technological trend and advances of data processing technologies for inventory management, this study conducted patent trend analysis, IPC (International Patent Classification) analysis, and patent citation analysis upon the patent data retrieved from the United States Patent and Trademark Office. Our patent trend analysis not only accessed the development of data processing technologies but also evaluated the novelty performances of different countries. Meanwhile, findings of our IPC analysis can help enterprises to understand the major categories of data processing innovations for inventory management. Moreover, we also identified the core data processing technologies through the computation of citation strength.","","Electronic:978-89-88678-50-3; POD:978-1-4577-0665-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093449","","Data processing;Industries;Inventory management;Patents;Supply chains;Technological innovation","citation analysis;information retrieval;innovation management;inventory management;patents","IPC analysis;United States patent;data processing innovation;international patent classification analysis;inventory management;patent citation analysis;patent data retrieval;trademark office","","1","","12","","","24-26 Oct. 2011","","IEEE","IEEE Conference Publications"
"Improved SIMD Architecture for High Performance Video Processors","W. Y. Lo; D. P. K. Lun; W. C. Siu; W. Wang; J. Song","Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","20111205","2011","21","12","1769","1783","Single instruction multiple data (SIMD) execution is in no doubt an efficient way to exploit the data level parallelism in image and video applications. However, SIMD execution bottlenecks must be tackled in order to achieve high execution efficiency. We first analyze in this paper the implementation of two major kernel functions of H.264/AVC namely, SATD and subpel interpolation, in conventional SIMD architectures to identify the bottlenecks in traditional approaches. Based on the analysis results, we propose a new SIMD architecture with two novel features: 1) parallel memory structure with variable block size and word length support, and 2) configurable SIMD structure. The proposed parallel memory structure allows great flexibility for programmers to perform data access of different block sizes and different word lengths. The configurable SIMD structure allows almost “random” register file access and slightly different operations in ALUs inside SIMD. The new features greatly benefit the realization of H.264/AVC kernel functions. For instance, the fractional motion estimation, particularly the half to quarter pixel interpolation, can now be executed with minimal or no additional memory access. When comparing with the conventional SIMD systems, the proposed SIMD architecture can have a further speedup of 2.1X to 4.6X when implementing H.264/AVC kernel functions. Based on Amdahl's law, the overall speedup of H.264/AVC encoding application can be projected to be 2.46X. We expect significant improvement can also be achieved when applying the proposed architecture to other image and video processing applications.","1051-8215;10518215","","10.1109/TCSVT.2011.2130250","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734815","Configurable SIMD;SIMD bottlenecks;parallel memory structure;video codec processor","Memory architecture;Parallel processing;Video codecs;Video signal processing","information retrieval;memory architecture;parallel architectures;reconfigurable architectures;video codecs","ALU;Amdahl law;H.264-AVC encoding application;H.264-AVC kernel function;SATD;configurable SIMD structure;data access;data level parallelism;fractional motion estimation;high execution efficiency;high performance video processor;image application;improved SIMD architecture;parallel memory structure;quarter pixel interpolation;random register file access;single instruction multiple data execution;subpel interpolation;video application;word length support","","8","","28","","20110317","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"SE<sup>2</sup> model to support software evolution","H. Kagdi; M. Gethers; D. Poshyvanyk","Dept. of Electrical Engineering and Computer Science, Wichita State University, KS 67260, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","20111117","2011","","","512","515","The paper proposes an integrated approach, namely SE<sup>2</sup>, to support three core software maintenance and evolution tasks: feature location, software change impact analysis, and expert developer recommendation. The approach is centered on the combinations of the conceptual and evolutionary relationships latent in structured and unstructured software artifacts. Information Retrieval (IR) and Mining Software Repositories (MSR) based techniques are used for analyzing and deriving these relationships. All the three tasks are supported under a single, common framework by providing systematic combinations of MSR and IR analyses on single and multiple versions of a software system. This combining ability of SE<sup>2</sup> sets it apart from previously reported relevant solutions in the literature. The outlined empirical assessment is aimed at identifying the exclusive and synergistic improvements offered by such combinations for each of the addressed tasks. Preliminary evaluation on a number of open source systems suggests that such combinations do offer improvements over individual approaches.","1063-6773;10636773","Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9","10.1109/ICSM.2011.6080820","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080820","","Computational modeling;Couplings;Data mining;History;Software maintenance;Software systems","data mining;evolutionary computation;information retrieval;public domain software;recommender systems;software maintenance","IR analyses;MSR;SE<sup>2</sup> model;expert developer recommendation;feature location;information retrieval;mining software repositories;open source systems;preliminary evaluation;software change impact analysis;software evolution;software maintenance;structured software artifacts;unstructured software artifacts","","0","","19","","","25-30 Sept. 2011","","IEEE","IEEE Conference Publications"
"Random Walk Search on Concatenated Hop-Limited Trees Embedded into Unstructured P2Ps","S. Shioda; P. Hieungmany","Grad. Sch. of Eng., Chiba Univ., Chiba, Japan","2011 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20111215","2011","","","43","50","We propose a random-walk-based file search for unstructured P2P networks. In the proposal, each node keeps two pieces of information, one is the hop-limited routing tables, storing the route information to nodes within a small number (say n) hops from each node. The hop-limited routing table is substantially equivalent to the hop-limited shortest path tree, which is made of nodes within limited hop distance from the root. The other is the file list, which is an index over files kept by nodes within n hops from each node. To find a file, a node first checks its file list. If the requested file is found in the list, the node sends a file request message to the file owner, otherwise, it sends a file-search message to a randomly-selected leaf node on the hop-limited shortest path tree rooted at itself. Numerical examples show that our proposal has much smaller search latency and lower file-search failure ratio than the primitive random-walk-based search, while it wastes much less network bandwidth than network-broadcast-based searches.","","Electronic:978-1-4244-9640-2; POD:978-1-4577-1448-1","10.1109/3PGCIC.2011.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103137","P2P;file search;random walk;unstructured","Barium;Indexes;Peer to peer computing;Proposals;Routing;Unicast;Vectors","information retrieval;message passing;peer-to-peer computing;random processes;storage management;tree data structures","concatenated hop-limited trees;file list;file request message;file-search failure ratio;file-search message;hop-limited routing tables;hop-limited shortest path tree;random walk-based file search;randomly-selected leaf node;route information storage;search latency;unstructured P2P networks","","0","","22","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"A system for scalable visualization of geographic archival records","J. R. Heard; R. J. Marciano","Renaissance Computing Institute (RENCI) and Sustainable Archives & Leveraging Technologies lab (SALT), University of North Carolina at Chapel Hill, USA","2011 IEEE Symposium on Large Data Analysis and Visualization","20111201","2011","","","121","122","We present a system that visualizes large collections of archival geographic records. This system is comprised of a data grid containing a 60TB test collection gleaned from the US National Archives, and three web-applications: an indexer and two web and mobile-device based visualizations focusing on collection understanding in a geographic context.","","Electronic:978-1-4673-0155-8; POD:978-1-4673-0156-5","10.1109/LDAV.2011.6092329","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092329","Visualization;archival records;large collections","Computer architecture;Context;Data visualization;Distributed databases;Indexing;Vectors","Internet;data visualisation;geographic information systems;grid computing;indexing;information retrieval systems;records management","US National Archives;Web-applications;Web-device based visualizations;archival geographic records;data grid;geographic archival records;geographic context;indexer;mobile-device based visualizations;scalable visualization","","4","","11","","","23-24 Oct. 2011","","IEEE","IEEE Conference Publications"
"Symbolic search-based testing","A. Baars; M. Harman; Y. Hassoun; K. Lakhotia; P. McMinn; P. Tonella; T. Vos","Universidad Polit&#x00E9;cnica de Valencia, Spain","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","20111212","2011","","","53","62","We present an algorithm for constructing fitness functions that improve the efficiency of search-based testing when trying to generate branch adequate test data. The algorithm combines symbolic information with dynamic analysis and has two key advantages: It does not require any change in the underlying test data generation technique and it avoids many problems traditionally associated with symbolic execution, in particular the presence of loops. We have evaluated the algorithm on industrial closed source and open source systems using both local and global search-based testing techniques, demonstrating that both are statistically significantly more efficient using our approach. The test for significance was done using a one-sided, paired Wilcoxon signed rank test. On average, the local search requires 23.41% and the global search 7.78% fewer fitness evaluations when using a symbolic execution based fitness function generated by the algorithm.","1938-4300;19384300","Electronic:978-1-4577-1639-3; POD:978-1-4577-1638-6","10.1109/ASE.2011.6100119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100119","Fitness Functions;Search-Based Testing;Symbolic Execution","Algorithm design and analysis;Approximation algorithms;Approximation methods;Educational institutions;Software algorithms;Software testing","automatic test software;information retrieval;program diagnostics;program testing","branch adequate test data generation;dynamic analysis;fitness functions;global search-based testing technique;industrial closed source system;local search-based testing technique;one-sided paired Wilcoxon signed rank test;open source systems;symbolic execution;symbolic information;symbolic search-based testing","","12","2","30","","","6-10 Nov. 2011","","IEEE","IEEE Conference Publications"
"A Comprehensive Prediction Method of Visit Priority for Focused Crawler","X. Li; M. Xing; J. Zhang","Coll. of Comput. Sci., Chongqing Univ., Chongqing, China","2011 2nd International Symposium on Intelligence Information Processing and Trusted Computing","20111215","2011","","","27","30","The purpose of a focused crawler is to crawl more topical portions of the Internet precisely. How to predict the visit priorities of candidate URLs whose corresponding pages have yet to be fetched is the determining factor in the focused crawler's ability of getting more relevant pages. This paper introduces a comprehensive prediction method to address this problem. In this method, a page partition algorithm that partitions the page into smaller blocks and interclass rules that statistically capture linkage relationships among the topic classes are adopted to help the focused crawler cross tunnel and to enlarge the focused crawler's coverage, URL's address, anchor text and block content are used to predict visit priority more precisely. Experiments are carried out on the target topic of tennis and the results show that crawler based on this method is more effective than a rule-based crawler on harvest ratio.","","Electronic:978-0-7695-4498-4; POD:978-1-4577-1130-5","10.1109/IPTC.2011.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103528","focused crawler;interclass rules;page partition;visit priority","Crawlers;Educational institutions;Partitioning algorithms;Search engines;Support vector machine classification;Training;Vectors","Internet;information retrieval;search engines","Internet;URL address;anchor text;block content;candidate URL;comprehensive prediction method;focused crawler coverage;harvest ratio;interclass rules;page partition algorithm;rule-based crawler;tennis;visit priority","","0","","16","","","22-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"Distributed Placement of Replicas in Hierarchical Data Grids with User and System QoS Constraints","M. Shorfuzzaman; P. Graham; R. Eskicioglu","Dept. of Comput. Sci., Univ. of Manitoba, Winnipeg, MB, Canada","2011 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20111215","2011","","","177","186","Data grids support distributed data-intensive applications that need to access massive datasets stored around the world. Ensuring efficient access to such datasets is hindered by the high latencies of wide-area networks. To speed up access, files can be replicated so a user can access a nearby replica. Much of the work on the replica placement problem in data grids has focused on average system performance and ignored quality assurance issues. In the existing work that considers QoS, a simplified replication model is often assumed, therefore, resulting solutions may not be applicable to real systems. In this paper, we introduce a more realistic model for replica placement in hierarchical Data Grids which determines the positions of a minimum number of replicas expected to satisfy certain quality requirements both from user and system perspectives. Our placement algorithm is based on a highly distributed and decentralized technique that exploits the data access history for popular data files and computes replica locations by minimizing overall replication cost (read and update) while maximizing QoS satisfaction for a given traffic pattern. The problem is formulated using dynamic programming. We assess our algorithm using OptorSim. Simulation results demonstrate the effectiveness of our replica placement technique considering various factors such as storage and workload constraints of replica servers, link capacity constraints, user QoS requirements, etc.","","Electronic:978-1-4244-9640-2; POD:978-1-4577-1448-1","10.1109/3PGCIC.2011.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103156","data grids;distributed algorithm;dynamic programming;quality of service;replication;workload and link constraints","Bandwidth;Cost function;Equations;Heuristic algorithms;Mathematical model;Quality of service;Servers","data analysis;distributed algorithms;dynamic programming;grid computing;information retrieval;quality assurance;quality of service;replicated databases;storage management","OptorSim;QoS satisfaction;average system performance;data access history;decentralized technique;distributed data-intensive applications;distributed placement;distributed technique;dynamic programming;hierarchical data grids;link capacity constraints;nearby replica;overall replication cost;placement algorithm;popular data files;quality assurance issues;quality requirements;realistic model;replica locations;replica placement problem;replica placement technique;replica servers;replicas;replication model;system QoS constraints;system perspective;traffic pattern;user QoS constraints;user QoS requirements;user perspective;wide-area networks","","3","","17","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Parallel in situ indexing for data-intensive computing","J. Kim; H. Abbasi; L. Chacón; C. Docan; S. Klasky; Q. Liu; N. Podhorszki; A. Shoshani; K. Wu","Lawrence Berkeley National Lab, USA","2011 IEEE Symposium on Large Data Analysis and Visualization","20111201","2011","","","65","72","As computing power increases exponentially, vast amount of data is created by many scientific research activities. However, the bandwidth for storing the data to disks and reading the data from disks has been improving at a much slower pace. These two trends produce an ever-widening data access gap. Our work brings together two distinct technologies to address this data access issue: indexing and in situ processing. From decades of database research literature, we know that indexing is an effective way to address the data access issue, particularly for accessing relatively small fraction of data records. As data sets increase in sizes, more and more analysts need to use selective data access, which makes indexing an even more important for improving data access. The challenge is that most implementations of indexing technology are embedded in large database management systems (DBMS), but most scientific datasets are not managed by any DBMS. In this work, we choose to include indexes with the scientific data instead of requiring the data to be loaded into a DBMS.We use compressed bitmap indexes from the FastBit software which are known to be highly effective for query-intensive workloads common to scientific data analysis. To use the indexes, we need to build them first. The index building procedure needs to access the whole data set and may also require a significant amount of compute time. In this work, we adapt the in situ processing technology to generate the indexes, thus removing the need of reading data from disks and to build indexes in parallel. The in situ data processing system used is ADIOS, a middleware for high-performance I/O. Our experimental results show that the indexes can improve the data access time up to 200 times depending on the fraction of data selected, and using in situ data processing system can effectively reduce the time needed to create the indexes, up to 10 times with our in situ technique when using identical parallel settings.","","Electronic:978-1-4673-0155-8; POD:978-1-4673-0156-5","10.1109/LDAV.2011.6092319","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092319","","Buildings;Computational modeling;Data models;Data processing;Indexing;Parallel processing","data analysis;database indexing;database management systems;disc storage;information retrieval;middleware;records management;storage management","DBMS;FastBit software;compressed bitmap indexes;data access;data analysis;data intensive computing;data records;data storage management;database management systems;database research literature;disk storage;middleware;parallel in situ indexing","","8","","22","","","23-24 Oct. 2011","","IEEE","IEEE Conference Publications"
"The Web Information Extraction for Update Summarization Based on Shallow Parsing","M. Peng; X. Ma; Y. Tian; M. Yang; H. Long; Q. Lin; X. Xia","Wuhan Univ., Wuhan, China","2011 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20111215","2011","","","109","114","Traditional text information extraction methods mainly act on static documents and are difficult to reflect the dynamic evolvement of information update on the web. To address this challenge, this work proposes a new method based on shallow parsing with rules. The rules are generated according to the syntactic features of English texts, such as the tense of verbs, the usages of modal verbs and so on. The latest novel information in English news texts is extracted correctly, to meet the needs of users for accessing to updated information of the developing events quickly and effectively. Performance results show the improvement of the proposed scheme in this work.","","Electronic:978-1-4244-9640-2; POD:978-1-4577-1448-1","10.1109/3PGCIC.2011.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103146","information extraction;shallow parsing;updated information;web texts","Data mining;Feature extraction;Green products;Natural language processing;Real time systems;Syntactics;Tagging","Internet;information retrieval;text analysis","English texts;Web information extraction;information update;shallow parsing;static documents;text information extraction;update summarization","","0","","9","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"A flow-guided file layout for out-of-core streamline computation","C. M. Chen; L. Xu; T. Y. Lee; H. W. Shen","The Ohio State University, 43210, USA","2011 IEEE Symposium on Large Data Analysis and Visualization","20111201","2011","","","115","116","We present a file reordering method to improve runtime I/O efficiency for out-of-core streamline computation. Because of the increasing discrepancy between the speed of computation and that of I/O on multi-core machines, the cost of I/O becomes a major bottleneck for out-of-core computation. Among techniques that reduce runtime I/O cost, reordering file layout to increase data locality has become popular in recent years. Better layout optimization relies on the knowledge of the data access pattern, which can be acquired from benchmarking. For streamline computation, we observe that the data access pattern is highly dependent on the flow directions. As a disk I/O request can generally be done more efficiently with shorter seek distances, we propose a novel flow-guided file layout method to improve the I/O performance. With a weighted directed graph to model the data access pattern, the file layout problem can be formulated as a linear graph arrangement problem. The goal is to minimized the sum of the disk seek time based on the linear distances between all pairs of adjacent graph nodes. We use a divide-and-conquer algorithm to approximate the optimal layout. The experimental results show that our flow-guided layout outperforms layouts that use space filling curves and some of the more recent cache-oblivious mesh layout methods.","","Electronic:978-1-4673-0155-8; POD:978-1-4673-0156-5","10.1109/LDAV.2011.6092326","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092326","","Benchmark testing;Computational modeling;Data models;Data visualization;Layout;Runtime;Vectors","benchmark testing;data flow computing;divide and conquer methods;file organisation;graphs;information retrieval;multiprocessing systems;optimisation","cache-oblivious mesh layout method;data access pattern;data locality;disk I-O request;divide-and-conquer algorithm;file reordering method;flow direction;flow-guided file layout;layout optimization;linear graph arrangement problem;multicore machine;optimal layout;out-of-core streamline computation;runtime I-O efficiency;space filling curve;streamline computation","","1","","4","","","23-24 Oct. 2011","","IEEE","IEEE Conference Publications"
"Concept map construction applying natural language processing on text extracted from e-commerce web pages","G. M. Caputo; N. F. F. Ebecken","Civil Engineering Department, Federal University of Rio de Janeiro, Brazil","2011 Third World Congress on Nature and Biologically Inspired Computing","20111201","2011","","","409","414","To understand the theory behind a given subject can be an expensive task depending on the domain complexity. In the products case, the quantity of functionalities and variants can be a big impediment for the user take a decision, since he/she has to understand each detail before choose from an offers variety. Moreover, there is a huge amount of information available in the internet and tutorials to explain each concept that is involved in each subject. The concepts are related to each other to create the main idea of the subject. This paper presents a methodology to extract these concepts in a way that it is easier to an end user to rapidly understand the subject using concept maps. Its also aims to extract the relationships between the concepts showing how they are related. Many techniques were combined to provide this knowledge mechanism, like natural language processing and information extraction. In the web documents case the information is presented with particularities, like tables, than it should be maintained to better access the associations.","","Electronic:978-1-4577-1124-4; POD:978-1-4577-1122-0","10.1109/NaBIC.2011.6089624","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089624","Information extraction;conceptual map;natural language processing","Clocks;Data mining;Indexing;Psychology;Tutorials","Web services;Web sites;electronic commerce;information retrieval;knowledge representation;natural language processing;text analysis","Internet;Web documents;Web pages;concept map;e-commerce;information extraction;knowledge mechanism;natural language processing;text extraction","","0","","19","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"Co-operative data access in multiple Road Side Units (RSUs)-based Vehicular Ad Hoc Networks (VANETs)","G. G. M. N. Ali; E. Chan","City University of Hong Kong, Kowloon, Hong Kong","2011 Australasian Telecommunication Networks and Applications Conference (ATNAC)","20111208","2011","","","1","6","Nowadays, data dissemination using Road Side Units (RSUs) in Vehicular Ad Hoc Networks (VANETs) has received important consideration to assist the inter-vehicle communication for overcoming the vehicle to vehicle frequent disconnection problem. During rush hour, an RSU may be overloaded by many requests submitted by the vehicles. Due to strict realtime and short wireless transmission range coverage constraints, a heavily overloaded RSU may experience high deadline miss rate in effect of serving too many requests beyond its capacity. In this work, we investigate the vehicle submitted requests are generally two types: delay sensitive and delay tolerant. We propose a multiple-RSU model, which offers the opportunity to the RSUs suffering from handling high volume workload to transfer some of its delay tolerant requests to other RSUs, which have light workload and located in the direction in which the vehicle is heading. By a series of simulation experiments, we also support our multiple-RSU based co-operative load transferring model, which extensively outperforms the single independent RSU based VANETs model against a number of performance metrics.","Pending","Electronic:978-1-4577-1712-3; POD:978-1-4577-1711-6","10.1109/ATNAC.2011.6096651","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096651","","Ad hoc networks;Data models;Delay;Load modeling;Roads;Time factors;Vehicles","cooperative communication;delays;information retrieval;vehicular ad hoc networks","VANET;cooperative data access;cooperative load transferring model;data dissemination;delay sensitive;delay tolerant;inter-vehicle communication;multiple road side units;multiple-RSU model;short wireless transmission range coverage constraints;vehicular ad hoc networks","","1","","19","","","9-11 Nov. 2011","","IEEE","IEEE Conference Publications"
"Strategies for orca call retrieval to support collaborative annotation of a large archive","S. R. Ness; A. Lerch; G. Tzanetakis","Computer Science, University of Victoria Canada","2011 IEEE 13th International Workshop on Multimedia Signal Processing","20111201","2011","","","1","6","The Orchive is a large audio archive of hydrophone recordings of Killer whale (Orcinus orca) vocalizations. Researchers and users from around the world can interact with the archive using a collaborative web-based annotation, visualization and retrieval interface. In addition a mobile client has been written in order to crowdsource Orca call annotation. In this paper we describe and compare different strategies for the retrieval of discrete Orca calls. In addition, the results of the automatic analysis are integrated in the user interface facilitating annotation as well as leveraging the existing annotations for supervised learning. The best strategy achieves a mean average precision of 0.77 with the first retrieved item being relevant 95% of the time in a dataset of 185 calls belonging to 4 types.","","Electronic:978-1-4577-1434-4; POD:978-1-4577-1432-0; USB:978-1-4577-1433-7","10.1109/MMSP.2011.6093798","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093798","","Collaboration;Correlation;Games;Mobile communication;Noise measurement;Sonar equipment;Whales","audio signal processing;hydrophones;information retrieval;learning (artificial intelligence);user interfaces","Killer whale vocalizations;audio archive;collaborative Web-based annotation;collaborative annotation;crowdsource Orca call annotation;hydrophone recordings;mobile client;orca call retrieval;retrieval interface;supervised learning;user interface","","0","","14","","","17-19 Oct. 2011","","IEEE","IEEE Conference Publications"
"Applying ontology to geographical scientific data extraction","Y. S. Chang; C. H. Chang; H. T. Cheng","Dept. of Comp. Sci. & Inf. Eng, National Taipei University, New Taipei City, Taiwan","2011 IEEE International Conference on Systems, Man, and Cybernetics","20111121","2011","","","3397","3402","It is increasingly concerning global change and its regional impacts. There are many scientific information systems focused on this issue. In this paper<sup>1</sup>, we, based on the metadata classification and ontology, propose a scientific information retrieving and extracting architecture that can tie the metadata classification mechanism to extract the searched file efficiently and can accurately access and view Argo' data. The architecture is built by utilizing mediator/wrapper architecture to develop a scientific data extracting system. The system can easily help oceanographer to analyze the ocean's ecology by means of temperature, salinity and other information. The result of performance evaluation shows that one is about the architecture with the help of metadata classification can extract user's desired data effectively and efficiently, and the other is concerned about correctness of information retrieval.","1062-922X;1062922X","Electronic:978-1-4577-0653-0; POD:978-1-4577-0652-3","10.1109/ICSMC.2011.6084194","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084194","Data Extraction;Geographical Ocean Data;Ontology","Data mining;Mediation;Natural languages;Ocean temperature;Ontologies;Temperature distribution","classification;ecology;geographic information systems;information retrieval;meta data;oceanography;ontologies (artificial intelligence);software architecture","geographical scientific data extraction;global change;information extracting architecture;information retrieval;metadata classification;ocean ecology;ontology;scientific information systems","","0","","16","","","9-12 Oct. 2011","","IEEE","IEEE Conference Publications"
"Improving automated documentation to code traceability by combining retrieval techniques","Xiaofan Chen; J. Grundy","Department of Computer Science, University of Auckland, New Zealand","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","20111212","2011","","","223","232","Documentation written in natural language and source code are two of the major artifacts of a software system. Tracking a variety of traceability links between software documentation and source code assists software developers in comprehension, efficient development, and effective management of a system. Automated traceability systems to date have been faced with a major open research challenge: how to extract these links with both high precision and high recall. In this paper we introduce an approach that combines three supporting techniques, Regular Expression, Key Phrases, and Clustering, with a Vector Space Model (VSM) to improve the performance of automated traceability between documents and source code. This combination approach takes advantage of strengths of the three techniques to ameliorate limitations of VSM. Four case studies have been used to evaluate our combined technique approach. Experimental results indicate that our approach improves the performance of VSM, increases the precision of retrieved links, and recovers more true links than VSM alone.","1938-4300;19384300","Electronic:978-1-4577-1639-3; POD:978-1-4577-1638-6","10.1109/ASE.2011.6100057","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100057","Clustering;Key Phrases;Regular Expression;Traceability;Vector Space Model","Clustering algorithms;Documentation;Engines;Software systems;Unified modeling language;Vectors","information retrieval;natural languages;pattern clustering;program diagnostics;source coding;system documentation","automated code traceability systems;automated software documentation;natural language;regular expression;retrieval techniques;source code;vector space model","","13","","33","","","6-10 Nov. 2011","","IEEE","IEEE Conference Publications"
"Unitization Benefits Associative Recognition Whereas Impairs Item Recognition","H. Shao; X. Weng","Inst. of Psychol., Beijing, China","2011 Third International Conference on Multimedia Information Networking and Security","20111215","2011","","","314","317","Multimedia storage involves unitized processing of different types of information, such as text, images, and audio. Although unitized processing could greatly enrich the input information, its role in subsequent retrieval is unknown. This study investigated the general effects of unitized processing on item recognition and associative recognition. Specifically, we induced two types of unitized processing, namely long-term unitized processing and short-term unitized processing through manipulating the types of the encoding tasks (short-term unitized processing) and the properties of the encoding materials (long-term unitized processing). Recognition for item information (single words) and associative information (word pairs) was tested using Tulving's (1985) remember/know response procedure. We found both long-term unitized processing and short-term unitized processing increased the overall performance for associative recognition whereas decreased the overall performance for item recognition. Therefore, strategic use of multimedia storage should be valued for the security and the flexibility of the individual information.","2162-8998;21628998","Electronic:978-0-7695-4559-2; POD:978-1-4577-1795-6","10.1109/MINES.2011.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103780","associtive recognition;item recognition;unitized processing","Animals;Compounds;Encoding;Multimedia communication;Neurons;Psychology;Security","associative processing;image recognition;information retrieval;multimedia systems","associative recognition;information retrieval;item recognition;multimedia storage","","0","","12","","","4-6 Nov. 2011","","IEEE","IEEE Conference Publications"
"Structure Framework of the Traditional Knowledge Database in China","D. Yuhuan; G. Luo; X. Dayuan; S. Faming","Coll. of Life & Environ. Sci, Minzu Univ. Of China, Beijing, China","2011 4th International Conference on Intelligent Networks and Intelligent Systems","20111215","2011","","","232","235","Recently, traditional knowledge (TK) has attracted increasing attention both in domestic and international society. As a long history and diverse culture in China, there are colourful traditional knowledge created by the ethnic people and local communities, but a lot of TK are disappearing or threatened by various factors. It is very important to inventory, organize and document these TK and associated genetic resource by use of databases, in order to record and protect the TK effectively. In this paper, a classification system for TK categories was proposed as a base structure for TK database development. Taking into account for the experience in India, a program planning and framework to develop the national TK Database was presented and structured in this paper, focusing on the TK in the ethnic minority areas of China, with the 5 categories. Therefore, the establishment of TKDL will be significant for fair and equitable benefit-sharing on access and use of TK in China.","","Electronic:978-0-7695-4543-1; POD:978-1-4577-1626-3","10.1109/ICINIS.2011.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104736","China;TK database;categories system;structure framework;traditional knowledge","Art;Biological information theory;Communities;Databases;Educational institutions;Patents","classification;history;information retrieval systems;information storage","China;classification system;ethnic people;history;local communities;program planning;traditional knowledge database","","2","","12","","","1-3 Nov. 2011","","IEEE","IEEE Conference Publications"
"An analysis of sentence level text classification for the Kannada language","R. Jayashree; M. K. Srikanta","Dept. of Comput. Sci., PES Inst. of Technol., Bangalore, India","2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR)","20111201","2011","","","147","151","With the rapid growth of internet, huge amount of data is available online. The ability to draw useful information from this digital data is quite challenging. The task of exploring and extracting information from native languages available on line is very much a useful task. The work presented here focuses on sentence level classification in the Kannada language. The most popular approaches in text categorization like Naïve Bayesian and Bag of Words (BOW) approaches are used in this work. It is evident that Bag of Words approach performs significantly better than Naïve Bayesian approach. The objective of the work is to find how sentence level classification works for Kannada Language, as it can be extended further to sentiment classification, Question Answering, Text Summarization and also for customer reviews in Kannada Blogs, because most user's comments, queries, opinions etc are expressed using sentences, hence this sentence level Text Classification becomes a special task of Text Classification problem. The work though focuses on very basic approaches presently, can later be extended to other methods like SVM, KNN etc.","","Electronic:978-1-4577-1196-1; POD:978-1-4577-1195-4","10.1109/SoCPaR.2011.6089130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089130","Bag of Words;Naïve Bayesian;kannada text classification;sentence level classification;single label","Bayesian methods;Computer science;Natural language processing;Text categorization;Training;Training data","Bayes methods;Internet;natural language processing;pattern classification;question answering (information retrieval);support vector machines;text analysis;word processing","Internet;KNN;Kannada Blogs;Kannada language;SVM;customer review;digital data;information extraction;question answering;sentence level text classification;sentiment classification;text summarization","","2","","14","","","14-16 Oct. 2011","","IEEE","IEEE Conference Publications"
"Data Archival to SD Card Via Hardware Description Language","O. Elkeelany; V. S. Todakar","Tennessee Tech University","IEEE Embedded Systems Letters","20111219","2011","3","4","105","108","The main objective of this letter is to present the design of an efficient, real-time data archival system to a secure digital flash memory card via reconfigurable hardware. The data access from the SD card is implemented completely using Verilog and hence, there is no use of any microcontroller or on-chip general purpose processors. And since the complete design is a single-purpose system, no extra hardware is required. The design has four independent modules for the required different operations on the SD memory card. These four modules are for single-block write, multiple-block write, single-block read, and multiple-block read operations. We show how the bidirectional access takes place correctly and the data integrity has been verified using cyclic redundancy code in both field-programmable gate array (FPGA) chip and the SD card controller.","1943-0663;19430663","","10.1109/LES.2011.2168804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022751","Flash memory read/write;Verilog HDL;secured digital (SD) protocol","Field programmable gate arrays;Flash memory;Information management;Microcontrollers;Program processors;Real time systems","data integrity;field programmable gate arrays;flash memories;hardware description languages;information retrieval;microcontrollers;reconfigurable architectures","SD memory card controller;Verilog;bidirectional access;cyclic redundancy code;data integrity;field programmable gate array chip;hardware description language;microcontroller;multiple block read operation;multiple block write operation;on-chip general purpose processor;real-time data archival system;secure digital flash memory card;single block read operation;single block write operation;single-purpose system","","3","","8","","20110919","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Robust correction of 3D geo-metadata in photo collections by forming a photo grid","S. Yousefi; F. A. Kondori; H. Li","Digital Media Lab., Applied Physics and Electronics, Ume&#x00E5; University, Ume&#x00E5;, Sweden 90187","2011 International Conference on Wireless Communications and Signal Processing (WCSP)","20111208","2011","","","1","5","In this work, we present a technique for efficient and robust estimation of the exact location and orientation of a photo capture device in a large data set. The provided data set includes a set of photos and the associated information from GPS and orientation sensor. This attached metadata is noisy and lacks precision. Our strategy to correct this uncertain data is based on the data fusion between measurement model, derived from sensor data, and signal model given by the computer vision algorithms. Based on the retrieved information from multiple views of a scene we make a grid of images. Our robust feature detection and matching between images result in finding a reliable transformation. Consequently, relative location and orientation of the data set construct the signal model. On the other hand, information extracted from the single images combined with the measurement data make the measurement model. Finally, Kalman filter is used to fuse these two models iteratively and enhance the estimation of the ground truth(GT) location and orientation. Practically, this approach can help us to design a photo browsing system from a huge collection of photos, enabling 3D navigation and exploration of our huge data set.","","Electronic:978-1-4577-1010-0; POD:978-1-4577-1009-4; USB:978-1-4577-1008-7","10.1109/WCSP.2011.6096689","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096689","","Cameras;Computational modeling;Data models;Estimation;Noise;Noise measurement;Uncertainty","Kalman filters;computer vision;feature extraction;image fusion;image matching;image retrieval;information retrieval;iterative methods;meta data","3D geometadata;3D navigation;GPS;GT orientation;Kalman filter;computer vision algorithms;data fusion;data set;ground truth location;image grid;image matching;information extraction;information retrieval;iterative fusion;measurement model;orientation sensor;photo browsing system;photo capture device;photo collections;photo grid;robust correction;robust estimation;robust feature detection;sensor data;signal model","","0","","21","","","9-11 Nov. 2011","","IEEE","IEEE Conference Publications"
"Toward a metrics suite for source code lexicons","L. R. Biggers; B. P. Eddy; N. A. Kraft; L. H. Etzkorn","Department of Computer Science, The University of Alabama, Tuscaloosa, USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","20111117","2011","","","492","495","In this paper we present an empirical study of relationships between three source code lexicons: the identifier, comment, and literal lexicons. We conjecture that shared and unique properties of these lexicons for the given subject system can inform the configuration of a source code retrieval technique for a particular software understanding activity or software evolution task. Thus, we seek to discover these lexicon properties, and so we investigate five lexicon measures that consider term frequency, term density, and term provenance.","1063-6773;10636773","Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9","10.1109/ICSM.2011.6080816","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080816","software lexicon;software metrics;text retrieval","Correlation;Density measurement;Indexes;Software measurement;Software systems","information retrieval;software metrics","literal lexicon;metrics suite;software evolution task;software understanding activity;source code lexicon;source code retrieval technique;term frequency;term provenance","","2","","14","","","25-30 Sept. 2011","","IEEE","IEEE Conference Publications"
"City sentinel - VAST 2011 mini challenge 1 award: “Outstanding integration of computational and visual methods”","N. Bánfi; L. Dudás; Z. Fekete; J. Göbö lös-Szabó; A. Lukács; Á. Nagy; A. Szabó; Z. Szabó; G. Szűcs","Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary","2011 IEEE Conference on Visual Analytics Science and Technology (VAST)","20111215","2011","","","305","306","We present City Sentinel, an in-house built visual analytic software capable of handling a large collection of textual documents by combining diverse text mining and visualization tools. We applied this tool for the Vast Challenge 2011, Mini Challenge 1 over millions of tweet messages. We demonstrate how City Sentinel aided the analyst in retrieving the hidden information from the tweet messages to analyze and locate a hypothetical epidemic outbreak.","","Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5","10.1109/VAST.2011.6102485","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102485","","Accidents;Blood;Cities and towns;Data mining;Image color analysis;Motion pictures;Tag clouds","data analysis;data mining;data visualisation;epidemics;information retrieval;medical computing","City sentinel;VAST 2011 mini challenge 1 award;computational methods;epidemic outbreak;hidden information retrieval;in-house built visual analytic software;text mining;textual documents;tweet messages;visual methods;visualization tools","","0","","3","","","23-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Requirements Traceability for Object Oriented Systems by Partitioning Source Code","N. Ali; Y. G. Gueheneuc; G. Antoniol","Ptidej Team, Ecole Polytech. de Montreal, Montreal, QC, Canada","2011 18th Working Conference on Reverse Engineering","20111117","2011","","","45","54","Requirements trace ability ensures that source code is consistent with documentation and that all requirements have been implemented. During software evolution, features are added, removed, or modified, the code drifts away from its original requirements. Thus trace ability recovery approaches becomes necessary to re-establish the trace ability relations between requirements and source code. This paper presents an approach (Coparvo) complementary to existing trace ability recovery approaches for object-oriented programs. Coparvo reduces false positive links recovered by traditional trace ability recovery processes thus reducing the manual validation effort. Coparvo assumes that information extracted from different entities (i.e., class names, comments, class variables, or methods signatures) are different information sources, they may have different level of reliability in requirements trace ability and each information source may act as a different expert recommending trace ability links. We applied Coparvo on three data sets, Pooka, SIP Communicator, and iTrust, to filter out false positive links recovered via the information retrieval approach, i.e., vector space model. The results show that Coparvo significantly improves the of the recovered links accuracy and also reduces up to 83% effort required to manually remove false positive links.","1095-1350;10951350","Electronic:978-0-7695-4582-0; POD:978-1-4577-1948-6","10.1109/WCRE.2011.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079774","Object-Oriented;Traceability;experts;requirements;source code;source code partitions","Accuracy;Electronic mail;Java;Manganese;Object oriented modeling;Software;Vectors","information retrieval;object-oriented methods;program diagnostics;systems analysis","Coparvo;information retrieval approach;object oriented systems;requirements traceability;software evolution;source code partitioning;source code portfolio;trace ability recovery approaches","","9","","25","","","17-20 Oct. 2011","","IEEE","IEEE Conference Publications"
"Challenges and proposed solutions towards telecentre sustainability: A Southern Africa case study","M. Sumbwanyambe; A. Nel; W. Clarke","University of Johannesburg, Mechanical Engineering Dept, Kingsway campus, Box 524, Auckland Park, 2006, South Africa","2011 IST-Africa Conference Proceedings","20111219","2011","","","1","8","Access to information through telecentres is essential for social and economical growth in rural areas of sub-Saharan Africa. While many governments have established telecentres as means of bridging the increasingly wide digital divide in rural or unserved areas, their corresponding sustainability or continual operation is in doubt due to various challenges. These challenges to information and communications technology (ICT) access has resulted in many of the rural population being unable to exploit the potential of promoting social economic development through innovative business solutions and education. In this study we evaluate the sustainability of telecentres in Zambia and South Africa and propose possible solutions to the problems that telecentres face. Strictly speaking, we focus on two telecentres; Comsol telecentre in KZN, South Africa and Kanyonyo Resource Centre in Mongu, Zambia.","","Electronic:978-1-905824-26-7; POD:978-1-4577-1077-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107359","Bottom of the Pyramid;Information;Telecentres;Universal access","Africa;Communities;Economics;Government;Internet","Digital Divide;information retrieval;socio-economic effects;sustainable development","ICT access;Kanyonyo Resource Centre;Mongu;Southern Africa;Zambia Comsol telecentre;digital divide;economical growth;education;information access;information and communications technology;innovative business solution;rural population;social economic development;social growth;subSaharan Africa;telecentre sustainability","","1","","24","","","11-13 May 2011","","IEEE","IEEE Conference Publications"
"Raster-vector integration based on SVG on mobile GIS platform","Feng Lin; Chaozhen Guo","College of Mathematics and Computer Science, Fuzhou University, China","2011 6th International Conference on Pervasive Computing and Applications","20111219","2011","","","378","383","First, this paper uses tile pyramid model based on SVG to realize raster-vector integration on mobile GIS platform, then a new storage architecture with two-level cache buffer is designed which can speed up the system to retrieve the data of map tiles. J2EE and J2ME technologies are used to make the mobile GIS platform with high scalability and modularization. Finally, this paper verifies the feasibility and efficient of the platform through a test.","","Electronic:978-1-4577-0208-2; POD:978-1-4577-0209-9","10.1109/ICPCA.2011.6106534","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106534","GIS;SVG;mobile platform;tile pyramid model;two-level cache","Argon;Image resolution;Libraries;Rendering (computer graphics);Servers;Tiles;XML","cache storage;computer graphics;geographic information systems;information retrieval;mobile computing","SVG;cache buffer;data retrieval;mobile GIS platform;raster vector integration;scalable vector graphics;storage architecture","","0","","10","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Evaluating Annotators Consistency with the Aid of an Innovative Database Schema","Z. Theodosiou; O. Georgiou; N. Tsapatsoulis","Dept. of Commun. & Internet Studies, Cyprus Univ. of Technol., Limassol, Cyprus","2011 Sixth International Workshop on Semantic Media Adaptation and Personalization","20111215","2011","","","74","78","Automatic semantic tagging of multimedia is still inefficient due to the difficulties in modelling abstract or complex terms using low level features. The degree of consensus and homogeneity in judgements among annotators is very important in semantic image and video retrieval. In this paper we present a novel method in evaluating the annotators consistency, which uses an innovative database schema and combines two different annotation approaches. A set of 100 images were annotated by 16 annotators using vocabulary keywords and free keywords. The results indicate that combination of annotation methods may lead to increased annotation consistency compared to a single method but this is not a general fact. As expected the use of free keywords and images require tagging that is not directly related to their content, lead to increase the annotators inconsistency.","","Electronic:978-0-7695-4524-0; POD:978-1-4577-1372-9","10.1109/SMAP.2011.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103506","Annotators consistency;Database schema;Free keywords;Vocabulary keywords","Conferences;Databases;Multimedia communication;Portals;Semantics;Tagging;Vocabulary","information retrieval;multimedia databases;multimedia systems","annotators consistency;automatic semantic tagging;free keywords;image retrieval;innovative database schema;multimedia;video retrieval;vocabulary keywords","","0","","14","","","1-2 Dec. 2011","","IEEE","IEEE Conference Publications"
"Promoting widespread diffusion of ICTs to speed up achievements of the MDGs in Africa: A case for the MDG 2 in Uganda","W. Okaka","Kyambogo University, Uganda","2011 IST-Africa Conference Proceedings","20111219","2011","","","1","9","The paper discusses the crucial role of ICTs to leap-frog the status of the African countries to achieve the full status of the Millennium Development Goals (MDGs) of the UN by 2015. Uganda is a typical show case of a sub-Saharan African country. The objectives are to: assess the status of the MDG 2 in Uganda, the challenges encountered, and the diffusion of ICTs in Africa. In this review we collated published evidence on the performance of Uganda in implementing the MDG 2 since its inception in 1997 in the county. Information was accessed using internet search engines and libraries. There is a wide rural-urban digital gap, weak ICT infrastructures, and low awareness at the expense of quality Universal Primary Education (UPE). There is limited access to ICTs, ICT illiteracy, poor quality education, lack of e- books or ICT instructional materials to cut the costs of school administration like communication.","","Electronic:978-1-905824-26-7; POD:978-1-4577-1077-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107342","ICTs;MDG2;UPE;infrastructure;policy;quality;transparency","Africa;Economics;Educational institutions;Government;Internet;Media","Internet;digital libraries;electronic publishing;information retrieval;search engines","ICT illiteracy;ICT infrastructures;ICT instructional materials;Internet search engines;MDG 2;Millennium Development Goal;UPE;Uganda;Universal Primary Education;e-books;information access;libraries;poor quality education;rural-urban digital gap;school administration;sub-Saharan African country","","0","","18","","","11-13 May 2011","","IEEE","IEEE Conference Publications"
"An exploratory study of feature location process: Distinct phases, recurring patterns, and elementary actions","J. Wang; X. Peng; Z. Xing; W. Zhao","School of Computer Science and Technology, Fudan University, Shanghai, China","2011 27th IEEE International Conference on Software Maintenance (ICSM)","20111117","2011","","","213","222","Developers often have to locate the parts of the source code that contribute to a specific feature during software maintenance tasks. This activity, referred to as feature location in software engineering, is a human- and knowledge-intensive process. Researchers have investigated information retrieval, static/dynamic analysis based techniques to assist developers in such feature location activities. However, little work has been done on better understanding how developers perform feature location tasks. In this paper, we report an exploratory study of feature location process, consisting of two experiments in which developers were given unfamiliar systems and asked to complete six feature location tasks in two hours. Our study suggests that feature location process can be understood hierarchically at three levels of granularities: phase, pattern, and action. Furthermore, our study suggests that these feature-location phases, patterns and actions can be effectively imparted to junior developers and consequently improve their performance on feature location tasks. Our results open up new opportunities to feature location research, which could lead to better tool support and more rigorous feature location process.","1063-6773;10636773","Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9","10.1109/ICSM.2011.6080788","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080788","conceputal framework;feature location;human study","Finance;Graphical user interfaces","information retrieval;program diagnostics;software maintenance","distinct phases;dynamic analysis based techniques;elementary actions;feature location process;human-intensive process;information retrieval;knowledge-intensive process;recurring patterns;software engineering;software maintenance tasks;source code;static analysis based techniques","","11","","18","","","25-30 Sept. 2011","","IEEE","IEEE Conference Publications"
"An adaptive approach to impact analysis from change requests to source code","M. Gethers; H. Kagdi; B. Dit; D. Poshyvanyk","Computer Science Department, The College of William and Mary, Williamsburg, VA 23185, USA","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","20111212","2011","","","540","543","The paper presents an adaptive approach to perform impact analysis from a given change request (e.g., a bug report) to source code. Given a textual change request, a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Additionally, the approach configures the best-fit combination of information retrieval, dynamic analysis, and data mining of past source code commits to produce an improved impact set. The tandem operation of the three techniques sets it apart from other related solutions.","1938-4300;19384300","Electronic:978-1-4577-1639-3; POD:978-1-4577-1638-6","10.1109/ASE.2011.6100120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100120","","Association rules;Couplings;History;Indexing;Maintenance engineering;Software","data mining;indexing;information retrieval;program compilers;program diagnostics","data mining;dynamic analysis;impact analysis;impact set;information retrieval;latent semantic indexing;source code;textual change request","","13","","27","","","6-10 Nov. 2011","","IEEE","IEEE Conference Publications"
"Using domain knowledge for fostering the collaborative ability of a web dialogue system","M. Gatius; M. González","Software Department, Technical University of Catalonia, Barcelona, Spain","2011 7th International Conference on Next Generation Web Services Practices","20111201","2011","","","129","134","In this paper we describe the use of domain ontologies in a flexible web dialogue system for improving both its adaptability and its collaborative ability. We have developed a multilingual mixed-initiative dialogue system for guiding the user when accessing several types of web services. The use of domain ontologies facilitates the adaptation of the system to new web services as well as the generation of more collaborative responses.","","Electronic:978-1-4577-1127-5; POD:978-1-4577-1125-1","10.1109/NWeSP.2011.6088165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088165","Dialogue systems;communication task modeling;domain ontologies","Collaboration;Cultural differences;Decision support systems;Ontologies;Prototypes;Semantics;Web services","Web services;groupware;human computer interaction;information retrieval;interactive systems;natural language processing;ontologies (artificial intelligence);user interfaces","Web dialogue system;Web services;collaborative ability;domain knowledge;domain ontologies;multilingual mixed-initiative dialogue system;natural language systems","","0","","19","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"Extracting Academic Information from Conference Web Pages","P. Wang; Y. You; B. Xu; J. Zhao","Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China","2011 IEEE 23rd International Conference on Tools with Artificial Intelligence","20111215","2011","","","952","959","Conference Web pages are the main platforms to share the conference information and organize conference events. To discover the academic knowledge from such Web pages for building academic ontologies or social networks, it is necessary to extract academic information from conference Web pages. This paper proposes an approach to extract academic information from conference Web pages. Firstly, Web pages are segmented into text blocks by analyzing the visual feature and DOM structure. Then Bayes Network is used to classify these text blocks into predefined categories, and the quality of initial classification results are improved after post-processing. Finally, the academic information is extracted from the classified text blocks. Our experimental results on the real world datasets show that the proposed method is highly effective and efficient for extracting academic information from conference Web pages, and it has average 90% precision and 89% recall.","1082-3409;10823409","Electronic:978-0-7695-4956-7; POD:978-1-4577-2068-0","10.1109/ICTAI.2011.164","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103455","Bayes Network;DOM structure;Visual Feature;Web Information Extraction","Algorithm design and analysis;Classification algorithms;Data mining;Feature extraction;Semantics;Web pages","Internet;belief networks;data mining;information retrieval;ontologies (artificial intelligence);social networking (online);text analysis","Bayes network;DOM structure;academic information extraction;academic knowledge discovery;academic ontologies;conference Web pages;conference event organization;conference information;social networks;text blocks;visual feature","","2","","26","","","7-9 Nov. 2011","","IEEE","IEEE Conference Publications"
"Towards a Dialogic Archive: Canadian Copyright Law, Digital Archives and Fair Dealing","D. M. Meurer","Joint Grad. Program in Commun. & Culture, York & Ryerson Univ., Toronto, ON, Canada","2011 Second International Conference on Culture and Computing","20111215","2011","","","57","62","Intellectual property laws present formidable challenges for digital cultural archiving projects. While cultural institutions may be permitted to make works available to the public, these rights may not cover online publishing or reproduction. Consequently, institutions undertake time-consuming, resource-intensive clearance processes in order to secure licenses for displaying copyright protected cultural works online. In response to these challenges, the New Media Collaboration Centre at York University (Toronto) has developed Art mob, a content management system (CMS) with a dialogic approach to copyright informed by the fair dealing provisions of Canadian copyright law. The user interface design of this CMS facilitates uses covered by fair dealing, enables administrators to enter and display information regarding the layering of rights in cultural works, solicits information about unidentified rights holders and the creation of works from communities of users, and invites rights holders to assert their rights and license works.","","Electronic:978-0-7695-4546-2; POD:978-1-4577-1593-8","10.1109/Culture-Computing.2011.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103210","copyright;cultural heritage;digital archives;digital collections;fair dealing","Cultural differences;Digital communication;Global communication;Law;Licenses;Materials;Production","content management;copyright;history;information retrieval systems;user interfaces","Artmob;York University;canadian copyright law;clearance processes;content management system;copyright protected cultural works;dialogic archive;digital cultural archiving projects;fair dealing;intellectual property laws;new media collaboration centre;user interface design","","0","","15","","","20-22 Oct. 2011","","IEEE","IEEE Conference Publications"
"Towards Searching Domain Assets Based on Semantic Similarity","W. Guo; J. Wang; K. He; D. Yu; P. Du","State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China","2011 Seventh International Conference on Semantics, Knowledge and Grids","20111201","2011","","","199","202","Domain Assets are the domain knowledge constructed according to the common requirements in the domain. In order to reuse the domain assets effectively, a domain assets search algorithm is proposed in this paper. Compared with the keyword search, this algorithm is based on semantic similarity, and the domain assets that closely satisfy the users can be selected so as to alleviate the burden of users when users search the domain assets. In addition, an example in transportation domain is illustrated to validate the effect of this algorithm.","","Electronic:978-0-7695-4515-8; POD:978-1-4577-1323-1","10.1109/SKG.2011.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088117","","Algorithm design and analysis;Data models;Databases;Educational institutions;Ontologies;Semantics;Transportation","information retrieval;search problems;semantic networks","domain assets search algorithm;domain knowledge;keyword search;semantic similarity;transportation domain","","0","","5","","","24-26 Oct. 2011","","IEEE","IEEE Conference Publications"
"A problem-snapshot view of patient history data for cardiac preoperative evaluation","H. Chen; J. An; X. Lu; H. Duan","College of Biomedical Engineering and Instrument Science, Yuquan Campus, Zhejiang University, The Key Laboratory of Biomedical Engineering, Ministry of Education, China, Hangzhou, Zhejiang Province, P.R. China, 310027","2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)","20111212","2011","4","","1904","1908","Preoperative risk evaluation is a very important work before a cardiac surgery. Clinicians always have to spend much time reviewing the information about operative risk factors. In this paper, we have made a few tentative attempts to help clinicians improve the efficiency of risk evaluation. First a surgery-problem network for cardiac surgeries is built to extract the associated medical problems from patient history problems. Then a set of data filter rules are defined for each medical problem to retrieve the required data, which is clustered around the problems and forms a set of snapshots. After that, we propose a multi-level-timeline method to visualize the problems and snapshots extracted. Finally, a special problem-snapshot visualization tool for cardiac preoperative evaluation is designed and developed as a result. The tool provides a quick way of reviewing the information which is helpful for cardiac preoperative risk evaluation through information visualization technology. It can save a lot of tedious work.","1948-2914;19482914","Electronic:978-1-4244-9352-4; POD:978-1-4244-9351-7","10.1109/BMEI.2011.6098707","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6098707","cardiac preoperative evaluation;data filter rules;problem-snapshot;surgery-problem network;visualization","Data mining;Data visualization;History;Laboratories;Medical diagnostic imaging;Surgery","cardiology;data visualisation;information retrieval;medical information systems;risk management;surgery","cardiac preoperative risk evaluation;cardiac surgery;data filter rules;data retrieval;information visualization technology;medical problems;multilevel timeline method;operative risk factors;patient history data;problem-snapshot view;surgery-problem network","","1","","18","","","15-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"Web service complex request ontology and its answers models","B. Batouche; Y. Naudet; F. Guinand","Public Research Center Henri Tudor, Luxembourg","2011 7th International Conference on Next Generation Web Services Practices","20111201","2011","","","55","60","The Web service requests are increasingly complex, the complex request requires many functionalities having any dependencies, binary dependencies or global dependencies. The request can requires also constraints, objectives and conditions. All requested elements having a semantically links which they are defined as an ontology. The ontology can be used as a smart questionnaire to formalize the request, and its semantic is useful to provide automatically the resolution way. Since we can not find an answering service for a complex request, we use the composition of existing Web services to design the answer (s). Then we determinate -according to the request characteristics- the suitable composition model. This paper presents an ontology of complex request and its corresponded answering compositions models.","","Electronic:978-1-4577-1127-5; POD:978-1-4577-1125-1","10.1109/NWeSP.2011.6088153","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088153","Semantic Web Services;Web Service Complex Request;Web Service Composition Models","Adaptation models;Analytical models;Cities and towns;Cultural differences;Ontologies;Unified modeling language;Web services","Web services;ontologies (artificial intelligence);question answering (information retrieval);semantic Web","Web service complex request ontology;answering compositions model;binary dependency;smart questionnaire","","0","","15","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"3D Model Multiple Semantic Automatic Annotation for Small Scale Labeled Data Set","F. Tian; X. k. Shen; L. Xian-mei; X. Hong-tao","State Key Lab. of Virtual Reality Technol. & Syst., BeiHang Univ., Beijing, China","2011 International Conference on Virtual Reality and Visualization","20111201","2011","","","193","198","Automatically assigning keywords to 3D models is of great interest as it allows one to retrieve, index, organize and understand large collections of 3D models. Most Methods require high sample size for training, so the data quality is in high demand. For small scale labeled data set, we propose a semi-supervised method to realize the 3D models multiple semantic annotation, which needs only a small amount of hand tagged information provided by users. The proposed technique utilizes low-level shape features and the keywords are assigned using a graphed-based label transfer mechanism to expand the training dataset. A weighted metric learning method is used to learn the distance measure from the extended dataset. Then multiple semantic annotation task can be completed on the learned distance measure. The proposed method outperforms the current state-of-the-art methods on the small scale labeled dataset and large unlabelled dataset. We believe that such measure will provide a strong platform to label 3D models when a small amount of labeled models were given.","","Electronic:978-0-7695-4602-5; POD:978-1-4577-2156-4","10.1109/ICVRV.2011.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092712","3D model annotation;3D model retrieval","Computational modeling;Data models;Labeling;Measurement;Semantics;Solid modeling;Three dimensional displays","information retrieval;learning (artificial intelligence);solid modelling","3D model multiple semantic automatic annotation;graphed-based label transfer mechanism;hand tagged information;semi-supervised method;small scale labeled data set;weighted metric learning method","","0","","15","","","4-5 Nov. 2011","","IEEE","IEEE Conference Publications"
"Automatic text classification and focused crawling","S. Samarawickrama; L. Jayaratne","University of Colombo School of Computing, 35, Reid Avenue, Colombo 7, Sri Lanka","2011 Sixth International Conference on Digital Information Management","20111201","2011","","","143","148","A focused crawler is a web crawler that traverse the web to explore information that is related to a particular topic of interest only. On the other hand, generic web crawlers try to search the entire web, which is impossible due to the size and the complexity of WWW. In this paper we make a survey of some of the latest focused web crawling approaches discussing each with their experimental results. We categorize them as focused crawling based on content analysis, focused crawling based on link analysis and focused crawling based on both the content and link analysis. We also give an insight to the future research and draw the overall conclusions.","Pending","Electronic:978-1-4577-1539-6; POD:978-1-4577-1538-9","10.1109/ICDIM.2011.6093329","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093329","","Crawlers;Search engines;Support vector machines;Training;Training data;Vectors;Web pages","Web sites;information retrieval;pattern classification;search engines;text analysis","WWW;Web crawler;automatic text classification;content analysis;focused crawler;link analysis","","1","","24","","","26-28 Sept. 2011","","IEEE","IEEE Conference Publications"
"Educational content retrieval based on semantic Web services","A. B. Gil; F. d. l. Prieta; S. Rodríguez; B. Martín","Computer Science and Automation Dept., University of Salamanca, Spain","2011 7th International Conference on Next Generation Web Services Practices","20111201","2011","","","135","140","In the current educational context there has been a significant increase in learning object repositories (LOR), which are found in large databases available on the hidden web. All the information about these learning objects (LOs) is described in any metadata labeling standard (LOM, Dublin Core, etc). It is necessary to work and develop solutions that provide efficiency in searching for heterogeneous content and finding distributed context. Distributed information retrieval, or federated search, attempts to respond to the problem of information retrieval in the hidden Web. The main aim of a federated search is to develop models and strategies to get the most benefit from these distributed sources. Thus, users perceive the system as a single point of access to information they require, regardless of the number of sources that exist, its location, or its management mechanism. The process is completely transparent to the user, who does not perceive its complexity and therefore treats any of the information retrieved uniformly. This paper describes an architecture that recovers educational content, called AIREH (Architecture for Intelligent Recovery of Educational content in Heterogeneous Environments), which combines the dynamism and flexibility of multi-agent organizations. The proposal uses systems and mechanisms by which agents acquire roles based on the generation and use of dynamic web services.","","Electronic:978-1-4577-1127-5; POD:978-1-4577-1125-1","10.1109/NWeSP.2011.6088166","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088166","Federated Search;Learning Object;Repositories","Adaptation models;Atmospheric modeling;Context;Metasearch;Organizations;Standards organizations;Web services","Web services;computer aided instruction;information retrieval;meta data;multi-agent systems","AIREH educational content;architecture for intelligent recovery of educational content in heterogeneous environments;distributed information retrieval;educational content retrieval;federated search;hidden Web;learning object repositories;metadata labeling standard;multi-agent organization;semantic Web service","","0","","12","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"Tracing requirements to tests with high precision and recall","C. Ziftci; I. Krueger","Computer Science Department, University of California at San Diego, USA","2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)","20111212","2011","","","472","475","Requirements traceability is linking requirements to software artifacts, such as source code, test-cases and configuration files. For stakeholders of software, it is important to understand which requirements were tested, whether sufficiently, if at all. Hence tracing requirements in test-cases is an important problem. In this paper, we build on existing research and use features, realization of functional requirements in software [15], to automatically create requirements traceability links between requirements and test-cases. We evaluate our approach on a chat system, Apache Pool [21] and Apache Log4j [11]. We obtain precision/recall levels of more than 90%, an improvement upon currently existing Information Retrieval approaches when tested on the same case studies.","1938-4300;19384300","Electronic:978-1-4577-1639-3; POD:978-1-4577-1638-6","10.1109/ASE.2011.6100102","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100102","automated analysis;program understanding;requirements traceability;testing","Documentation;Large scale integration;Programming;Software engineering;Software systems;Testing","formal specification;formal verification;information retrieval;program diagnostics;source coding","Apache Log4j;Apache Pool;chat system;functional software Celal requirements;information retrieval approaches;requirement traceability;software artifacts;software stakeholders;source code","","4","","26","","","6-10 Nov. 2011","","IEEE","IEEE Conference Publications"
"A Bottom-up Approach of Web Data Extraction based on Entity Recognition and Integration","T. Liu; D. Shen; J. Shan; T. Nie; Y. Kou","Coll. of Inf. Sci. & Eng., Northeastern Univ., Shenyang, China","2011 Eighth Web Information Systems and Applications Conference","20111201","2011","","","150","155","Nowadays, most popular methods for web data extraction (WDE) are top-down ones depending on structure. However, these techniques are not scalable enough when coming to complex pages. Consequently, we put forward a bottom-up approach for WDE based on entity recognition and integration to avoid over dependency to structure of web pages. The approach proposed focuses on primary text sequences labeling first and also gives consideration to repetitive patterns of them as well. We propose a Two-Level extraction model for entity recognition and repetitive pattern extraction algorithm for entity integration. Our approach can effectively reduce the attribute labeling mistakes. Also, we demonstrate our approach by scientifically experimental results. The conclusion is that our approach perform better than the traditional extraction techniques, especially on complex Web pages.","","Electronic:978-0-7695-4555-4; POD:978-1-4577-1812-0","10.1109/WISA.2011.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093582","","Arrays;Context;Data mining;HTML;Labeling;Redundancy;Web pages","Internet;information retrieval","Web data extraction;Web pages;bottom-up approach;entity integration;entity recognition;text sequences","","0","","14","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"Interactive Mobile Campus Based on Position Perception","W. Liu; Z. Liu; Y. Zhang","Jiangsu Province Support Software Eng. R&D, Center for Modern Inf. Technol. Applic. in Enterprise, Suzhou, China","2011 Fourth International Symposium on Computational Intelligence and Design","20111117","2011","1","","147","150","Mobile location service of dynamic geographic information space application that supported school teachers and students is a currently a research hotspot for universities. The applications of classroom navigation, teacher-student tracking monitoring, registration management, etc. it will meet the school management which offers convenience for personal position information retrieval, recommend services and individualized maps constructed. How to use mobile network to organically integrate LBS and WebGIS, and to construct a new generation of mobile information services platform and system environment based on the campus network location perception, this paper presents several key technologies of system framework and implementation.","","Electronic:978-0-7695-4500-4; POD:978-1-4577-1085-8","10.1109/ISCID.2011.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079657","Interactive;LBS;Mobile campus;WebGIS","Business;Educational institutions;Mobile communication;Mobile handsets;Servers;Wireless communication;Zigbee","educational institutions;geographic information systems;information retrieval;interactive systems;mobile computing;mobility management (mobile radio);personal information systems;radionavigation","LBS;WebGIS;campus network location perception;classroom navigation;dynamic geographic information space application;individualized map construction;interactive mobile campus;mobile information services platform;mobile location services;mobile network;personal position information retrieval;position perception;registration management;school management;school teachers;service recommendation;teacher-student tracking monitoring;universities","","1","","10","","","28-30 Oct. 2011","","IEEE","IEEE Conference Publications"
"A framework of service oriented semantic search engine","S. Su; D. Wu; Y. Tang; J. Li; S. Wang; Z. Li","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Sichuan, China","2011 International Conference on Computational Problem-Solving (ICCP)","20111201","2011","","","499","504","A service oriented semantic search engine framework is proposed to extract automatically and accurately information on service provider on the Internet. Domain knowledge on theme and abstract service provider are modeled by ontology to guide information extraction. Theme identification for each web page is executed after the page is fetched by search engine. A page or a part of content of a page is assigned a theme. Concrete service provider entity and attributes are extracted from content of web pages based on themes of the pages. Several algorithms are developed. Computational experiments demonstrate that service oriented semantic search engine shows high recall rate and precision. Highest recall rate of concrete service provider entity exceeds 97% on tested web sites. Precision of concrete service provider entity reaches up to 100% on two tested web sites. It also shows good recall rate and precision of concrete service provider attributes.","","Electronic:978-1-4577-0603-5; POD:978-1-4577-0602-8","10.1109/ICCPS.2011.6092243","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092243","","Concrete;Data mining;Ontologies;Organizations;Search engines;Semantics;Web pages","Internet;Web sites;information retrieval;ontologies (artificial intelligence);search engines;service-oriented architecture","Internet;Web pages;concrete service provider entity;information extraction;ontology;service oriented semantic search engine","","0","1","10","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"IntentFinder: A system for discovering significant information implicit in large, heterogeneous document collections and computationally mapping social networks and command nodes","L. Ungar; S. Leibholz; C. Chaski","Computer and Information Science, University of Pennsylvania, and VizorNet, Inc., Philadelphia, PA, USA","2011 IEEE International Conference on Technologies for Homeland Security (HST)","20111219","2011","","","219","223","IntentFinder is a computational method of extracting mutually relevant information from a large collection of narrative data. We describe an approach that takes advantage of a new view of documents as coming from evolving stories. IntentFinder consists of six main components: 1) A document management system; 2) A story extraction system; 3) A significance determination system; 4) A reputation management; 5) A lexical-semantic analysis; 6) A user interface. In addition a method has been found for quantitatively determining the topology and hierarchy of a social subnetwork embedded inside a very noisy self-reorganizing network (e.g., the Internet). All these components will work together to allow analysts to discover and understand events and stories implicit in collections of documents, including newswire, reports, emails and tweets, which would be prohibitively difficult to uncover manually, and ultimately estimating the organizational structure of a social network.","","Electronic:978-1-4577-1376-7; POD:978-1-4577-1375-0","10.1109/THS.2011.6107874","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107874","Documents;Intelligence;Lexical;Messages;Reputation;Semantic;Story integration","Correlation;Data mining;Network topology;Organizations;Social network services;Topology;User interfaces","document handling;information retrieval;social networking (online);user interfaces","IntentFinder;Internet;command nodes;document management system;heterogeneous document collections;lexical-semantic analysis;mutually relevant information extraction;noisy self-reorganizing network;reputation management;significance determination system;social networks;social subnetwork;story extraction system;topology;user interface","","1","","21","","","15-17 Nov. 2011","","IEEE","IEEE Conference Publications"
"Open-access MIMIC-II database for intensive care research","J. Lee; D. J. Scott; M. Villarroel; G. D. Clifford; M. Saeed; R. G. Mark","Harvard-MIT Division of Health Sciences and Technology, Massachusetts Institute of Technology, Cambridge, MA, USA","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20111201","2011","","","8315","8318","The critical state of intensive care unit (ICU) patients demands close monitoring, and as a result a large volume of multi-parameter data is collected continuously. This represents a unique opportunity for researchers interested in clinical data mining. We sought to foster a more transparent and efficient intensive care research community by building a publicly available ICU database, namely Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II). The data harnessed in MIMIC-II were collected from the ICUs of Beth Israel Deaconess Medical Center from 2001 to 2008 and represent 26,870 adult hospital admissions (version 2.6). MIMIC-II consists of two major components: clinical data and physiological waveforms. The clinical data, which include patient demographics, intravenous medication drip rates, and laboratory test results, were organized into a relational database. The physiological waveforms, including 125 Hz signals recorded at bedside and corresponding vital signs, were stored in an open-source format. MIMIC-II data were also deidentified in order to remove protected health information. Any interested researcher can gain access to MIMIC-II free of charge after signing a data use agreement and completing human subjects training. MIMIC-II can support a wide variety of research studies, ranging from the development of clinical decision support algorithms to retrospective clinical studies. We anticipate that MIMIC-II will be an invaluable resource for intensive care research by stimulating fair comparisons among different studies.","1094-687X;1094687X","Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8","10.1109/IEMBS.2011.6092050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092050","","Arterial blood pressure;Biomedical imaging;Biomedical monitoring;Databases;Hospitals;Monitoring","data mining;decision support systems;information retrieval systems;medical information systems;patient care;patient monitoring;relational databases","ICU database;ICU patient;bedside;clinical data mining;clinical decision support algorithms;data use agreement;frequency 125 Hz;health information;human subject training;intensive care II;intensive care research;intensive care unit patient;intravenous medication drip rates;laboratory test;multiparameter data;multiparameter intelligent monitoring;open-access MIMIC-II database;open-source format;patient demographics;physiological waveforms;relational database;signal recording","Access to Information;Adult;Databases, Factual;Humans;Intensive Care;Monitoring, Physiologic;Research;Wavelet Analysis","4","","15","","","Aug. 30 2011-Sept. 3 2011","","IEEE","IEEE Conference Publications"
"Semantic P2P search engine","I. Rudomilov; I. Jelínek","Czech Technical University in Prague, Department of Computer Science and Engineering","2011 Federated Conference on Computer Science and Information Systems (FedCSIS)","20111114","2011","","","991","995","This paper discusses the possibility to use Peer-to-Peer (P2P) scenario for information-retrieval (IR) systems for higher performance and better reliability than classical client-server approach. Our research emphasis has been placed on design intelligent Semantic Peer-to-Peer search engine as multi-agent system (MAS). The main idea of the proposed project is to use semantic model for P2P overlay network, where peers are specified as semantic meta-models by the standardized OWL language from The World Wide Web Consortium. Using semantic model improve the quality of communication between intelligent peers in this P2P network. Undoubtedly, proposed semantic P2P network has all advantages of normal P2P networks and in the first place allow deciding a point with bottle-neck effect (typical problem for client-server applications) by using a set of peers for storing and data processing.","","Electronic:978-83-60810-39-2; POD:978-1-4577-0041-5; USB:978-83-60810-35-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078266","","Computer science;Network topology;Peer to peer computing;Search engines;Semantics;Servers;Topology","Internet;information retrieval;knowledge representation languages;multi-agent systems;peer-to-peer computing;search engines","OWL language;P2P network;P2P overlay network;World Wide Web consortium;client-server approach;information-retrieval systems;multiagent system;peer-to-peer scenario;semantic P2P search engine;semantic meta-models;semantic model;semantic peer-to-peer search engine","","0","","16","","","18-21 Sept. 2011","","IEEE","IEEE Conference Publications"
"Tracking Web Video Topics: Discovery, Visualization, and Monitoring","J. Cao; C. W. Ngo; Y. D. Zhang; J. T. Li","Institute of Computing Technology, Chinese Academy of Science, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","20111205","2011","21","12","1835","1846","Despite the massive growth of web-shared videos in Internet, efficient organization and monitoring of videos remains a practical challenge. While nowadays broadcasting channels are keen to monitor online events, identifying topics of interest from huge volume of user uploaded videos and giving recommendation to emerging topics are by no means easy. Specifically, such process involves discovering of new topic, visualization of the topic content, and incremental monitoring of topic evolution. This paper studies the problem from three aspects. First, given a large set of videos collected over months, an efficient algorithm based on salient trajectory extraction on a topic evolution link graph is proposed for topic discovery. Second, topic trajectory is visualized as a temporal graph in 2-D space, with one dimension as time and another as degree of hotness, for depicting the birth, growth, and decay of a topic. Finally, giving the previously discovered topics, an incremental monitoring algorithm is proposed to track newly uploaded videos, while discovering new topics and giving recommendation to potentially hot topics. We demonstrate the application on three months' videos crawled from YouTube during December 2008 to February 2009. Both objective and user studies are conducted to verify the performance.","1051-8215;10518215","","10.1109/TCSVT.2011.2148470","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756649","Topic trajectory mining;video recommendation;visualization","Algorithm design and analysis;Monitoring;Recommender systems;Trajectory;Visualization","Internet;data visualisation;graph theory;information retrieval;video signal processing","Internet;Web video topic tracking;Web-shared videos;broadcasting channels;salient trajectory extraction;temporal graph;topic content visualization;topic discovery;topic evolution incremental monitoring;topic evolution link graph;topic trajectory","","19","","24","","20110429","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Concept Similarity Computation for Ontology in the Automatic Question-Answering System","W. Wen-yan; W. Wei; L. Xing-yang; X. Li-zhen","Dept. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China","2011 Eighth Web Information Systems and Applications Conference","20111201","2011","","","41","46","As the key point of the ontology research field, concept similarity computation has been widely used in the automatic question-answering systems. This paper firstly analyzes the existing concept similarity computation algorithm which is based on the semantic distance, and then proposes a new computation algorithm formula by modifying and adding some factors which can affect the accuracy of concept similarity computation. Finally, the experimental results show that the proposed algorithm offers better effectiveness for the concept similarity computation.","","Electronic:978-0-7695-4555-4; POD:978-1-4577-1812-0","10.1109/WISA.2011.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093600","Automatic question-answering system;Concept similarity;Ontology;semantic","Algorithm design and analysis;Computer science;Databases;Educational institutions;Ontologies;Semantics;Web services","ontologies (artificial intelligence);question answering (information retrieval)","automatic question answering system;concept similarity computation;ontology research field;semantic distance","","0","","7","","","21-23 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Mining Technique Using <formula formulatype=""inline""><tex Notation=""TeX"">$N$</tex> </formula>n-Grams and Motion Transcripts for Body Sensor Network Data Repository","V. Loseu; H. Ghasemzadeh; R. Jafari","Computer Engineering Department, University of Texas at Dallas , Richardson, TX, USA","Proceedings of the IEEE","20111219","2012","100","1","107","121","Recent years have witnessed a large influx of applications in the field of cyber-physical systems. An important class of these systems is body sensor networks (BSNs) where lightweight embedded processors and communication systems are tightly coupled with the human body. BSNs can provide researchers, care providers and clinicians access to tremendously valuable information extracted from data that are collected in users' natural environment. With this information, one can monitor the progression of a disease, identify its early onset, or simply assess user's wellness. One major obstacle is managing repositories that store the large amount of sensing data. To address this issue, we propose a data mining approach inspired by the experience in the areas of text and natural language processing. We represent sensor readings with a sequence of characters, called motion transcripts. Transcripts reduce complexity of the data significantly while maintaining morphological and structural properties of the physiological signals. To further take advantage of the physiological signal's structure, our data mining technique focuses on the characteristic transitions in the signals. These transitions are efficiently captured using the concept of <i>n</i>-grams. To facilitate a lightweight and fast mining approach, we reduce the overwhelmingly large number of <i>n</i>-grams via information gain (IG) feature selection. We report the effectiveness of the proposed approach in terms of the speed of mining while maintaining an acceptable accuracy in terms of the F-score combining both precision and recall.","0018-9219;00189219","","10.1109/JPROC.2011.2161238","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995280","<formula formulatype=""inline""><tex Notation=""TeX"">$n$</tex></formula>-grams;Body sensor networks (BSNs);Patricia tree;data mining;string templates","Biomedical monitoring;Body sensor networks;Cyberspace;Data mining;Data models;Network topology","body sensor networks;data mining;diseases;information retrieval;medical signal processing;natural language processing;patient monitoring;physiological models;text analysis","body sensor network;cyber physical systems;data mining;data repository;disease;feature selection;information extraction;information gain;motion transcripts;n-grams;natural language processing;patient monitoring;physiological signals;text analysis","","3","","48","","20110822","Jan. 2012","","IEEE","IEEE Journals & Magazines"
"Concepts and Implementation of a Semantic Web Archiving and Simulation System for RF Propagation Measurements","V. K. Rajendran; J. N. Murdock; A. Duran; T. S. Rappaport","Wireless Networking & Commun. Group (WNCG), Univ. of Texas at Austin, Austin, TX, USA","2011 IEEE Vehicular Technology Conference (VTC Fall)","20111201","2011","","","1","5","In this paper, we present an Open-Source web-based archiving system to organize and share wireless RF propagation measurement data, models, and simulation software in a centralized, standardized archive. This archiving system is based on Semantic Web ideas that will enable the wireless research community to easily share and access measured data and simulators provided by researchers across the globe. To begin development of the web-based archiving environment, we use a previously developed RF propagation simulator, SIRCIM, to represent the range of values, types of measurements, and file format types that would be needed to properly archive measurements from the research community at large. This paper also explores development issues and considerations required to build a Semantic Web on-line propagation channel measurement and modeling archiving system for global use.","1090-3038;10903038","Electronic:978-1-4244-8327-3; POD:978-1-4244-8328-0","10.1109/VETECF.2011.6093285","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093285","","Computational modeling;Data models;Extraterrestrial measurements;Frequency measurement;Narrowband;Semantic Web;Wireless communication","computerised instrumentation;information retrieval systems;public domain software;radiowave propagation;semantic Web;telecommunication computing","RF propagation measurement simulation system;SIRCIM RF propagation simulator;open-source Web-based archiving system;semantic Web archiving;semantic Web on-line propagation channel measurement;simulation software;wireless RF propagation measurement data;wireless research community","","0","","15","","","5-8 Sept. 2011","","IEEE","IEEE Conference Publications"
"On a proposal to integrate web sources using semantic-web technologies","H. A. Sleiman; C. R. Rivero; R. Corchuelo","University of Sevilla, Spain","2011 7th International Conference on Next Generation Web Services Practices","20111201","2011","","","326","331","Companies comprise a variety of software applications to carry out their business activities. A recurrent challenge is how to make them interoperate with each other which is usually handcrafted, which is a tedious task that increases integration costs. Enterprise Service Buses range amongst the most popular solution to reduce these costs, and they allow to implement integration solutions by means of one or more layers between software applications and business processes. In this paper, we present a framework for information extraction that allow to wrap information from different web sources and to generate linked data. Furthermore, we survey a number of approaches in the bibliography to build Enterprise Service Buses in the context of semantic-web technologies, which comprise RDF, RDFS, OWL, and SPARQL languages. Finally, we conclude that, thanks to linked data, we may integrate software applications with other applications that generate and/or consume these linked data.","","Electronic:978-1-4577-1127-5; POD:978-1-4577-1125-1","10.1109/NWeSP.2011.6088199","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088199","Semantic web services and linked data;Semantic web services architectures","Context;Data mining;OWL;Ontologies;Web services;Web sites","business data processing;information retrieval;semantic Web","SPARQL language;Web sources;business activity;business process;enterprise service buses;information extraction;semantic Web technology;software application","","0","","64","","","19-21 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Fingerprint Recognition Scheme Based on Assembling Invariant Moments for Cloud Computing Communications","J. Yang; N. Xiong; A. V. Vasilakos; Z. Fang; D. Park; X. Xu; S. Yoon; S. Xie; Y. Yang","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, Jiangxi, China","IEEE Systems Journal","20111121","2011","5","4","574","583","In cloud computing communications, information security entails the protection of information elements (e.g., multimedia data), only authorized users are allowed to access the available contents. Fingerprint recognition is one of the popular and effective approaches for priori authorizing the users and protecting the information elements during the communications. However, traditional fingerprint recognition approaches have demerits of easy losing rich information and poor performances due to the complex inputs, such as image rotation, incomplete input image, poor quality image enrollment, and so on. In order to overcome these shortcomings, in this paper, a new fingerprint recognition scheme based on a set of assembled invariant moment (geometric moment and Zernike moment) features to ensure the secure communications is proposed. And the proposed scheme is also based on an effective preprocessing, the extraction of local and global features and a powerful classification tool, thus it is able to handle the various input conditions encountered in the cloud computing communication. The experimental results show that the proposed method has a higher matching accuracy comparing with traditional or individual feature based methods on public databases.","1932-8184;19328184","","10.1109/JSYST.2011.2165600","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026222","Assembling;cloud computing communications;fingerprint recognition scheme;invariant moments","Authentication;Biometrics;Cloud computing;Feature extraction;Fingerprint recognition;Moment methods;Multimedia communication","assembling;cloud computing;feature extraction;fingerprint identification;information retrieval;security of data","assembling invariant moments;cloud computing communication;feature extraction;fingerprint recognition;information protection;information security;public databases;users authorization","","12","","32","","20110923","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"US IOOS<sup>®</sup> - from integrated to Interdependent and Indispensible","Z. Willis","US IOOS Program Office, NOAA, Sliver Spring, USA","OCEANS'11 MTS/IEEE KONA","20111219","2011","","","1","4","Integration is defined providing rapid access to multidisciplinary data from many sources and to provide data and information required to achieve multiple goals that historically have been the domain of separate agencies, offices, or programs. There are plenty of examples and efforts underway within US Integrated Ocean Observing System (IOOS<sup>®</sup>) that are moving us to a fully integrated system. The President's Executive Order 13547, outlines our policy to achieve “an America whose stewardship ensures that the ocean, our coasts, and the Great Lakes are healthy and resilient, safe and productive, and understood and treasured so as to promote the wellbeing, prosperity, and security of present and future generations.” For US IOOS to be a vital contributor to this goal we suggest a need to move beyond Integration to become “Interdependent” and “Indispensible.” Given the current fiscal climate, no one agency, entity or program can act alone. The scientific information needed to guide personal and programmatic decisions necessitates taking the next step to interdependence. Are we ready to take that step? Interdependences also refer to the fact that the subsystems can not stand alone. To deliver necessary capabilities IOOS elements must progress from the research and development stages, through efficient test and operational efforts into routine operations. Technologies must be incubated and rapidly inserted to keep the US IOOS system operating effectively and efficiently. Prompt delivery of meaningful output to the end user demands a fully interdependent system. One in which the models and observations are standardized and work seamlessly together to rapidly tailor the output to users' needs. But interdependence also suggests that each partner be it at the Federal, State, Local and Tribal level or the Private sector needs to be the- e for the long term. The analogy is really the three legged stool. Once you become interdependent, if one of the partners pulls back the stool falls when ability for the other partners/legs are unable to support the enterprise. This demands not only policy and technology, but trust. Once partners and programs are fully interdependent they become indispensible. This paper will provide examples within IOOS that suggests the move is underway. The paper will provide an update on the US IOOS Blueprint to Full Capability, will provide examples of observation networks that rely on multiple partnerships to remain viable, and how the information, products and services are now have been fully endorsed by managers and decision makers.","0197-7385;01977385","Electronic:978-0-933957-39-8; Paper:978-1-4577-1427-6","10.23919/OCEANS.2011.6106947","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106947","Integrated Ocean Observing System;Integration;Interdependent;Ocean Observing","Communities;Radar;Sea measurements;Sea surface;Tsunami;US Government agencies","data integration;geophysics computing;information networks;information retrieval;oceanographic techniques","IOOS elements;US IOOS;US Integrated Ocean Observing System;data integration;fully interdependent system;multidisciplinary data;observation networks;rapid data access;scientific information","","0","","","","","19-22 Sept. 2011","","IEEE","IEEE Conference Publications"
"A Web Service search approach based on semantic and search engine","Gao Shu; XiongWei Xu; Huang Hua","School of Computer Science, Wuhan University of Technology, 430063 China","2011 6th International Conference on Pervasive Computing and Applications","20111219","2011","","","484","489","Traditional method of Web Service search is based on the UDDI registry. UDDI is very popular in the domain of Web Service publishing, but it's not very suitable for Web Service discovering as it is based on keyword match. In this paper we propose a model to discover Web service based on semantic and search engine, and design its architecture. At the same time, we put forward the algorithm of splitting words and the algorithm of query expansion, develop a Web Service search prototype, give a case study, and discuss the recall, the precision and the efficiency of the prototype.","","Electronic:978-1-4577-0208-2; POD:978-1-4577-0209-9","10.1109/ICPCA.2011.6106551","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106551","Web Service search;expansion query;search engine;semantic","Education;HTML;Internet;Microstrip","Web services;information retrieval;search engines","UDDI registry;Web service discovery;Web service publishing;Web service search approach;keyword match;search engine;semantic engine","","0","","10","","","26-28 Oct. 2011","","IEEE","IEEE Conference Publications"
"Renovation by Machine-Assisted Program Transformation in Production Reporting and Integration","S. Mintchev","Baring Asset Manage., London, UK","2011 18th Working Conference on Reverse Engineering","20111117","2011","","","406","410","In corporate IT, subject areas like Production Reporting and Enterprise Application Integration are routinely considered in isolation. Needs are often met by purchasing separate product suites or packages, which can be incompatible, and contain unused overlapping functionality. In this paper we discuss our experience of applying a more holistic approach. We look at how purchased software can be extended in-house with the help of a program transformation technique, and can then be utilised in a service-oriented architecture for the purposes of information retrieval, data and process integration. By reusing software components for reporting and integration purposes, we have been able to realise savings in all phases of the software lifecycle.","1095-1350;10951350","Electronic:978-0-7695-4582-0; POD:978-1-4577-1948-6","10.1109/WCRE.2011.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079868","Program transformation;SQR;enterprise application integration;production reporting;service architecture","Generators;Investments;Java;Layout;Software;XML","information retrieval;manufacturing data processing;object-oriented programming;program compilers;service-oriented architecture;software reliability;software reusability","corporate IT;data integration;enterprise application integration;information retrieval;machine-assisted program transformation;process integration;product packages;production integration;production reporting;program transformation technique;purchased software;separate product suites;service-oriented architecture;software components reuse;software lifecycle;unused overlapping functionality","","0","","10","","","17-20 Oct. 2011","","IEEE","IEEE Conference Publications"
"Research on Question Classification Method of Tibetan Online Automatic Question-Answering System","R. Te","Qinghai Tibetan Center for Inf. Process. Comput. Sci. Dept., Qinghai Normal Univ., Xining, China","2011 4th International Conference on Intelligent Networks and Intelligent Systems","20111215","2011","","","211","213","This paper mainly describes the question classification method of Tibetan online automatic question-answering system. As each type of question is designed of certain matching rules, if a question and a rule matches, this question belongs to the corresponding answer category. Therefore, the use of this strategy can reduce the search space and improve the efficiency of information retrieval. In particular, using this method in the Tibetan question-answering system can effectively improve people to obtain more accurate information.","","Electronic:978-0-7695-4543-1; POD:978-1-4577-1626-3","10.1109/ICINIS.2011.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104730","Tibetan;question classification;question-answering system","Grammar;Information processing;Natural language processing;Search engines;Semantics","natural language processing;pattern classification;question answering (information retrieval)","Tibetan online automatic question-answering system;information retrieval;matching rule;question classification method","","0","","8","","","1-3 Nov. 2011","","IEEE","IEEE Conference Publications"
