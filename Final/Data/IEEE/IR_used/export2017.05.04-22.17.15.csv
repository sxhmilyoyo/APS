"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6965074,6965086,6964667,6964981,6964670,6965218,6962478,6964681,6963051,6960713,6958423,6963067,6958364,6958852,6958564,6959906,6956590,6837494,6954222,6949269,6950180,6950922,6950659,6949382,6949843,6949463,6943852,6944623,6947835,6945108,6947493,6947862,6946844,6947957,6587035,6598664,6940528,6938748,6774489,6936652,6931418,6931303,6932954,6932687,6933648,6931456,6933023,6883166,6927094,6927531,6927615,6927630,6927559,6927543,6927555,6928198,6930544,6927550,6927046,6927553,6930203,6927061,6927769,6927638,6927662,6927641,6921978,6918444,6926174,6921618,6921489,6923178,6921853,6924291,6926506,6921608,6921986,6921417,6921606,6923239,6924476,6923122,6921604,6923245,6921771,6921607,6921412,6921769,6921605,6921497,6921676,6923816,6920710,6921172,6921147,6921126,6920724,6919856,6920734,6921064",2017/05/04 22:17:15
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Geographic focus detection using multiple location taggers","P. Berger; P. Hennig; D. Glaeser; H. Klement; C. Meinel","Hasso-Plattner-Institute, University of Potsdam, Germany","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","403","408","Being able to identify locations associated to a Web resource is essential for providing location-based Web applications. However, geographical information in Web documents is rarely supplied in a machine-readable way and therefore not easily discoverable. As a consequence, it is necessary to extract geographical keywords from Web documents and to associate locations with them. This method is called location tagging. In this paper we present a location tagging approach for unstructured documents which utilizes multiple external location providers. Detected locations are ranked according to their relevance for the document, in order to identify a document's geographical focus, which is its most representative location. We present an exemplary implementation of our proposed approach using two location providers and evaluate our method's applicability.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921618","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921618","","Asia;Blogs;Conferences;Social network services;Tagging;Text analysis;Web pages","Internet;document handling;geographic information systems;information retrieval","Web documents;Web resource;document geographical focus identification;geographic focus detection;geographical information;geographical keyword extraction;location-based Web applications;multiple external location providers;multiple location taggers;unstructured documents","","0","","9","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Improving security mechanism to access HDFS data by mobile consumers using middleware-layer framework","S. Singh; S. Sharma","Department of Computer Science, Lovely Professional University, Punjab, India","Fifth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20141120","2014","","","1","7","Revolution in the field of technology leads to the development of cloud computing which delivers on-demand and easy access to the large shared pools of online stored data, softwares and applications. It has changed the way of utilizing the IT resources but at the compromised cost of security breaches as well such as phishing attacks, impersonation, lack of confidentiality and integrity. Thus this research work deals with the core problem of providing absolute security to the mobile consumers of public cloud to improve the mobility of user's, accessing data stored on public cloud securely using tokens without depending upon the third party to generate them. This paper presents the approach of simplifying the process of authenticating and authorizing the mobile user's by implementing middleware-centric framework called MiLAMob model with the huge online data storage system i.e. HDFS. It allows the consumer's to access the data from HDFS via mobiles or through the social networking sites eg. facebook, gmail, yahoo etc using OAuth 2.0 protocol. For authentication, the tokens are generated using one-time password generation technique and then encrypting them using AES method. By implementing the flexible user based policies and standards, this model improves the authorization process.","","CD-ROM:978-1-4799-2695-4; Electronic:978-1-4799-2696-1; POD:978-1-4799-2697-8","10.1109/ICCCNT.2014.6963051","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963051","Authentication;Authorization;Computing;HDFS;MiLAMob;OAuth 2.0;Security;Token","Authentication;Cloud computing;Data models;Mobile communication;Permission;Social network services","authorisation;cloud computing;cryptography;information retrieval;middleware;mobile computing;protocols;social networking (online);storage management","AES method;Facebook;Gmail;HDFS data access;IT resources;MiLAMob model;OAuth 2.0 protocol;Yahoo;authorization process;cloud computing;encryption;flexible user based policies;middleware-centric framework;middleware-layer framework;mobile consumers;one-time password generation technique;online data storage system;online stored data;public cloud;security mechanism;social networking sites;tokens","","0","","6","","","11-13 July 2014","","IEEE","IEEE Conference Publications"
"LBDA: A novel framework for extracting content from web pages","A. S. Vijendran; C. Deepa","Department Of MCA SNR Sons College, Coimbatore, Tamilnadu, India","2013 International Conference on Advanced Computing and Communication Systems","20141030","2013","","","1","7","The internet presents an enormous amount of useful information which is usually formatted for web users, but it is a complex task to extract the relevant data from various web sources. Recently, many approaches for data extraction from web pages have been proposed and each having their own merits and limitations. This paper provides a simple but effective approach, named layout based detachment approach (LBDA). The proposed approach extracts the main content from the web page and removes the irrelevant information like header, footer contents, navigation bars, advertisements and other noisy images. The proposed methodology uses the following techniques: tag tree parsing to get the analysis structure, block acquiring page segmentation method to remove unwanted tags, and data extraction to retrieve the necessary contents. It can eliminate noise and extract the main content blocks from web page effectively and display the essential content to the users. The performance is evaluated based on the following metrics like precision, recall, accuracy, execution time and memory usage. The implementation results obviously show that our proposed LBDA approach is performed better than the existing heuristic approach.","","Electronic:978-1-4799-3506-2; POD:978-1-4799-3507-9","10.1109/ICACCS.2013.6938748","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6938748","DOM tree analysis;Web mining;Web page content extraction;Web structure mining","Data mining;HTML;Layout;Noise;Visualization;Web pages","Internet;information retrieval","LBDA approach;Web page;accuracy metric;block acquiring page segmentation method;content extraction;content retrieval;data extraction;execution time metric;layout based detachment approach;memory usage metric;precision metric;recall metric;tag tree parsing technique","","0","","18","","","19-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"Swarming in the Urban Web Space to Discover the Optimal Region","C. Kumar; U. Gruenefeld; W. Heuten; S. Boll","Univ. of Oldenburg, Oldenburg, Germany","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","2","","234","241","People moving to a new place usually look for a suitable region with respect to their multiple criteria of interests. In this work we map this problem to the migration behavior of other species such as swarming, which is a collective behavior exhibited by animals of similar size which aggregate together, milling about the same region. Taking the swarm intelligence perspective, we present a novel method to find relevant geographic region for citizens based on Particle Swarm Optimization (PSO) framework. Particles represent geographic regions which are moving in the map space to find a region most relevant with respect to user's query. The characterization of geographic regions is based on the multi-criteria distribution of geo-located facilities or landscape structure from the Open Street Map data source. We enable end users to visualize and evaluate the regional search process of PSO via a Web interface. The proposed framework demonstrates high precision and computationally efficient performance for regional search over a vast city based dataset.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927630","Geographic Regions;Local Search;Particle Swarm Optimization;Regional Search;Spatial Decision Making;Swarm Intelligence","Cities and towns;Data visualization;Equations;Geospatial analysis;Mathematical model;Optimization;Particle swarm optimization","Internet;geographic information systems;information retrieval;particle swarm optimisation;user interfaces","OpenStreetMap data source;PSO;Web interface;city based dataset;computationally efficient performance;geo-located facilities;geographic region;landscape structure;migration behavior;multicriteria distribution;optimal region discovery;particle swarm optimization framework;regional search process;swarm intelligence perspective;urban Web space swarming","","0","","20","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Data encryption using Counting Bloom Filters for cloud security","A. Nagamalli; S. S. Krishna; B. KeerthiPriya","E.C.E Dept., Gayatri Vidya Parishad Coll. of Eng.(A), Visakhapatnam, India","Third International Conference on Computational Intelligence and Information Technology (CIIT 2013)","20141110","2013","","","514","518","Encryption of data is of utmost importance for secure transmission. Traditional keyword search techniques downloading all the data and decrypting locally is clearly impractical, due to the huge amount of bandwidth cost in cloud scale systems. This paper presents a methodology for encrypting the data for limited reserved strings that are stored in Counting Bloom Filters (CBF). CBF is a small array of element addresses that are connected to the array through a hashing mechanism. Counting bloom filter is used for increasing the speed of the set membership tests. If the string is a member of counting bloom filter it is encrypted using Data Encryption Standard, else the string is transmitted without encryption. This reduces the effective time for encryption and increases security.","","Electronic:978-1-84919-859-2","10.1049/cp.2013.2638","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950922","Counting bloom filter;DES algorithm;Data encryption;Fiestal function;Linear Feedback Shift register","","cloud computing;cryptography;data structures;information retrieval;set theory","CBF;cloud scale systems;cloud security;counting bloom filters;data encryption standard;hashing mechanism;keyword search techniques;set membership tests","","0","","","","","18-19 Oct. 2013","","IET","IET Conference Publications"
"Analysis of patent portfolio and knowledge flow of the global semiconductor industry","C. C. Chiu; H. N. Su","Graduate Institute of Technology Management, National Chung Hsing University, Taichung, Taiwan","Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration","20141013","2014","","","3621","3634","This study aims to analyze semiconductor patents in following dimensions: characterizing patent, forecasting future trends, uncovering key patent, as well as proposing patent strategies for the development of semiconductor industry. In addition, this research studies investigate of patents, in 1) national, 2) industrial and 3) organizational levels, through social network analysis, two-dimensional contour map analysis and patent characteristics analysis. In this study, semiconductor patents are retrieved, on the basis of IPC, from the United State Patent and Trademark Office (USPTO) database. Furthermore, the development trend and knowledge flow are investigated and visualized through the analysis of the numbers of patents, litigated patents, technological life cycle, patent citations and patent information, etc.","2159-5100;21595100","Electronic:978-1-890843-29-8; POD:978-1-4799-5769-9; USB:978-1-890843-30-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921147","","Companies;Electronics industry;Integrated circuits;Patents;Silicon;Social network services;Transistors","information retrieval;manufacturing data processing;patents;semiconductor industry;social networking (online)","IPC;USPTO database;United State Patent and Trademark Office;future trends forecasting;global semiconductor industry;industrial levels;knowledge flow;litigated patents;national levels;organizational levels;patent characteristics analysis;patent characterization;patent citations;patent information;patent portfolio analysis;patent strategies;semiconductor industry development;semiconductor patents;social network analysis;technological life cycle;two-dimensional contour map analysis","","0","","73","","","27-31 July 2014","","IEEE","IEEE Conference Publications"
"Enhanced Particle Swarm Optimization algorithm with resuse guided retrieval capabilities","N. Nouaouria; M. Boukadoum; R. Proulx","Department of computer science, UQAM, Montreal, Canada","2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing","20141016","2014","","","392","399","This paper presents a novel associative memory model to perform the retrieval stage in a case based reasoning system. The described approach makes no prior assumption of a specific organization of the case memory, thus leading to a generic recall process. This is made possible by using Particle Swarm Optimization (PSO) to compute the neighborhood of a new problem, followed by direct access to the cases it contains. The fitness function of the PSO stage has a reuse semantic that combines similarity and adaptability as criteria for optimal case retrieval. The model was experimented on two proprietary databases and compared to the flat memory model for performance. The obtained results are very promising.","","Electronic:978-1-4799-6081-1; POD:978-1-4799-6082-8","10.1109/ICCI-CC.2014.6921489","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921489","Case Based Reasoning (CBR);Particle Swarm Optimization (PSO);Reuse-Guided-Retrieval (RGR)","Adaptation models;Computational modeling;Mathematical model;Organizations;Particle swarm optimization;Search problems;Wind speed","associative processing;case-based reasoning;information retrieval;particle swarm optimisation","PSO;case based reasoning system;enhanced particle swarm optimization algorithm;flat memory model;novel associative memory model;optimal case retrieval;resuse guided retrieval capabilities","","0","","21","","","18-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"A stochastic model for budget distribution over time in search advertisements","R. Qin; Y. Yuan; J. Li; Y. Yang","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","Proceedings of 2014 IEEE International Conference on Service Operations and Logistics, and Informatics","20141120","2014","","","166","171","In search advertisements, advertisers have to seek for an effective allocation strategy to distribute the limited budget over a series of sequential temporal slots (e.g., days). However, advertisers usually have no sufficient knowledge to determine the optimal budget for each temporal slot, because there exist much uncertainty in search advertising markets. In this paper, we present a stochastic model for budget distribution over a series of sequential temporal slots under a finite time horizon, assuming that the best budget is a random variable. We study some properties and feasible solutions for our model, taking the best budget being characterized by either normal distribution or uniform distribution, respectively. Furthermore, we also make some experiments to evaluate our model and identify strategies with the real-world data collected from practical advertising campaigns. Experimental results show that a) our strategies outperform the baseline strategy that is commonly used in practice; b) the optimal budget is more likely to be normally distributed than uniformly distributed.","","Electronic:978-1-4799-6058-3; POD:978-1-4799-6059-0; USB:978-1-4799-6057-6","10.1109/SOLI.2014.6960713","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960713","budget constraint;budget distribution;optimal budget;search advertisement;stochastic programming","Gaussian distribution;Programming","advertising;information retrieval;stochastic processes","allocation strategy;budget distribution;finite time horizon;normal distribution;optimal budget;search advertisements;search advertising markets;sequential temporal slots;stochastic model;temporal slot;uniform distribution","","0","","8","","","8-10 Oct. 2014","","IEEE","IEEE Conference Publications"
"Sparse Modeling of Magnitude and Phase-Derived Spectra for Playing Technique Classification","L. Su; H. M. Lin; Y. H. Yang","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20141016","2014","22","12","2122","2132","Computational modeling of musical timbre is important for a variety of music information retrieval applications. While considerable progress has been made to recognize musical genres and instruments, relatively little attention has been paid to modeling playing techniques, which affect timbre in more subtle ways. In this paper, we contribute to this area of research by systematically evaluating various audio features and processing methods for multi-class playing technique classification, considering up to nine distinct playing techniques of bowed string instruments. Specifically, a collection of 6,759 chamber-recorded single notes of four bowed string instruments and a collection of 33 real-world solo violin recordings are used in the evaluation. Our evaluation shows that using sparse features extracted from the magnitude spectra and phase derivatives including group delay function (GDF) and instantaneous frequency deviation (IFD) leads to significantly better performance than using a combination of state-of-the-art temporal, spectral, cepstral and harmonic feature descriptors. For playing technique classification of violin singe notes, the former approach attains 0.915 macro-average F-score under a tenfold cross validation setting, while the latter only attains 0.835. Moreover, sparse modeling of magnitude and phase-derived spectra also performs well for single-note joint instrument-technique classification (F-score 0.770) and for playing technique classification of real-world violin solos (F-score 0.547). We find that phase information is particularly important in discriminating playing techniques with subtle differences, such as playing with different bowing positions (i.e., normal, sul tasto, and sul ponticello). A systematic investigation of the effect of parameters such as window sizes, hop factors, window types for phase-derived features is also reported to provide more insights.","2329-9290;23299290","","10.1109/TASLP.2014.2362006","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918444","Group delay function;instantaneous frequency deviation;phase;playing technique classification;sparse coding","Encoding;Feature extraction;Harmonic analysis;Instruments;Timbre;Time-frequency analysis","information retrieval;music;musical instruments","GDF;IFD;audio features methods;audio processing methods;bowed string instruments;computational modeling;group delay function;hop factors;instantaneous frequency deviation;magnitude derived spectra;magnitude spectra;multiclass playing technique classification;music information retrieval applications;musical genres;musical instruments;musical timbre;phase derivatives;phase derived features;phase derived spectra;phase information;playing technique classification;real-world violin solos;sparse feature extraction;sparse modeling;violin singe notes;window sizes;window types","","1","","53","","20141008","Dec. 2014","","IEEE","IEEE Journals & Magazines"
"A novel replica of fuzzy ontology for web information gathering","R. L. Kumar; M. Vina; P. Sengottuvelan","Department of information technology, Bannari Amman Institute of Technology, Sathyamangalam, Tamilnadu, India","2013 Fifth International Conference on Advanced Computing (ICoAC)","20141016","2013","","","366","369","Information retrieval (IR) has become an active area of research in recent years to handle with the dynamic environment by establishing intelligent systems which can offer effective web content in real time to various wired or wireless devices. Evolutionary and adaptive systems (EASs) have been widely used. Web information gathering has been widely investigated and a number of approaches have been proposed to carry out web information gathering in an effluent way. In such systems ontology has been expected to have a bright future. In order to represent user profiles in personalized web information gathering several ontology models are widely used. These models are used for representing and reasoning over user profiles. To further improve the overall performance of the ontology based information retrieval system, fuzzy ontology model has been presented. This fuzzy based ontology model learns user profiles from both world knowledge base and user local instance warehouse. This ontology model is evaluated by comparing its performance against benchmark models in web information gathering.","2377-6927;23776927","CD-ROM:978-1-4799-3447-8; Electronic:978-1-4799-3448-5; POD:978-1-4799-3449-2","10.1109/ICoAC.2013.6921978","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921978","Fuzzy;Ontology;Personalization;User Profiles;World knowledge","Frequency modulation;Indexes;Ontologies","Internet;fuzzy set theory;information retrieval;ontologies (artificial intelligence)","EAS;IR;Web content;Web information gathering;evolutionary and adaptive systems;fuzzy ontology;information retrieval;reasoning;user local instance warehouse;user profiles;world knowledge base","","0","","12","","","18-20 Dec. 2013","","IEEE","IEEE Conference Publications"
"Lone Star Stack: Architecture of a Disk-Based Archival System","M. Grawinkel; G. Best; M. Splietker; A. Brinkmann","Johannes-Gutenberg Univ. Mainz, Mainz, Germany","2014 9th IEEE International Conference on Networking, Architecture, and Storage","20141016","2014","","","176","185","The need for huge storage systems rises with the ever growing creation of data. With growing capacities and shrinking prices, ""write once read sometimes"" workloads become more common. New data is constantly added, rarely updated or deleted, and every stored byte might be read at any time - a common pattern for digital archives or big data scenarios. We present the LoneStar Stack, a disk based archival storage system building block that is optimized for high reliability and energy efficiency. It provides a POSIX file system interface that uses flash based storage for write-offloading and metadata and the disk-based LoneStar RAID for user data storage. The RAID attempts to spin down disks as soon and as long as possible. For reads, only a single disk is accessed, while writes require 3 additional parity disks to be spun up. The cache aggregates new files and a semantic data placement engine decides how they are persisted to the RAID. Asynchronous data movers then persist the data. The system provides an end-to-end data integrity, an elastic fault tolerance that can at least recover from all 3-disk failures, and provides multiple paths for data integrity checking and recovery. The system can use 70% of the raw disk capacity and is optimized for fast reads with a minimum number of powered on disk drives.","","Electronic:978-1-4799-4087-5; POD:978-1-4799-4086-8","10.1109/NAS.2014.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923178","Archival Storage;Disk;Energy-efficient;MAID","Ash;Databases;Disk drives;Nonvolatile memory;Random access memory;Reliability;Spinning","RAID;Unix;cache storage;data integrity;disc drives;fault tolerance;flash memories;information retrieval systems;memory architecture;meta data","3-disk failures;LoneStar Stack;POSIX file system interface;asynchronous data movers;cache;data integrity checking;data integrity recovery;disk based archival storage system;disk capacity;disk drives;disk-based LoneStar RAID;disk-based archival system architecture;elastic fault tolerance;end-to-end data integrity;energy efficiency;flash based storage;huge storage systems;metadata;parity disks;reliability;semantic data placement engine;single disk;user data storage;write once read sometimes workloads;write-offloading","","1","","29","","","6-8 Aug. 2014","","IEEE","IEEE Conference Publications"
"Identification of requirements for focused crawlers in technology intelligence","G. Schuh; A. Br√§kling; K. Apfel","Fraunhofer Institute for Production Technology IPT, Aachen, Germany","Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration","20141013","2014","","","2918","2923","The fast and high availability of knowledge is at first seen as a benefit for knowledge workers in the information age. On closer examination the outcome of this is a big challenge: The amount of data that is available these days has to be reasonably structured and conditioned. Only the US Library of Congress collected 235 terabyte of data on its own by April 2011. Technology intelligence as a fundamental component of technology management is expected to monitor these data, so technology managers are able to respond to new developments and trends just in time. Possible tools to meet this challenge in an efficient way are the focused crawlers. These are programs, which explore data collections independently to identify material related to the current working context. To implement such a tool, there exist a multitude of different approaches within the field of information retrieval, but they have to be used and combined on an individual basis to fit the requirements of a particular task. Hence, before a focused crawler can make the processes of technology intelligence more efficient, the dedicated requirements have to be identified. In this paper we develop a requirements model to close this gap.","2159-5100;21595100","Electronic:978-1-890843-29-8; POD:978-1-4799-5769-9; USB:978-1-890843-30-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921172","","Companies;Crawlers;Estimation;Indexes;Libraries;Monitoring","Internet;data handling;information retrieval;search engines","focused crawlers;fundamental component;information age;information retrieval;knowledge workers;technology intelligence;technology management;technology managers","","0","","21","","","27-31 July 2014","","IEEE","IEEE Conference Publications"
"Archives-holding XCS Classifier System: A preliminary study","T. Komine; M. Nakata; K. Takadama","Dept. of Informatics, The Univ. of Electro-Communications, Tokyo, JAPAN","2014 Sixth World Congress on Nature and Biologically Inspired Computing (NaBIC 2014)","20141016","2014","","","53","58","In dynamic environment, Learning Classifier System (LCS) evolves classifiers to fit the current situation, but may forget classifiers which were useful for previous situations. Our main idea is that, we store the forgotten classifiers as archives and generate new classifiers by recombining them to fit the current situation. Specifically, we propose an archive-based LCS called Arc-XCS, which detects environmental changes and generates classifiers based on the archive. The experimental results on the benchmark problem show that, Arc-XCS successfully stored good classifiers when each environmental changes occurs; compared to the conventional LCS (XCS), Arc-XCS reaches better performances with fewer trainings.","","Electronic:978-1-4799-5937-2; POD:978-1-4799-5938-9","10.1109/NaBIC.2014.6921853","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921853","dynamic environment;generalization;genetic algorithm;learning classifier system;reinforcement learning;single-step problem","Sociology;Statistics","information retrieval systems;learning (artificial intelligence);pattern classification","Arc-XCS;archive-based LCS;archives-holding XCS classifier system;dynamic environment;environmental changes;forgotten classifiers;learning classifier system","","0","","11","","","July 30 2014-Aug. 1 2014","","IEEE","IEEE Conference Publications"
"Hybrid Algorithm for Precise Recommendation from Almost Infinite Set of Websites","D. Deja; R. Nielek; X. Lin; A. Wierzbicki","Polish-Japanese Inst. of Inf. Technol., Warsaw, Poland","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","318","322","Typically, recommendation systems are used for increasing users' activity and spending. Most of them select a recommendation from a finite set of objects (e.g. News, products etc.) but in some cases the number of objects is so huge that standard approach based on similarity of objects will not work. The paper presents recommendation algorithms which work with an almost infinite set of websites by utilizing inherent characteristic of web pages and services delivered by search engines. Proposed algorithms were validated with the help of data from an Article Feedback Tool used on English Wikipedia for evaluating trustworthiness of articles. As the algorithms were developed to meet the special requirements of the Reconcile plug in, a community based websites' credibility evaluation system, performance of proposed solutions have been measured not only as precision and recall but also in terms of increasing inflow of credibility assessments for new websites. Gathered results show that is possible to create effective recommendation systems, which offer better scalability and lesser computational complexity than standard recommendation systems.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927559","Reconcile;Wikipedia;recommender systems","Electronic publishing;Encyclopedias;History;Internet;Search engines;Web sites","Web services;Web sites;information retrieval;recommender systems;search engines;trusted computing","English Wikipedia;Reconcile plug in;Web pages;Web services;Web sites infinite set;article feedback tool;credibility evaluation system;recommendation algorithm;search engine","","0","","17","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"An Interest-Aware Caching Scheme for Mobile Devices","B. N. Thyamagondlu; R. K. Wong; B. S. Vidyalakshmi; C. H. Chi","Univ. of New South Wales, Sydney, NSW, Australia","2014 IEEE International Conference on Mobile Services","20141016","2014","","","31","38","Mobile services are gaining importance in handling data on mobile devices. With heaps of information flowing in the network, a service for caching preferential posts will be helpful. Understanding the profile of an individual in a social network will help determine the relationship value with respect to other individuals in the network. Similarly a post in a social network can have contexts associated with it. If these contexts are a priority to the user, they can be one of the deciding factors for pre-fetching. In summary, these values called the profile similarity value and context priority values can be used to determine strengths between individuals and their preference. Getting a uniform bandwidth is always a challenge due to the mobility of the device. In this context pre-fetching of posts most related to the user can be beneficial in increasing the social network experience on mobile devices. The value addition is that the cached data apart from being personalised it can also be consumed later and helps when the individual moves to a place with no bandwidth or even a narrow bandwidth. This paper considers the personal similarity value as social network strength coupled with context priority of the post determined by the user, to evaluate what posts could be cached.","2329-6429;23296429","Electronic:978-1-4799-5060-7; POD:978-1-4799-5061-4","10.1109/MobServ.2014.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6924291","Interest Aware;Mobile Caching;Profile Similarity","Bandwidth;Context;Facebook;Mobile communication;Mobile handsets;Servers","cache storage;data handling;information retrieval;mobile computing;social networking (online)","caching preferential post;context prefetching;context priority;context priority value;data handling;information flow;interest aware caching scheme;mobile device;mobile services;personal similarity value;profile similarity value;social network strength","","0","","23","","","June 27 2014-July 2 2014","","IEEE","IEEE Conference Publications"
"Ranking tourist attractions using time series GPS data of cabs","J. Anu; R. Agrawal; S. Bhattacharya","Department of Computer Systems Technology, North Carolina A&T State University, Greensboro, USA","IEEE SOUTHEASTCON 2014","20141110","2014","","","1","6","Global Position System (GPS) is a widely used satellite-based navigation system. Popular use of these devices has produced large collections of data, some of which have been archived. These archived data sets and sometimes real time GPS data are now readily available over the internet and their analysis through computational methods can generate meaningful insights. These insights when applied appropriately can be used in everyday life. The purpose of this research is to make the case that automated analysis can provide insight that can otherwise be difficult to achieve due the large volume and noisy characteristics of GPS data. We present experiments that were performed on archived data which contains GPS traces of over 500 yellow cabs in the San Francisco Bay area. Using data analysis we determine the most visited tourist destinations within the San Francisco Bay area during the time period of the captured data.","1091-0050;10910050","Electronic:978-1-4799-6585-4; POD:978-1-4799-6586-1","10.1109/SECON.2014.6950659","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950659","","Frequency modulation;Geology;Global Positioning System;Image color analysis","Global Positioning System;automobiles;geographic information systems;information retrieval systems;time series","Global Position System;San Francisco Bay area;automated analysis;computational methods;data archiving;data capture;satellite-based navigation system;time period;time series GPS cab data;tourist attraction ranking;yellow cabs","","1","","11","","","13-16 March 2014","","IEEE","IEEE Conference Publications"
"Copy detection mechanism for documents using position based weighted scheme","R. Sharma; D. Sharma","Dept. of CEA, GLA University, Mathura, India","2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)","20141110","2014","","","521","526","Nowadays, every information is easily available in digital form as documents over the internet. So, it is very easy to copy and access those documents on the internet. When we copy someone's content or idea without permission or citation then plagiarism occurs. This is a very big problem, as it violates the sharing of important information among valid users. According to this paper, we developed a system to detect piracy or copies regarding documents using position based weighted scheme. This scheme firstly extracts all keywords from all the sentences and then applies weighting for each keyword. And, to cover the whole sentence, we used first and last position based scheme on all extracted keywords. Thereafter, assign the weight of all extracted keywords to related synonyms or antonyms of that extracted keyword with the help of Word Net dataset 2.0. Finally, we apply the cosine similarity measure for similarity detection between two documents by considering a static database. After performing experiments on Pan Plagiarism Corpus-9 datasets, the results represent better time efficiency and best performance in comparison of existing copy detection mechanisms on the basis of Precision, Recall, and F-measure.","","CD-ROM:978-1-4799-4237-4; Electronic:978-1-4799-4236-7; POD:978-1-4799-4235-0","10.1109/CONFLUENCE.2014.6949382","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949382","Copy detection mechanism;keyword extraction;similarity measure;weighted scheme","Databases;Feature extraction;Information technology;Next generation networking;Plagiarism;Pragmatics;Semantics","Internet;computer crime;copy protection;information retrieval;text analysis","F-measure;Internet;Pan Plagiarism Corpus-9 datasets;Word Net dataset 2.0;antonyms;copy detection mechanism;cosine similarity measure;digital form;documents access;documents copy;information sharing;keywords extraction;piracy detection;plagiarism;position based weighted scheme;sentences;similarity detection;static database;synonyms","","0","","17","","","25-26 Sept. 2014","","IEEE","IEEE Conference Publications"
"Users' reaction to network quality during web browsing on smartphones","H. Koto; N. Fukumoto; S. Niida; H. Yokota; S. Arakawa; M. Murata","Communications Network Planning Laboratory KDDI R&D Laboratories, Inc. 2-1-15 Ohara, Fujimino Saitama 356-8502, Japan","2014 26th International Teletraffic Congress (ITC)","20141023","2014","","","1","9","Understanding the Quality of Experience (QoE) of users is important for network and service providers. Many researches of QoE have been reported where subjective analysis, e.g. in the form of Mean Opinion Score (MOS), for audio/video applications and data services are performed. These studies mainly focus on how users feel while using such applications. In this paper, we focus on how users behave and react to communication network quality. We analyze the telecom traffic of smartphones passively monitored from 3G mobile network and show the effect of network quality on the users' web behavior. In particular, we investigate user behavior as a series of multiple actions in a flow of web browsing. We show that the observed results coincide with the MOS obtained through subjective analysis: The condition of network quality where the users' web behavior changes matches with the condition where the MOS starts to degrade. In addition, the obtained results illustrate that the users perform various reactive actions depending on the perceived network quality. Specifically, users generate more web traffic and transactions when the qualities of the network are both good and poor.","","Electronic:978-0-9883045-0-5; POD:978-1-4799-5599-2","10.1109/ITC.2014.6932954","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932954","QoE;mobile traffic;network quality;passive monitoring;web behavior","Correlation;Delays;Mobile computing;Monitoring;Quality of service;Smart phones;Throughput","3G mobile communication;Internet;human factors;information retrieval;mobile computing;quality of experience;smart phones;telecommunication traffic","3G mobile network;MOS;QoE;Web browsing;communication network quality;mean opinion score;quality of experience;smart phone telecom traffic;user reaction","","0","","24","","","9-11 Sept. 2014","","IEEE","IEEE Conference Publications"
"Expertise Finding in Bibliographic Network: Topic Dominance Learning Approach","M. Neshati; S. H. Hashemi; H. Beigy","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Cybernetics","20141113","2014","44","12","2646","2657","Expert finding problem in bibliographic networks has received increased interest in recent years. This problem concerns finding relevant researchers for a given topic. Motivated by the observation that rarely do all coauthors contribute to a paper equally, in this paper, we propose two discriminative methods for realizing leading authors contributing in a scientific publication. Specifically, we cast the problem of expert finding in a bibliographic network to find leading experts in a research group, which is easier to solve. We recognize three feature groups that can discriminate relevant experts from other authors of a document. Experimental results on a real dataset, and a synthetic one that is gathered from a Microsoft academic search engine, show that the proposed model significantly improves the performance of expert finding in terms of all common information retrieval evaluation metrics.","2168-2267;21682267","","10.1109/TCYB.2014.2312614","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837494","DBLP;expert finding;learning to rank;pairwise learning;pointwise learning","Bars;Communities;Cybernetics;Equations;Mathematical model;Search engines","bibliographic systems;information retrieval;learning (artificial intelligence);publishing;search engines","Microsoft academic search engine;bibliographic networks;discriminative methods;information retrieval evaluation metrics;scientific publication;topic dominance learning approach","","0","","37","","20140617","Dec. 2014","","IEEE","IEEE Journals & Magazines"
"Healthcare information exchange system based on a Hybrid central/federated model","K. Ghane","Anagira INC","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","1362","1365","The quality of care can be significantly enhanced and healthcare cost can be substantially reduced if healthcare providers can have access to patient records that are scattered among several paper and electronic based systems. Major challenges of Healthcare Information Exchange result from patient's medical records being kept in several healthcare provider offices, clinics and hospitals in different formats. There are two major problems with healthcare information retrieval. The first problem is lack of visibility and knowledge as to where patient's medical records reside. The second problem is lack of access to information and also incompatibility of data formats. A considerable coverage of Electronic Information Exchange among Electronic Health Record (EHR) systems remains to be implemented despite extensive standardization efforts toward providing solutions. The adoption pace of available standards and solutions has been slow with the exception of some public/government entities. This paper describes a comprehensive and practical solution based on a distributed system with independent subsystems that control and manage processes and data flow of information exchange. The Registrar Subsystem creates a directory of healthcare providers and patients. The Security Subsystem provides authentication and authorization services across all subsystems. The Locators address patient and medical location lookup. The Agents act on behalf of healthcare providers to communicate with other subsystems. The Mediators facilitate information retrieval. The Solution comprises of three levels of participation that allows healthcare providers to join the system easily by starting from basic semi-manual information exchange level and then migrating to a fully electronic and automated information exchange. The Solution is modeled based on variety of standards and protocols used in Internet and other application fields as well as healthcare specific standards and proposals.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6943852","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943852","","Authorization;Computer architecture;Information exchange;Medical services;Protocols;Standards","Internet;authorisation;electronic health records;health care;information retrieval","EHR systems;Internet;authentication services;authorization services;automated information exchange;data flow;distributed system;electronic based systems;electronic health record system;electronic information exchange;healthcare cost;healthcare information exchange system;healthcare information retrieval;healthcare providers;hybrid central-federated model;independent subsystems;locators;mediators;medical location lookup;patient medical records;protocols;registrar subsystem;security subsystem;semimanual information exchange level;solution modelling","","0","","23","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"Ontology Based Semantic Search: An Introduction and a Survey of Current Approaches","A. S. Ramkumar; B. Poorna","Bharathiar Univ., Coimbatore, India","2014 International Conference on Intelligent Computing Applications","20141124","2014","","","372","376","Ontology based semantic search will lead to new generation of search based on the meaning of keyword rather than keyword and helps in finding correct information on the web. Here, ontology provides an explicit specification of conceptualization which helps to connect the information on the existing web pages with the background knowledge. Ontology based search overcomes the semantic gap between the keyword found in documents and those in query. This survey provides an introduction to ontology based semantic search and review the different details of selected ontology based search approaches and compare them by means of classification criteria. Based on this comparison, this survey attempts to identify the possible directions for future research.","","Electronic:978-1-4799-3966-4; POD:978-1-4799-3967-1","10.1109/ICICA.2014.82","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965074","classification criteria;ontology;semantic search","Indexing;Ontologies;Resource description framework;Semantics;Syntactics;Web search","document handling;information retrieval;ontologies (artificial intelligence);semantic Web","Web pages;conceptualization specification;ontology based semantic search;semantic gap","","1","","23","","","6-7 March 2014","","IEEE","IEEE Conference Publications"
"E&VRobot: A crawler of education and vocation","Gang Wan; Yi Ding; Bing Li; Xiaohu Tan","School of Computer, Wuhan Vocational College of Software and Engineering, China","2014 9th International Conference on Computer Science & Education","20141016","2014","","","473","476","Recruitment website in China has become an important employment platform for university graduates. It allows the job seekers to register their information, search employment opportunities and help them finding the jobs they are satisfied with. At the same time, a lot of employers have released employment information through the recruitment website, including the release date, the name of the company, the company size, the working location, the hiring, the salary range, especially the job responsibilities, the job requirements and so on. The information obtained from the recruitment website may be used to create several applications in various fields. However, due to the growing number of employment information posted every day, as well as the dynamicity of the recruitment website, the task of extracting relevant information from the recruitment webs has become difficult and time consuming. Faced with this scenario, this paper proposes E&VRobot, an intelligent crawler of education and vocation for recruitment web. Finally, it presents a preliminary evaluation of the proposed crawler.","","Electronic:978-1-4799-2951-1; POD:978-1-4799-2952-8; USB:978-1-4799-2950-4","10.1109/ICCSE.2014.6926506","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926506","Recruitment website;content extraction;crawler","Adaptation models;Biological system modeling;Computers;Educational institutions;Employment;Indexes;Remuneration","Web sites;employment;information retrieval;recruitment","China;E&VRobot;education crawler;employment information;employment platform;recruitment Web site;search employment opportunities;university graduates;vocation crawler","","0","","17","","","22-24 Aug. 2014","","IEEE","IEEE Conference Publications"
"Improving Personalized Ranking in Recommender Systems with Multimodal Interactions","A. F. D. Costa; M. A. Domingues; S. O. Rezende; M. G. Manzato","Inst. of Math. & Comput. Sci., Univ. of Sao Paulo, Sao Carlos, Brazil","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","198","204","This paper proposes a conceptual framework which uses multimodal user feedback to generate a more accurate personalized ranking of items to the user. Our technique is a response to the actual scenario on the Web, where users can consume content following different interaction paradigms, such as rating, browsing, sharing, etc. We developed a post-processing step to ensemble rankings generated by unimodal-based state-of-art algorithms, using a set of heuristics which analyze the behavior of the user during consumption. We provide an experimental evaluation using the Movie Lens 10M dataset, and the results show that better recommendations can be provided when multimodal interactions are considered for profiling the preferences of the users.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927543","ensemble;framework;multimodal user feedback","Business process re-engineering;History;Motion pictures;Navigation;Prediction algorithms;Recommender systems;Vectors","Internet;behavioural sciences computing;information retrieval;recommender systems;user interfaces","Movie Lens 10M dataset;Web;browsing paradigm;multimodal interactions;multimodal user feedback;personalized ranking improvement;rating paradigm;recommender systems;sharing paradigm;unimodal-based state-of-art algorithms;user behavior analysis","","0","","21","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"An improved algorithm for relation extraction based on tri-training","Z. Zhong; F. Liu; Y. Wu; N. Jing","College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China 410073","2014 International Conference on Information Science, Electronics and Electrical Engineering","20141106","2014","2","","11078","11081","The tri-training algorithm is an efficient co-training method for semi-supervised learning, and it has been used to extract semantic relation between entities in text. However, the tri-training method will introduce noises and lose some valuable samples while expanding the training set. In this paper, we propose an improved algorithm based on tri-training. New voting mechanism and active learning method are introduced into the improved algorithm to solve the problems of traditional tri-training algorithm. Experiments demonstrate that the performance of the improved algorithm is superior to the existing tri-training algorithms.","","CD-ROM:978-1-4799-3195-8; Electronic:978-1-4799-3197-2; POD:978-1-4799-3198-9","10.1109/InfoSEEE.2014.6947835","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6947835","","Accuracy;Algorithm design and analysis;Noise;Prediction algorithms;Predictive models;Semisupervised learning;Training","information retrieval;learning (artificial intelligence);text analysis","active learning method;co-training method;improved algorithm;semantic relation extraction;semisupervised learning;text entity;tri-training algorithm;voting mechanism","","0","","8","","","26-28 April 2014","","IEEE","IEEE Conference Publications"
"An Eccentric Approach for Paraphrase Detection Using Semantic Matching and Support Vector Machine","P. Vigneshvaran; E. Jayabalan; A. V. Kathiravan","Dept. of Comput. Sci., Gov. Arts Coll., Salem, India","2014 International Conference on Intelligent Computing Applications","20141124","2014","","","431","434","Paraphrase is the process of writing a sentence in another form. It relates to computing the similarity between sentences which are not lexicographically similar. This paper proposed an efficient method to estimate paraphrase. Specifically this paper defines various word co-occurrence in the sentence measured and its synonyms are also identified using web. By using the algorithm, similarity has been measured. This research focuses on evaluating the paraphrase between the sentences. In this paper, an approach for detecting paraphrase is tested by counting tokens. It processes token matching with the help of synonym of tokens, and produces the result with SVM classifier using Plagiarism detection corpus (PAN). Thus the SVM produces the output as +1 if the given text is paraphrased, - 1 if the given text is not paraphrased. This approach may also be used to identify Plagiarism of documents and to eliminate duplicates in a text repository.","","Electronic:978-1-4799-3966-4; POD:978-1-4799-3967-1","10.1109/ICICA.2014.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965086","POS tagging;Paraphrase;Plagiarism Detection Corpus (PAN);Semantic Matching;Support Vector Machine (SVM);Tokenization","Measurement;Object oriented programming;Plagiarism;Semantics;Support vector machines;Tagging","information retrieval;natural language processing;pattern matching;security of data;support vector machines;text analysis","SVM classifier;paraphrase detection;plagiarism detection corpus;semantic matching;support vector machine;text repository;token matching","","0","","12","","","6-7 March 2014","","IEEE","IEEE Conference Publications"
"Evaluation of the performance of open-source RDBMS and triplestores for storing medical data over a web service","V. Kilintzis; N. Beredimas; I. Chouvarda","Laboratory of Medical Informatics, Medical School, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","4499","4502","An integral part of a system that manages medical data is the persistent storage engine. For almost twenty five years Relational Database Management Systems(RDBMS) were considered the obvious decision, yet today new technologies have emerged that require our attention as possible alternatives. Triplestores store information in terms of RDF triples without necessarily binding to a specific predefined structural model. In this paper we present an attempt to compare the performance of Apache JENA-Fuseki and the Virtuoso Universal Server 6 triplestores with that of MySQL 5.6 RDBMS for storing and retrieving medical information that it is communicated as RDF/XML ontology instances over a RESTful web service. The results show that the performance, calculated as average time of storing and retrieving instances, is significantly better using Virtuoso Server while MySQL performed better than Fuseki.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6944623","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944623","","Benchmark testing;Ontologies;Resource description framework;Servers;Time factors;Web services;XML","Web services;information retrieval;medical information systems;ontologies (artificial intelligence);relational databases","Apache JENA-Fuseki triplestore;MySQL 5.6 RDBMS;RDF-XML ontology instances;RESTful Web service;Virtuoso Universal Server 6 triplestore;extensible markup language;medical data storage;medical information retrieval;medical information storage;open-source RDBMS;relational database management system;resource description framework","","2","","15","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"Understanding and Classifying the Quality of Technical Forum Questions","L. Ponzanelli; A. Mocci; A. Bacchelli; M. Lanza","REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","2014 14th International Conference on Quality Software","20141120","2014","","","343","352","Technical questions and answers (Q&A) services have become a valuable resource for developers. A prominent example of technical Q&A website is StackOverflow (SO), which relies on a growing community of more than two millions of users who actively contribute by asking questions and providing answers. To maintain the value of this resource, poor quality questions - among the more than 6,000 asked daily - have to be filtered out. Currently, poor quality questions are manually identified and reviewed by selected users in SO, this costs considerable time and effort. Automating the process would save time and unload the review queue, improving the efficiency of SO as a resource for developers. We present an approach to automate the classification of questions according to their quality. We present an empirical study that investigates how to model and predict the quality of a question by considering as features both the contents of a post (e.g., from simple textual features to more complex readability metrics) and community-related aspects (e.g., popularity of a user in the community). Our findings show that there is indeed the possibility of at least a partial automation of the costly SO review process.","1550-6002;15506002","Electronic:978-1-4799-7198-5; POD:978-1-4799-7199-2","10.1109/QSIC.2014.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958423","Q&amp;A;Quality;StackOverflow;Technical Forum","Communities;Decision trees;Entropy;Feature extraction;Indexes;Measurement;Predictive models","Web sites;question answering (information retrieval)","Q & A Web site;SO;StackOverflow;questions and answers services;technical forum questions","","9","","24","","","2-3 Oct. 2014","","IEEE","IEEE Conference Publications"
"Personalized recommendation based on link prediction in dynamic super-networks","W. Hong; S. Yanshen; Y. Xiaomei","Institute of Information Science and Engineering, Shandong Normal University, Jinan China 250014","Fifth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20141120","2014","","","1","7","Personalized recommendation is one of the most effective methods to solve the problem of information overloading. As many real existing systems in nature, a recommendation system can also be considered as a complex network system, so we can do personalized recommendation by using the link prediction method which is a new one in complex networks research area. In this paper, we present personalized recommendation method based on the link prediction in Super-networks. Firstly, we give several definitions such as a Super-network, a dynamic Super-network and a utility matrix etc. Secondly, we construct a personalized recommendation model based on these definitions. Thirdly, we define a similarity metric for users and some similarity criteria, put forward five link prediction related algorithms in dynamic Supernetworks and present our recommendation algorithms based on these link prediction algorithms. Finally, we apply our methods to classic datasets in order to evaluate the performance of our algorithms.","","CD-ROM:978-1-4799-2695-4; Electronic:978-1-4799-2696-1; POD:978-1-4799-2697-8","10.1109/ICCCNT.2014.6963067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963067","complex networks;dynamic Super-networks;link prediction;prediction models;recommendation systems","Algorithm design and analysis;Complex networks;Heuristic algorithms;Internet;Prediction algorithms;Prediction methods;Social network services","information retrieval;matrix algebra;recommender systems","complex network system;dynamic super-networks;information overloading;link prediction;personalized recommendation;similarity metric;utility matrix","","0","","29","","","11-13 July 2014","","IEEE","IEEE Conference Publications"
"coreSNP: Parallel Processing of Microarray Data","P. H. Guzzi; G. Agapito; M. Cannataro","Department of Medical and Surgical Sciences, Magna Graecia University of Catanzaro, Italy","IEEE Transactions on Computers","20141105","2014","63","12","2961","2974","The availability of high-throughput technologies, such as next generation sequencing and microarray, and the diffusion of genomics studies to large populations are producing an increasing amount of experimental data. In particular, pharmacogenomics studies the impact of genetic variation on drug response in patients and correlates gene expression or single nucleotide polymorphisms (SNPs) with the toxicity or efficacy of a drug, with the aim to improve drug therapy with respect to the patients' genotype ensuring maximum efficacy with minimal adverse effects. However, the storage, preprocessing, and analysis of experimental data are becoming a main bottleneck in the pharmacogenomics analysis pipeline, due to the increasing number of genes and patients investigated. This paper presents a new parallel software tool named coreSNP for the parallel preprocessing and statistical analysis of DMET (Drug Metabolism Enzymes and Transporters) SNP microarray data produced by Affymetrix for pharmacogenomics studies. The scalable multi-threaded implementation of coreSNP allows to handle the huge volumes of experimental pharmacogenomics data in a very efficient way, while its easy to use graphical user interface and its ability to annotate significant SNPs allow biologists to interpret the results easily. Performance evaluation conducted using real datasets shows good speed-up and scalability and effective response times.","0018-9340;00189340","","10.1109/TC.2013.176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6587035","Bioinformatics (genome or protein) databases;distributed programming;distributed systems;health care;healthcare;medical information systems;statistical software","Bioinformatics;DNA;Drugs;Genomics;Parallel processing;Statistical analysis;Throughput","bioinformatics;drugs;enzymes;genetics;genomics;graphical user interfaces;health care;information retrieval;lab-on-a-chip;multi-threading;statistical analysis","Affymetrix;DMET SNP microarray data;Drug Metabolism Enzymes and Transporters;SNP annotation;coreSNP parallel software tool;drug response;drug therapy improvement;drug toxicity;experimental data analysis;experimental data preprocessing;experimental data storage;gene expression;genetic variation;genomics diffusion;graphical user interface;high-throughput technologies;maximum drug efficacy;microarray data;minimal adverse effects;next generation sequencing;parallel processing;patient genotype;performance evaluation;pharmacogenomics analysis pipeline;response times;scalable multithreaded implementation;single-nucleotide polymorphisms;statistical analysis","","2","","20","","20130826","Dec. 2014","","IEEE","IEEE Journals & Magazines"
"Software model to MEMS data access based upon MongoDB","I. Vasyliuk; V. Teslyuk","CAD System Lviv Politechnic National University Lviv, Ukraine","2014 XIXth International Seminar/Workshop on Direct and Inverse Problems of Electromagnetic and Acoustic Wave Theory (DIPED)","20141120","2014","","","183","186","In the paper it is described the program model that enables to access and retrieve the data about microelectromechanical system component structure, the designed solution of microsystems etc.","2165-3585;21653585","Electronic:978-1-4799-6214-3; POD:978-1-4799-6215-0","10.1109/DIPED.2014.6958364","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958364","MEMS ontology;data model;database (DB);ontology model","Data models;Databases;Electromagnetic scattering;Electromagnetics;Micromechanical devices;Solid modeling;Unified modeling language","CAD;database management systems;electronic engineering computing;information retrieval;micromechanical devices","MEMS data access;MongoDB;data retrieval;microelectromechanical system component structure;microsystems;program model;software model","","1","","7","","","22-25 Sept. 2014","","IEEE","IEEE Conference Publications"
"Empirical study on overlapping community detection in question and answer sites","Z. Meng; F. Gandon; C. F. Zucker; G. Song","INRIA Sophia Antipolis M&#x00E9;diterran&#x00E9;e, 06900, France","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","344","348","In many social networks, people interact based on their interests. Community detection algorithms are then useful to reveal the sub-structures of a network and help us find interest groups. Identifying these social communities can bring benefit to understanding and predicting users behaviors. However, for some kind of online community sites such as question-and-answer (Q&A) sites or forums, there is no friendship based social network structure, which means people are not aware who they are in contact with. Therefore, many traditional community detection techniques do not apply directly. In this paper, we propose an empirical approach for extracting data from Q&A sites suitable to apply community detection methods. Then we compare three kinds of community detection methods we applied on a dataset extracted from the popular Q&A site StackOverflow. We analyze and comment the results of each method.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921608","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921608","","Cascading style sheets;Clustering algorithms;Communities;HTML;Layout;Social network services;Vegetation","information retrieval;social networking (online)","Q&A sites;community detection algorithms;online community sites;overlapping community detection;question and answer sites;social communities;social network structure","","1","","12","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"EDSRPPC: An efficient data storage and retrieval through personalization and prediction in cloud","P. Varalakshmi; M. Thangavel; K. Nithya; T. Priya; D. Sakthya","Department of Information Technology, MIT Campus, Anna University, Chromepet, Chennai - 600044, Tamilnadu, India","2013 Fifth International Conference on Advanced Computing (ICoAC)","20141016","2013","","","413","418","In cloud computing, remote based massive and scalable data storage services are provided to the users. Data storage and retrieval services need to be modernized for secure communication in the cloud. So, in this paper, user centric personalization and prediction based searching techniques have been proposed to improve the data storage and retrieval services in cloud. Personalization is the concept of categorizing the data based on the user needs and prediction, where the relevant files needed by a user are retrieved based on the user requirement. Personalization is achieved by categorizing the user's files storage. The middleware is used as a support to the data owner, to perform all the necessary encryption and hashing required for secure storage. The data in the cloud server is partitioned into clusters based on the category and similarity. When a user requests for a file, the middleware receives a set of similar files from the cloud server from which the most relevant file is predicted and returned to the user based on the search log and access rights of the user by boosting and bagging techniques. The performance of the EDSRPPC system is measured based on the rate of relevancy of the returned files and also the time required for retrieval of the files from the cloud storage. The experimental result proves that EDSRPPC system as an efficient personalization and prediction framework for data storage and retrieval.","2377-6927;23776927","CD-ROM:978-1-4799-3447-8; Electronic:978-1-4799-3448-5; POD:978-1-4799-3449-2","10.1109/ICoAC.2013.6921986","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921986","Bagging;Boosting;Cloud computing;Personalization;Prediction","Barium;Cryptography;Servers","cloud computing;cryptography;information retrieval;middleware;network servers;telecommunication security;user centred design","EDSRPPC system;bagging techniques;boosting techniques;cloud computing;cloud personalization;cloud server;data storage and retrieval;file retrieval;middleware;necessary encryption;prediction based searching techniques;remote based massive and scalable data storage services;secure communication;user centric personalization;user file storage;user needs;user prediction","","0","","22","","","18-20 Dec. 2013","","IEEE","IEEE Conference Publications"
"Hierarchical and binary spatial descriptors for lung nodule image retrieval","G. Ng; Y. Song; W. Cai; Y. Zhou; S. Liu; D. Dagan Feng","Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","6463","6466","With the increasing amount of image data available for cancer staging and diagnosis, it is clear that content-based image retrieval techniques are becoming more important to assist physicians in making diagnoses and tracking disease. Domain-specific feature descriptors have been previously shown to be effective in the retrieval of lung tumors. This work proposes a method to improve the rotation invariance of the hierarchical spatial descriptor, as well as presents a new binary descriptor for the retrieval of lung nodule images. The descriptors were evaluated on the ELCAP public access database, exhibiting good performance overall.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6945108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945108","","Biomedical imaging;Educational institutions;Feature extraction;Image retrieval;Lungs;Robustness;Support vector machines","binary codes;cancer;data mining;feature extraction;hierarchical systems;image classification;information retrieval;lung;medical image processing;medical information systems;tumours;visual databases","ELCAP public access database;binary spatial descriptor;cancer diagnosis;cancer staging;content-based image retrieval techniques;descriptor evaluation;disease tracking;domain-specific feature descriptor;hierarchical spatial descriptor;image data;lung nodule image retrieval;lung tumor retrieval;rotation invariance","","2","","17","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"Singer identification using clustering algorithm","D. Dharini; A. Revathy","Saranathan College of engineering, Venkateswara Nagar, Panjappur, Thiruchirappalli, 620012, India, Department of ECE","2014 International Conference on Communication and Signal Processing","20141110","2014","","","1927","1931","The main objective of this paper is to discuss the effectiveness of features and clustering algorithm to evaluate the performance of the singer identification system. The goal of singer identification is to identify the singer independent of training data. The training and testing phase are done for direct film song (vocal with background) for 10 singers. In training phase 15 film songs of a singer is taken as input. The input songs are made to undergo a set of pre-processing steps. The three stages of preprocessing are pre-emphasis, frame blocking and windowing. The Perceptual Linear Prediction (PLP) features are extracted from each frames of pre-processed signal. The singer model is developed by K-means clustering algorithm for each singer. In clustering method, the cluster centroids are obtained for cluster size of 256 and stored. One model is created for each singer by performing training and testing on the songs considered directly. Mean of minimum distances is computed for each model. Singer is classified based on selection of the model which produces minimum of average. The singer information is main factor in organizing and exploring music data. Singer identification also extends its application in music indexing and retrieval.","","CD-ROM:978-1-4799-3356-3; Electronic:978-1-4799-3358-7; POD:978-1-4799-3359-4","10.1109/ICCSP.2014.6950180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950180","Clustering Algorithm;PLP;Singer identification","Clustering algorithms;Indexes","cepstral analysis;feature extraction;indexing;information retrieval;speaker recognition","K-means clustering algorithm;PLP;cluster centroids;feature extraction;frame blocking;music indexing;music retrieval;perceptual linear prediction;singer identification;singer model;training data","","0","","8","","","3-5 April 2014","","IEEE","IEEE Conference Publications"
"Detection of table structure and content extraction from scanned documents","S. Deivalakshmi; K. Chaitanya; P. Palanisamy","Department of Electronics and Communication Engineering, National Institute of Technology, Trichy- 620015, India","2014 International Conference on Communication and Signal Processing","20141110","2014","","","270","274","Tables are one of the efficient information conveying methods used now days in larger extent. This paper report a fast, language independent (English and Tamil), skilled technique for table structure detection and its content extraction from a scanned document image based on morphological operation, connected components and labeling. From the conducted exhaustive experimentation, it is observed that the proposed method is the fastest approach because of its simple operations. In addition with that it is noticed that it does not lead to any kind of degradation in the extracted table content since after detecting contents location it is retrieved from the original image. More over it is also very interesting to note that the presented approach works well for documents with different font's size and font styles.","","CD-ROM:978-1-4799-3356-3; Electronic:978-1-4799-3358-7; POD:978-1-4799-3359-4","10.1109/ICCSP.2014.6949843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949843","connected components;labeling;morphological operation;scanned document image;table detection and content extraction","Companies;Context;Labeling;Lead;Morphology","document image processing;information retrieval","content extraction;information conveying method;morphological operation;scanned document image;table structure detection","","0","","22","","","3-5 April 2014","","IEEE","IEEE Conference Publications"
"Semantic orientation approach for sentiment classification","S. J. Veeraselvi; C. Saranya","Computer Science and Engineering, Kalaignar Karunanidhi Institute of Technology, Coimbatore, India","2014 International Conference on Green Computing Communication and Electrical Engineering (ICGCCEE)","20141016","2014","","","1","6","Opinions are the fundamental aspect to almost all decision making activities. The increased usage of internet and the exchange of user opinions through social media and public forums on the web has become the motivation for sentiment analysis. Due to the infinite amount of user opinions available throughout the web it is necessary to automatically analyze and classify sentiment expressed in opinions. The basic task of sentiment analysis or opinion mining is sentiment classification which classifies the content as positive, negative and irrelevant. This paper discusses an approach where an exposed stream of tweets from the Twitter micro blogging site are preprocessed and classified based on their sentiments. In sentiment classification system the concept of opinion subjectivity has been accounted. In this paper, we present opinion detection and organization subsystem, which have already been integrated into our larger question-answering system. The subjectivity classification system uses Genetic-Based Machine Learning (GBML) technique that considers subjectivity as a semantic problem. The classification of a review is predicted through the average semantic orientation of the phrases in the review that contain adjectives or adverbs. Experimental results of the proposed techniques are efficient and generate eminent evaluations.","","Electronic:978-1-4799-4982-3; POD:978-1-4799-4981-6","10.1109/ICGCCEE.2014.6921417","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921417","Genetic algorithm;Semantic orientation;Sentiment Classification;Sentiment analysis","Blogs;Feature extraction;Semantics;Sentiment analysis;Sociology;Speech;Twitter","Internet;data mining;decision making;learning (artificial intelligence);question answering (information retrieval);social networking (online);user interfaces","GBML technique;Internet;Twitter;World Wide Web;decision making;genetic-based machine learning;micro blogging site;opinion mining;public forums;question-answering system;semantic orientation;sentiment analysis;sentiment classification;social media;user opinions","","2","","17","","","6-8 March 2014","","IEEE","IEEE Conference Publications"
"A survey of intellectual property rights literature from 1971 to 2012: The main path analysis","L. Y. Y. Lu; J. S. Liu","Yuan Ze University, Chung-Li, Taiwan","Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration","20141013","2014","","","1274","1280","Intellectual property rights (IPRs) protection is strategically crucial for multinational corporations which are heavily conducting technology innovation to keep their technological superiority, competitiveness, and return of innovation investment. Developed countries are concerned that unequal protection of IPRs may result in a significant loss through unauthorized imitation in other countries while developing countries think that stronger IPRs would increase the costs of technology acquirement and raise the price of consumer products. The differences in IPR policies have led to significant disputes in international trade. Over the past four decades many researchers have investigated various issues regarding the protection of intellectual property rights. This paper adopts a unique approach to review the development trajectories of IPR research over the past four decades. We use ISI web of science (WOS) as the data source to retrieve the related literature and their citation data, and then apply the main path analysis on the citation data to identify the key-route main paths of the citation-based network. A total of 3184 papers and their citation data were retrieved and analyzed. The key-route main path discloses three different focuses on the research of IPRs - the scope of the patents, the preferences of the North and the South, and patent reform.","2159-5100;21595100","Electronic:978-1-890843-29-8; POD:978-1-4799-5769-9; USB:978-1-890843-30-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921126","","Economics;Indexes;Investment;Patents;Technological innovation;Trajectory","Internet;citation analysis;data analysis;industrial property;information retrieval;innovation management;international trade;investment;patents;technology management","IPR policies;ISI Web of science;WOS;citation data analysis;citation data retrieval;consumer products;data source;intellectual property rights protection;international trade;multinational corporations;patent reform;return of innovation investment;technological competitiveness;technological superiority;technology acquirement;technology innovation;unauthorized imitation","","0","","39","","","27-31 July 2014","","IEEE","IEEE Conference Publications"
"Question difficulty evaluation by knowledge gap analysis in Question Answer communities","C. L. Lin; Y. L. Chen; H. Y. Kao","Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","336","339","The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares knowledge among people. There are thousands of questions that are posted and solved every day. Because of the various users of the CQA service, question search and ranking are the most important topics of research in the CQA portal. In this study, we addressed the problem of identifying questions as being hard or easy by means of a probability model. In addition, we observed the phenomenon called knowledge gap that is related to the habit of users and used a knowledge gap diagram to illustrate how much of a knowledge gap exists in different categories. To this end, we proposed an approach called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines the user-user network and the architecture of the CQA service to find hard questions. We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate the experimental results. Our results show that our approach leads to better performance than other baseline approaches across all evaluation metrics.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921606","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921606","CQA portal;Difficulty;Expert finding;Knowledge gap;Link analysis;Social network","Communities;Conferences;Mathematical model;Measurement;Portals;Social network services;Web sites","Internet;diagrams;portals;probability;question answering (information retrieval)","AUC;CQA portal;CQA service;KG-DRank algorithm;MAP;NDCG;Web 2.0;community question answer service;concordance analysis;f-measure;knowledge gap analysis;knowledge gap diagram;knowledge-gap-based difficulty rank algorithm;probability model;question difficulty evaluation;question ranking;question search;user-user network","","1","","17","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Extracting Attributes and Synonymous Attributes from Online Encyclopedias","Q. Liu; D. Wu; Y. Liu; X. Cheng","Univ. of Chinese Acad. of Sci., Beijing, China","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","290","296","In this paper, we present an approach that extracts attributes of open-domain named entities for the Chinese language. The approach contains two steps. The first step consists in an unsupervised technique which captures high frequency attributes from online encyclopedias. The second step discovers uncommon attributes with low frequency. Lastly, an integrated framework is proposed to obtain attributes and their synonymous attributes simultaneously. Experimental results show that the proposed approach boosts the coverage of extracted attributes without losing the precision.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927555","attribute extraction;entity attribute;named entity;synonymous attribute","Context;Data mining;Educational institutions;Encyclopedias;Mobile handsets;Semantics;Vectors","Internet;encyclopaedias;information retrieval;natural language processing","Chinese language;high frequency attributes;information extraction;online encyclopedias;open-domain named entity attribute extraction;synonymous attribute extraction;uncommon attribute discovery;unsupervised technique","","0","","35","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"iHANDs: Intelligent Health Advising and Decision-Support Agent","B. Hannan; X. Zhang; K. Sethares","Comput. & Inf. Sci. Dept., Univ. of Massachusetts at Dartmouth, Dartmouth, MA, USA","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","3","","294","301","This paper describes an intelligent health and decision support agent (iHANDs) built with various artificial intelligence mechanisms. Upon receiving the user's symptom descriptions, iHands conducts both web search and local medical knowledge database search, utilizing the user's electronic health records (EHR) to direct the search process. An information-fusion algorithm is developed based on Dempster-Shafer theory to merge the information from various sources with different reliabilities and generate the strength of support for each possible cause. A dynamic reference network is created and updated to record all information obtained during the interleaved search and reasoning process. Ihands performs a bi-directional search: from symptoms to possible causes and also from possible causes to most likely symptoms and risk factors. Bayesian inference mechanism is used to identify the confidence level for each possible cause given the user's symptoms and EHR. When needed, an iterative broaden search will be conducted to increase the confidence level to exceed a pre-set threshold or to further distinguish a few possible causes with very close confidence levels. The preliminary experiment results show the promise of iHands in assisting individuals in their healthcare decision-making process.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928198","decision support;health informatics;intelligent agent;reasoning under uncertainty;web knowledge mining","Cognition;Electronic medical records;Knowledge engineering;Medical diagnostic imaging;Medical services;Radio frequency;Reliability","Internet;belief networks;bioinformatics;data mining;decision support systems;electronic health records;health care;inference mechanisms;information retrieval;iterative methods;multi-agent systems","Bayesian inference mechanism;Dempster-Shafer theory;EHR;Internet;Web knowledge mining;Web search;artificial intelligence mechanisms;bidirectional search;confidence level;dynamic reference network;electronic health records;health informatics;iHANDs;information-fusion algorithm;intelligent health advising-and-decision-support agent;interleaved search process;iterative broaden search;local medical knowledge database search;reasoning process;risk factors","","1","","14","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Don‚Äôt Classify Ratings of Affect; Rank Them!","H. P. Mart√≠nez; G. N. Yannakakis; J. Hallam","Institute of Digital Games, University of Malta, Msida, Malta","IEEE Transactions on Affective Computing","20141022","2014","5","3","314","326","How should affect be appropriately annotated and how should machine learning best be employed to map manifestations of affect to affect annotations? What is the use of ratings of affect for the study of affective computing and how should we treat them? These are the key questions this paper attempts to address by investigating the impact of dissimilar representations of annotated affect on the efficacy of affect modelling. In particular, we compare several different binary-class and pairwise preference representations for automatically learning from ratings of affect. The representations are compared and tested on three datasets: one synthetic dataset (testing ‚Äúin vitro ‚Äù) and two affective datasets (testing ‚Äúin vivo‚Äù). The synthetic dataset couples a number of attributes with generated rating values. The two affective datasets contain physiological and contextual user attributes, and speech attributes, respectively; these attributes are coupled with ratings of various affective and cognitive states. The main results of the paper suggest that ratings (when used) should be naturally transformed to ordinal (ranked) representations for obtaining more reliable and generalisable models of affect. The findings of this paper have a direct impact on affect annotation and modelling research but, most importantly, challenge the traditional state-of-practice in affective computing and psychometrics at large.","1949-3045;19493045","","10.1109/TAFFC.2014.2352268","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6883166","Affect annotation;affect modelling;classification;computer games;preference learning;ranks;ratings;sensitive artificial listener (SAL) corpus","Affective computing;Computational modeling;Data models;Numerical models;Predictive models;Training;Transforms","cognition;information retrieval;learning (artificial intelligence);physiology;psychology","affect annotation;affective computing;affective datasets;affective states;binary class representation;cognitive states;contextual user attributes;machine learning;pairwise preference representation;physiological user attributes;preference learning;psychometrics;rank-based transformations;rating annotations;speech attributes;synthetic dataset","","12","","45","","20140826","July-Sept. 1 2014","","IEEE","IEEE Journals & Magazines"
"Knowledge representation of remote sensing quantitative retrieval models","J. Zhang; Y. Xue; J. Dong; J. Liu; L. Liu; S. Siva; J. Guang","Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing 100094, China","2014 IEEE Geoscience and Remote Sensing Symposium","20141106","2014","","","4504","4507","A large number of quantitative retrieval models have been proposed in recent years, and there is continuous momentum in proposing new ones. Building a model, from design through to implementation stages, involves a process of knowledge collection, organization and transmission. In this paper we introduce the SECI model to manage the conversion of qualitative remote sensing knowledge and propose a mode of knowledge representation on the basis of the ontology for geospatial modeling. We develop a platform based on the above research and demonstrate the efficiency of the knowledge representation mode using this platform.","2153-6996;21536996","Electronic:978-1-4799-5775-0; POD:978-1-4799-5314-1; USB:978-1-4799-5774-3","10.1109/IGARSS.2014.6947493","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6947493","Ontology;Remote Sensing;explicit knowledge;quantitative retrieval models;tacit knowledge;workflow","Aerosols;Computational modeling;Data models;Geospatial analysis;Ontologies;Remote sensing","geographic information systems;geophysics computing;information retrieval;knowledge acquisition;knowledge management;knowledge representation;ontologies (artificial intelligence);remote sensing","SECI model;continuous momentum;geospatial modeling;knowledge collection process;knowledge organization;knowledge representation mode;knowledge transmission;ontology;qualitative remote sensing knowledge management;remote sensing quantitative retrieval model","","0","","13","","","13-18 July 2014","","IEEE","IEEE Conference Publications"
"Size estimation in the hidden database with form-like interface: A survey","H. Yan; Z. Gong; T. Huang","University of Science and Technology of China, Hefei, China, University of Macau, Macau","2014 IEEE International Conference on Information and Automation (ICIA)","20141023","2014","","","388","393","Hidden databases which can only be accessed through the restrictive web interface have attracted much attention in recent years. This paper presents a survey on random drill-down sampling for aggregate estimation, especial size estimation, of a hidden database with form-like drop-down list interface. Various approaches are proposed for the purpose of getting an accurate size estimation efficiently.","","Electronic:978-1-4799-4100-1; POD:978-1-4799-4099-8; USB:978-1-4799-4101-8","10.1109/ICInfA.2014.6932687","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932687","Aggregate Estimation;Deep Web;Hidden Database;Random Drill-Down Sampling","Accuracy;Aggregates;Databases;Educational institutions;Estimation;History;Size measurement","Internet;database management systems;information retrieval;user interfaces","aggregate estimation;especial size estimation;form-like drop-down list interface;form-like interface;hidden database;hidden databases;random drill-down sampling;restrictive Web interface","","0","","19","","","28-30 July 2014","","IEEE","IEEE Conference Publications"
"OPLITOP: A localized broacast media","T. Jetiyanuwat; T. Kositwutisopon; P. Tanthawatkul; W. Viriyasitavat","Faculty of Information and Communication Technology, Mahidol University Bangkok, Thailand","2014 Third ICT International Student Project Conference (ICT-ISPC)","20141016","2014","","","151","154","Nowadays, social media is an important thing in people's daily lives because it has become the communication means to convey information such as news, events, and advertisements. However, such the social media only delivers and feeds information to the users based on his or her social network, closed-friends, or predefined preferences but not by their location and time. With overwhelming amount of information available in social media today, it becomes also impossible to search for information that is time and location-relevant. Therefore, we see a need to create a mobile application that can provide users interesting and relevant information based on user's preferences, location and time and thus develops a mobile application, namely OPLITOP. We hope that the developed application could serve as a localized broadcast media that can notify users of critical and dangerous situations that may be relevant to the users. OPLITOP is first developed on the Android operating system and it uses several software tools such as Google maps, phpMyAdmin web server and Eclipse for implementation. Moreover, in addition to allowing users to manual input information, we also implement data crawler techniques that can automatically fetch data from publicly available databases (e.g. official websites, calendars and possibly Facebook events).","","Electronic:978-1-4799-5573-2; POD:978-1-4799-5574-9","10.1109/ICT-ISPC.2014.6923239","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923239","communication means;event notification;localized broadcast;mobile application;relevant information;social media","Global Positioning System;Manuals","Android (operating system);data mining;information retrieval;mobile computing;social networking (online)","Android operating system;Eclipse;Facebook events;Google maps;OPLITOP;advertisements;automatic data fetching;calendars;critical situations;dangerous situations;data crawler technique;events information;information search;interesting information;localized broadcast media;location-relevant information;manual information input;mobile application;news information;official Websites;phpMyAdmin Web server;publicly available database;social media;social network;software tools;time-relevant information;user location;user preference","","0","","9","","","26-27 March 2014","","IEEE","IEEE Conference Publications"
"The research on the automatic generation of micro-blog user tags based on clustering analysis","L. Haiyan; C. Xiaowei; R. Ying","Naval Aeronautical and Astronautical University, Yantai, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","20141023","2014","","","633","636","The main research is the automatic generation of micro-blog user tags based on cluster analysis. Key technologies used in this paper are introduced firstly; mainly include cluster technology and TextRank. A Baseline system is proposed in order to show the validity of the research proposed by this paper. Then the automatic generation method based on clustering analysis is illustrated detailedly. Finally analyze and evaluate the method by experiments. The experimental results show that the user tags generated by the method can solve the problem of synonymy tags stack, and the tags can reflect the users' interest in more dimensions.","2327-0586;23270586","CD-ROM:978-1-4799-3277-1; Electronic:978-1-4799-3279-5; POD:978-1-4799-3280-1","10.1109/ICSESS.2014.6933648","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933648","Cluster Analysis;Tags for Micro-blog Users;TextRank","Blogs;Clustering algorithms;Clustering methods;Educational institutions;Mutual information;Semantics;Tagging","Web sites;information retrieval;pattern clustering;text analysis","TextRank;automatic generation method;baseline system;cluster technology;clustering analysis;microblog user tags;synonymy tags stack;user interest","","0","","11","","","27-29 June 2014","","IEEE","IEEE Conference Publications"
"Obtaining Technology Insights from Large and Heterogeneous Document Collections","L. Dey; D. Mahajan; H. Gupta","Innovation Labs., Delhi Tata Consultancy Services, New Delhi, India","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","102","109","Keeping up with rapid advances in research in various fields of Engineering and Technology is a challenging task. Decision makers including academics, program managers, venture capital investors, industry leaders and funding agencies not only need to be abreast of latest developments but also be able to assess the effect of growth in certain areas on their core business. Though analyst agencies like Gartner, McKinsey etc. Provide such reports for some areas, thought leaders of all organisations still need to amass data from heterogeneous collections like research publications, analyst reports, patent applications, competitor information etc. To help them finalize their own strategies. Text mining and data analytics researchers have been looking at integrating statistics, text analytics and information visualization to aid the process of retrieval and analytics. In this paper, we present our work on automated topical analysis and insight generation from large heterogeneous text collections of publications and patents. While most of the earlier work in this area provides search-based platforms, ours is an integrated platform for search and analysis. We have presented several methods and techniques that help in analysis and better comprehension of search results. We have also presented methods for generating insights about emerging and popular trends in research along with contextual differences between academic research and patenting profiles. We also present novel techniques to present topic evolution that helps users understand how a particular area has evolved over time.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927531","analyzing research trends;mining patent databases;mining publications","Context;Data mining;Data visualization;Hidden Markov models;Indexing;Market research;Patents","data analysis;information retrieval;patents;text analysis","academic research;automated topical analysis;heterogeneous document collections;insight generation;large heterogeneous text collections;patenting profiles;publications;topic evolution","","1","","14","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Ranking Linked-Entities in a Sentiment Graph","F. Peleja; J. Santos; J. Magalh√£es","Dept. Comput. Sci., Univ. Nova Lisboa, Lisbon, Portugal","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","2","","118","125","Reputation analysis is naturally associated to a sentiment analysis task of the targeted named-entities. This analysis leverages on a sentiment lexicon that includes general sentiment words that characterize the general sentiment towards the targeted named-entity. However, in most cases, target entities are themselves part of the sentiment lexicon, creating a loop from which it is difficult to infer an entity reputation. Sometimes, the entity became a reference in the domain and is vastly cited as an example of a highly reputable entity. For example, in the movies domain it is not uncommon to see reviews citing Batman or Anthony Hopkins as esteemed references. In this paper we describe a three-step procedure to perform reputation analysis of linked entities. First, our method jointly extracts named entities reputation and a domain specific sentiment lexicon. Second, an entities graph is created by analyzing cross-citations in subjective sentences. Third, the entities reputation are updated through an iterative optimization that exploits the graph of the linked-entities. The proposed approach closely models real-world domains, where domain specific jargon is common and entities are so popular that they become widely used as sentiment references. The evaluation on a graph with 12,687 vertices, of which 3,177 are linked entities and 9,510 are sentiment words, shows that our approach can improve the correct detection of an entity's reputation.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.88","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927615","LDA;Reputation analysis;sentiment lexicons","Computational modeling;Computer science;Dictionaries;Markov random fields;Motion pictures;Sentiment analysis;Twitter","graph theory;information retrieval systems;iterative methods;natural language processing;optimisation","cross-citation analysis;domain specific sentiment lexicon extraction;entities graph;entity reputation detection;general sentiment words;information retrieval system;iterative optimization;linked entities;linked-entity ranking;movies domain;named entities reputation extraction;named-entities;reputable entity;reputation analysis;sentiment analysis task;sentiment graph;subjective sentences;vertices","","0","","32","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Clustering based semantic data summarization technique: A new approach","M. Ahmed; A. N. Mahmood","School of Engineering and Information Technology, University of New South Wales, Canberra, Australia","2014 9th IEEE Conference on Industrial Electronics and Applications","20141023","2014","","","1780","1785","Due to advancement of computing and proliferation of data repositories, efficient data mining techniques are required to extract meaningful information. Summarization is such an important data analysis technique which can be broadly classified into two categories as semantic and syntactic methods. Syntactic methods consider a dataset as a sequence of bytes whereas semantic methods convert large dataset into a much smaller one yet maintaining low information loss. Clustering algorithms are widely used for semantic summarization such as basic k-means. Existing clustering based summarization techniques assume that a summary is represented using the cluster centroids. However, the centroids might not represent the actual data points in summary. In addition, many clustering algorithms, such as the most popular k-means algorithm requires the number of clusters as an input, which is not available for unsupervised summarization of unlabeled data. To address these issues, we propose a clustering based semantic summarization using a combination of x-means and k-medoid clustering algorithms. Our experimental analysis shows that, the proposed algorithm outperforms k-means based summarization techniques.","2156-2318;21562318","CD-ROM:978-1-4799-4316-6; Electronic:978-1-4799-4315-9; POD:978-1-4799-4314-2","10.1109/ICIEA.2014.6931456","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6931456","Clustering;Data Summarization","Conferences;Decision support systems;Industrial electronics","data analysis;data mining;information retrieval;pattern clustering","bytes sequence;cluster centroids;clustering based semantic data summarization technique;data analysis technique;data mining techniques;data repositories;information extraction;information loss;k-means algorithm;k-medoid clustering algorithms;large dataset;semantic methods;syntactic methods;x-means clustering algorithms","","1","","14","","","9-11 June 2014","","IEEE","IEEE Conference Publications"
"A Framework for Environmental Monitoring with Arduino-Based Sensors Using Restful Web Service","S. Lee; J. Jo; Y. Kim; H. Stephen","Dept. of Comput. Sci., Univ. of Nevada Las Vegas, Las Vegas, NV, USA","2014 IEEE International Conference on Services Computing","20141020","2014","","","275","282","The Nevada Solar Energy-Water-Environment Nexus project generates a large amount of environmental monitoring data from variety of sensors. This data is valuable for all related research areas, such as soil, atmosphere, biology, and ecology. An important aspect of this project is promoting data sharing and analysis using a common platform. To support this effort, we developed a comprehensive architecture that can efficiently collect the data from various sensors, store them in a database, and offer an intuitive user interface for data retrieval. We employed Arduino-based sensors due to their flexibility and cost-effectiveness. Restful Web Service is used for communication with the Arduino-based sensors, and Google Charts service has been used for data visualization. This framework for sensor data monitoring with Web Service is expected to allow the Nevada Nexus project to seamlessly integrate all types of sensor data and to provide a common platform for researchers to easily share the data.","","Electronic:978-1-4799-5066-9; POD:978-1-4799-5067-6","10.1109/SCC.2014.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930544","Arduino;Restful;Sensor network;Web Service","Data collection;Databases;Servers;Temperature measurement;Temperature sensors;Web services","Web services;data analysis;data visualisation;environmental science computing;information retrieval;search engines;user interfaces;wireless sensor networks","Arduino-based sensors;Google charts service;Nevada Nexus project;Nevada solar energy-water-environment nexus project;comprehensive architecture;cost-effectiveness;data analysis;data retrieval;data sharing;data visualization;environmental monitoring data;flexibility;intuitive user interface;restful Web service;sensor data monitoring","","4","","28","","","June 27 2014-July 2 2014","","IEEE","IEEE Conference Publications"
"Adaptive Semantic Search: Re-Ranking of Search Results Based on Webpage Feature Extraction and Implicitly Learned Knowledge of User Interests","G. Venkataraman; A. Ravichandran","Sri Venkateswara Coll. of Eng., Anna Univ., Sriperumbudur, India","2014 10th International Conference on Semantics, Knowledge and Grids","20141124","2014","","","75","78","The quest for information in the contemporary world ends at search engines that crawl millions of web pages on the World Wide Web and it is clearly essential that the results should be ranked in an order that would best fit the user interests. This paper proposes a method of re-ranking the search results that have been primarily ranked using either conventional algorithms that use link structure and user clicks or semantic algorithms, using a combination of general webpage features and user interests. The features of web pages like images, videos etc., are extracted by crawling them and the user's general interest in those features are learnt from past queries made and clicks on particular results. Using the degree to which each feature is present and the corresponding interest of the user, the user's interest in a particular search result is predicted and consequently the results are re-ranked in such a way that it augments the efficiency and effectiveness of conventional intent / meaning driven semantic search concept.","","Electronic:978-1-4799-6715-5; POD:978-1-4799-6716-2","10.1109/SKG.2014.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964667","Search engine ranking;re-ranking of search results;semantic search;user interests;webpage features","Feature extraction;Search engines;Semantics;Vectors;Videos;Web pages;Web search","Internet;information retrieval;learning (artificial intelligence)","Web page feature extraction;adaptive semantic search;implicit learning;link structure;semantic algorithms;semantic search concept;user clicks;user interests","","0","","18","","","27-29 Aug. 2014","","IEEE","IEEE Conference Publications"
"Automated Semantic Tagging of Textual Content","J. Jovanovic; E. Bagheri; J. Cuzzola; D. Gasevic; Z. Jeremic; R. Bashash","University of Belgrade, Serbia","IT Professional","20141124","2014","16","6","38","46","Motivated by a continually increasing demand for applications that depend on machine comprehension of text-based content, researchers in both academia and industry have developed innovative solutions for automated information extraction from text. In this article, the authors focus on a subset of such tools--semantic taggers--that not only extract and disambiguate entities mentioned in the text but also identify topics that unambiguously describe the text's main themes. The authors offer insight into the process of semantic tagging, the capabilities and specificities of today's semantic taggers, and also indicate some of the criteria to be considered when choosing a tagger.","1520-9202;15209202","","10.1109/MITP.2014.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964981","artificial intelligence;computing methodologies;data analysis;expert and knowledge-intensive system tools and techniques;intelligent systems;natural language processing;software;text analysis","Electronic publishing;Internet;Knowledge based systems;Semantics;Tagging;Text mining","data mining;information retrieval;text analysis","automated information extraction;automated semantic tagging;semantic taggers;text-based content;textual content","","2","","13","","","Nov.-Dec. 2014","","IEEE","IEEE Journals & Magazines"
"Compact web browsing profiles for click-through rate prediction","B. √ò. Fruergaard; L. K. Hansen","Adform ApS, Hovedvagtsgade 6, DK-1103 Copenhagen K, Denmark","2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","20141120","2014","","","1","6","In real time advertising we are interested in finding features that improve click-through rate prediction. One source of available information is the bipartite graph of websites previously engaged by identifiable users. In this work, we investigate three different decompositions of such a graph with varying degrees of sparsity in the representations. The decompositions that we consider are SVD, NMF, and IRM. To quantify the utility, we measure the performances of these representations when used as features in a sparse logistic regression model for click-through rate prediction. We recommend the IRM bipartite clustering features as they provide the most compact representation of browsing patterns and yield the best performance.","1551-2541;15512541","Electronic:978-1-4799-3694-6; POD:978-1-4799-3695-3","10.1109/MLSP.2014.6958852","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958852","","Computational modeling;Data models;Logistics;Matrix decomposition;Predictive models;Uniform resource locators;Vectors","Bayes methods;Web sites;advertising;feature extraction;graph theory;information retrieval;pattern clustering;regression analysis;singular value decomposition","Bayesian generative model;IRM bipartite clustering features;NMF;SVD;URL;Website;bipartite graph;browsing pattern representation;click-through rate prediction;compact Web browsing profile;graph decomposition;infinite relational model;nonnegative matrix factorization;real time advertising;representation sparsity;singular value decomposition;sparse logistic regression model","","0","","17","","","21-24 Sept. 2014","","IEEE","IEEE Conference Publications"
"Summarizing Search Results with Community-Based Question Answering","C. L. Chiang; S. Y. Chen; P. J. Cheng","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","254","261","Previous work on snippet generation focused mainly on how to produce one snippet for an individual search result. This paper aims to generate snippets as a comprehensive overview for an entity query (e.g., flu) in a search-result page. Our approach first extracts the attributes (e.g., Symptom and diagnose) of the categories (e.g., Disease) from a community-based question-answering (CQA) website, and then generates the snippets based on how central a sentence is to the meaning of the query, its category, and how well it diversifies the attributes. Integer Linear Programming (ILP) is adopted to find the optimal sentence set. The experiments are conducted on Wikipedia and Yahoo! Answers. Experimental results demonstrate the effectiveness of our approach, compared to an existing commercial search engine and several summarization baselines.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927550","Search-result summarization;snippet generation","Context;Electronic publishing;Encyclopedias;Internet;Search engines;Vectors","Web sites;integer programming;linear programming;query processing;question answering (information retrieval);search engines","CQA Website;ILP;Wikipedia;Yahoo! Answers;community-based question answering;entity query;integer linear programming;optimal sentence set;search engine;search result summarization;snippet generation;summarization baselines","","0","","25","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Semantic question answering of umrah pilgrims to enable self-guided education","N. M. Sharef; M. A. Murad; A. Mustapha; S. Shishechi","Faculty of Computer Science and Information Technology, University Putra Malaysia, Serdang, Malaysia","2013 13th International Conference on Intellient Systems Design and Applications","20141013","2013","","","141","146","Umrah as a pilgrimage to Mecca is obligatory in Islam madhab. Pilgrims can gain their knowledge about the requirements of Umrah using books, expert and existent question answering systems but still they suffer from the lack of complexity and natural language patterns. This paper proposes the semantic based question answering system to able pilgrims to compose any question about Umrah in natural language format. The proposed system used ontology to represent the knowledge about ritual of Umrah and Pilgrims. The question complexity for question in natural language is observed since it needs to be map with the contents in the ontology. An Umrah Knowledge module in this system covers all the rules and fact in the ontology format and the Educational modules is responsible for question answering interaction.","2164-7143;21647143","Electronic:978-1-4799-3516-1; POD:978-1-4799-3517-8","10.1109/ISDA.2013.6920724","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920724","Ontology;Semantic question answering;Umrah","Electronic publishing;Encyclopedias;Internet;Knowledge based systems;Organizations","natural language processing;ontologies (artificial intelligence);question answering (information retrieval)","Islam madhab;Mecca;Umrah knowledge module;Umrah pilgrims;educational modules;knowledge representation;natural language format;natural language pattern;ontology format;question answering interaction;question answering systems;question complexity;self-guided education;semantic question answering","","1","","17","","","8-10 Dec. 2013","","IEEE","IEEE Conference Publications"
"Digital multimedia archiving based on optimization steganography system","R. A. Wazirali; Z. Chaczko; A. Kale","University of Technology, Sydney","2014 Asia-Pacific Conference on Computer Aided System Engineering (APCASE)","20141016","2014","","","82","86","As soon as digital artifacts have become a part and parcel of everyday life, the need for digital media archives with the capacity of preserving the given metadata has risen impressively. The process of converting the digital metadata to archives, however, is fraught with a number of difficulties, the key one concerning the methodology for embedding high payload capacity information into the digital multimedia and at the same time retains high quality of the image. The given paper will consider steganography as a possible solution to the aforementioned issue. Allowing for detecting the genetic algorithm for boosting the PSNR value with the information of high capacity will help solve the issue regarding the digital multimedia archiving. Many sizes of data are embeded inside the images and the PSNR (Peak signal-to-noise ratio) is also taken for each of the images verified.","","Electronic:978-1-4799-4568-9; POD:978-1-4799-4567-2; USB:978-1-4799-4569-6","10.1109/APCASE.2014.6924476","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6924476","Block mapping;Genetic Algorithm;LSB;Steganography;message segmentation","Biological cells;Genetic algorithms;Media;Multimedia communication;PSNR;Sociology;Statistics","image processing;information retrieval systems;meta data;multimedia systems;optimisation;steganography","PSNR;digital artifacts;digital multimedia archiving;image quality;meta data;optimization steganography system;peak signal-to-noise ratio","","4","","18","","","10-12 Feb. 2014","","IEEE","IEEE Conference Publications"
"Data Replication Approach with Consistency Guarantee for Data Grid","J. H. Abawajy; M. M. Deris","Deakin University, Geelong, Australia","IEEE Transactions on Computers","20141105","2014","63","12","2975","2987","Data grids have been adopted by many scientific communities that need to share, access, transport, process, and manage geographically distributed large data collections. Data replication is one of the main mechanisms used in data grids whereby identical copies of data are generated and stored at various distributed sites to either improve data access performance or reliability or both. However, when data updates are allowed, it is a great challenge to simultaneously improve performance and reliability while ensuring data consistency of such huge and widely distributed data. In this paper, we address this problem. We propose a new quorum-based data replication protocol with the objectives of minimizing the data update cost, providing high availability and data consistency. We compare the proposed approach with two existing approaches using response time, data consistency, data availability, and communication costs. The results show that the proposed approach performs substantially better than the benchmark approaches.","0018-9340;00189340","","10.1109/TC.2013.183","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598664","Data grid;availability;big data;data constancy;data replication;reliability","Big data;Data models;Data replication;Distributed databases;Time factors","Big Data;data integrity;grid computing;information retrieval;reliability","big data;data access performance;data availability;data consistency;data grids;data replication approach;data update cost;geographically distributed large data collections;quorum-based data replication protocol;reliability;response time","","6","","33","","20130913","Dec. 2014","","IEEE","IEEE Journals & Magazines"
"A Device-Similarity-Based Recommendation System in Mobile Terminals","K. Lei; Q. Yu; R. Ning","Shenzhen Key Lab. for Cloud Comput. Technol. & Applic., Peking Univ. Shenzhen, Shenzhen, China","2013 Fourth International Conference on Networking and Distributed Computing","20141013","2013","","","60","64","Smart Mobile device are becoming popular platforms for information accessing, especially when coupled with recommendation system technologies. They are also treated as key tools for mobile users both for leisure and business applications. Recommendation techniques can increase the usability of mobile systems by providing more personalized and interested content. In this paper, a novel personalized recommender system is proposed, focusing on Mobile Terminal (MT) similarities, such as brands, versions and types of Operating Systems. These similarities play a key role in filtering original recommendation data sets at the preprocessing stage. By calculating and comparing the Mean Absolute Error (MAE) values through 5-fold cross validation of the Slope One algorithm with/without optimizing data sets by device-similarity, the overall effectiveness and accuracy of the recommendation results are at least 20% improved in our experiment.","2165-4999;21654999","CD-ROM:978-1-4799-3045-6; Electronic:978-1-4799-3046-3; POD:978-1-4799-3047-0","10.1109/ICNDC.2013.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6919856","Mobile Devices;Mobile Internet;Recommendation System;Similarity","Collaboration;Context;Educational institutions;Filtering;Mobile communication;Mobile handsets;Operating systems","information retrieval;mobile computing;recommender systems","5-fold cross validation;MAE values;business applications;device-similarity-based recommendation system;information access;leisure applications;mean absolute error;mobile terminals;slope one algorithm;smart mobile device","","1","","11","","","21-24 Dec. 2013","","IEEE","IEEE Conference Publications"
"Real-time data elicitation from Twitter: Evaluation and depiction strategies of tweets concerned to the blazing issues through Twitter application","K. Singh; S. Dhawan; Pratibha","Dept. of Comput. Sci. & Eng., Kurukshetra Univ., Kurukshetra, India","2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)","20141110","2014","","","103","108","Internet or more formally the World Wide Web is all about ‚ÄúInformation‚Äù. Information is that we can create; information is that we can share; information is that we can analyze to derive further more meaningful and interesting information. The prevailing web of documents is all about these concerns related to information. We create some information in any possible form regarding any possible domain of it and we are free to share it with others via internet. We can retrieve information from the web through the efficient search engines like Yahoo and Google but as far as sharing of information on a more profound platform is concerned we are availed with equally efficient and interactive social networking websites like Twitter, facebook, LinkedIn and many more. We can freely share our views, opinions and perspectives on these social media and additionally it can make people follow any person, community, organization or any particular context of their interest. However, security concerns about the information we share are always a matter of consideration. That is the part with sharing and letting people follow any piece of information on social media but a more fascinating job resides in collecting and analyzing such information so as to derive something really different and more reasonable and what could be better if we can perform this information extraction while it's been shared at the same time. Here we are talking about the live or Real-Time data retrieval from SNSs and predominantly from Twitter. Aided with a Twitter application we retrieved 13,708 live opinions regarding the trending issues and performed the analysis of the same to produce some valuable results in form of positive, negative emotion detection in that concern and we embellished all that with detailed graph plots in the context and made the evaluations of the diversified domains of information on being the most virulent of all.","","CD-ROM:978-1-4799-4237-4; Electronic:978-1-4799-4236-7; POD:978-1-4799-4235-0","10.1109/CONFLUENCE.2014.6949269","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949269","Real-time data retrieval;Twitter;Twitter Application Development;Twitter Application Management;Twitter Authentication","Blogs;Context;Media;Nominations and elections;Real-time systems;Twitter","information retrieval;search engines;security of data;social aspects of automation;social networking (online)","Facebook;Google;Internet;LinkedIn;SNSs;Tweets;Twitter application;World Wide Web;Yahoo;depiction strategies;emotion detection;evaluation strategies;graph plots;information creation;information extraction;information retrieval;information security;information sharing;live data retrieval;real-time data elicitation;real-time data retrieval;search engines;social media;social networking Websites","","0","","17","","","25-26 Sept. 2014","","IEEE","IEEE Conference Publications"
"Toward full-text searching middleware over hierarchical documents","K. Ma; B. Yang; A. Abraham","Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, China","2013 13th International Conference on Intellient Systems Design and Applications","20141013","2013","","","194","198","Currently, full-text searching can benefit from the emerging NoSQL databases and traditional indexing tools in the big data era. However, there are some drawbacks of current solutions. On one hand, the indexing documents lack of the hierarchy. On the other hand, big data have become the bottleneck of full-text searching. In the context of big data, we design a full-text searching middleware over hierarchical documents. We discuss the architecture of this middleware in detail. In addition, we propose a structure-independent hierarchical document model to present the hierarchical document. Moreover, the transformation engine is designed to translate the rich files into models. The core log event listener is responsible for capturing the changed documents and push them to the indexing storage at the same time. The experimental results show that our middleware is more advantageous than RDBMS with indexes and RDBMS with Lucene solutions.","2164-7143;21647143","Electronic:978-1-4799-3516-1; POD:978-1-4799-3517-8","10.1109/ISDA.2013.6920734","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920734","Full-text searching;NoSQL;hierarchical documents;middleware","Engines;Indexes;Middleware;Open source software;Real-time systems","document handling;indexing;information retrieval;middleware;relational databases","Lucene solutions;NoSQL databases;RDBMS;big data era;full-text searching middleware;indexing documents;indexing tools;log event listener;structure-independent hierarchical document model","","1","","12","","","8-10 Dec. 2013","","IEEE","IEEE Conference Publications"
"Exploring and exploiting knowledge in multiple resources","A. Almuhaimeed; M. Fasli","School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK","2014 6th Computer Science and Electronic Engineering Conference (CEEC)","20141120","2014","","","109","114","With the huge growth of the Internet and the WWW, several scientific fields have witnessed a proliferation of information available online. Techniques that are able to extract relations and associations that may exist in various resources including structured (ontologies and taxonomies) and unstructured (corpora) to inform search and can offer enhanced recommendations are highly desirable. The aim of this work is to pull together multiple bioinformatics resources with different structures such as ontologies and corpora and develop reasoning methods to extract semantic relations and hidden associations, which do not exist in the original resources. We have designed methods which reason through these resources and contain semantic rules (for instance, identifying sibling relations) that allow us to infer more hidden information between resources and aim to enhance recommendations in situations as described above. Moreover, we have designed a recommender approach for the bioinformatics field that provides recommendations on content (i.e. articles) as a personalised service for users based on information extracted from multiple resources as well as their individual profiles.","","Electronic:978-1-4799-6692-9; POD:978-1-4799-6693-6","10.1109/CEEC.2014.6958564","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958564","Bioinformatics;Personalisation;Reasoning;Recommendations;Semantic Techniques","Bioinformatics;Cognition;Data mining;Educational institutions;OWL;Ontologies;Semantics","bioinformatics;information retrieval;ontologies (artificial intelligence);recommender systems;semantic Web","Internet;World Wide Web;bioinformatics resources;corpora;information extraction;knowledge exploitation;knowledge exploration;ontologies;recommender approach;semantic rules;taxonomies","","0","","22","","","25-26 Sept. 2014","","IEEE","IEEE Conference Publications"
"Preparing to use informatics medicine, and trends in the age of meaningful use","S. H. Hung; C. H. Wang; H. H. Lin","National Center for High-performance Computing, Taichung, Taiwan","Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration","20141013","2014","","","3518","3521","Mistaken or wasted medicine has been a problem to Taiwan government, not only the caused worsen condition of patients' health by mistaken medicine but also, the wasted or thrown away medicine may led Taiwan government medicine expense costly and environment pollutions as well. According to Taiwan government records that, there is more than 130 tons (t) medicine had been abandoned in year 2011. The most obviously reasons for people abandoned medicine are, self-adjusting or self-stopping medicine by themselves; sharing medicine with friends; forgetting or mistiming of taken medicine. In the experiment, we intended to provide a web-based portal to assist patients for searching the medicine side effects information via our dataset of medicine definition or compounds, usage notice, side effects and so on. The dataset contained and integrated from medicine manufactures prescriptions and pictures and two medical centers, which based on Taiwan government provided medicine database, and in addition, the result information from web surfing in the text mapping search engine will also be considered as well. The dataset itself needs to be leveraged and structured as the promise of meaningful use. We consider linking the chronic disease management such as kidney disease and asthma for medicine suggestions between patients and health providers.","2159-5100;21595100","Electronic:978-1-890843-29-8; POD:978-1-4799-5769-9; USB:978-1-890843-30-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921064","","Databases;Diseases;Drugs;Government;Hospitals;Insurance","Internet;diseases;electronic health records;government data processing;health care;information retrieval;medical computing;portals;search engines;text analysis","Taiwan government medicine expense;Taiwan government records;Web surfing;Web-based portal;asthma;chronic disease management;environment pollutions;informatics medicine;kidney disease;medical centers;medicine compound dataset;medicine database;medicine definition dataset;medicine side effects information searching;medicine suggestions;mistaken medicine;self-adjusting medicine;self-stopping medicine;text mapping search engine;usage notice;wasted medicine","","0","","8","","","27-31 July 2014","","IEEE","IEEE Conference Publications"
"Parallelizing K-Means Algorithm for 1-D Data Using MPI","I. K. Savvas; G. N. Sofianidou","Dept. of Comput. Sci. & Eng., T.E.I. of Thessaly, Larissa, Greece","2014 IEEE 23rd International WETICE Conference","20141020","2014","","","179","184","Nowadays, colossal amount of information is produced by computational systems and electronic instruments such as telescopes, medical devices and so on. To explore these petabytes of data, new fast algorithms must be discovered or old ones may be redesigned. One of the most popular and useful techniques in order to discover and extract information from data pools is clustering, and k-means is an algorithm which clusters data according its characteristics. Its main disadvantage is its computational complexity which makes the technique very difficult to apply on big data sets. Although k-means is a very well studied technique, a fully parallel version of it has not been explored yet. In this work, a parallel version of the k-means is presented for 1-d objects. The experimental results obtained are inline with the theoretical outcome and prove both the correctness and the effectiveness of the technique.","1524-4547;15244547","Electronic:978-1-4799-4249-7; POD:978-1-4799-4248-0","10.1109/WETICE.2014.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927046","","Algorithm design and analysis;Clustering algorithms;Computational complexity;Data mining;Equations;Peer-to-peer computing","Big Data;application program interfaces;computational complexity;information retrieval;message passing;parallel algorithms;pattern clustering","1D data;MPI;big data sets;computational complexity;computational systems;data pools;electronic instruments;information discovery;information extraction;medical devices;message passing interface;parallelizing k-means algorithm;telescopes","","1","","14","","","23-25 June 2014","","IEEE","IEEE Conference Publications"
"A Reference Framework for the Automated Exploration of Web Applications","G. L. Breton; N. Bergeron; S. Hall√©","Dept. d'Inf. et de Math., Univ. du Quebec a Chicoutimi, Chicoutimi, QC, Canada","2014 19th International Conference on Engineering of Complex Computer Systems","20141016","2014","","","81","90","Web crawling is the process of exhaustively exploring the contents of a web site or application through automated means. While the results of such a crawling can be put through numerous uses ranging from a simple backup to comprehensive testing and analysis, features of modern-day applications prevent crawlers from properly exploring applications. We provide an in-depth analysis of 15 such features, and report on their presence in a study of 16 real-world web sites. Based on that study, we develop a configurable web application where the presence of each such feature can be turned on or off, aimed as a test bench where existing crawlers can be compared in a uniform way. Our results, which are the first exhaustive comparison of available crawlers, indicates areas where future work should be aimed.","","Electronic:978-1-4799-5482-7; POD:978-1-4799-5483-4","10.1109/ICECCS.2014.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923122","benchmark;crawlers;web applications","Browsers;Crawlers;HTML;Navigation;Servers;Testing;Web sites","Internet;information retrieval","Web application exploration;Web crawling;Web site","","0","","23","","","4-7 Aug. 2014","","IEEE","IEEE Conference Publications"
"A Semantically Enriched Context-Aware OER Recommendation Strategy and Its Application to a Computer Science OER Repository","A. Ruiz-Iniesta; G. Jim√©nez-D√≠az; M. G√≥mez-Albarr√°n","Department of Software Engineering and Artificial Intelligence, Complutense University of Madrid, Spain","IEEE Transactions on Education","20141028","2014","57","4","255","260","This paper describes a knowledge-based strategy for recommending educational resources-worked problems, exercises, quiz questions, and lecture notes-to learners in the first two courses in the introductory sequence of a computer science major (CS1 and CS2). The goal of the recommendation strategy is to provide support for personalized access to the resources that exist in open educational repositories. The strategy uses: 1) a description of the resources based on metadata standards enriched by ontology-based semantic indexing, and 2) contextual information about the user (her knowledge of that particular field of learning). The results of an experimental analysis of the strategy's performance are presented. These demonstrate that the proposed strategy offers a high level of personalization and can be adapted to the user. An application of the strategy to a repository of computer science open educational resources was well received by both educators and students and had promising effects on the student performance and dropout rates.","0018-9359;00189359","","10.1109/TE.2014.2309554","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6774489","Computer science repositories;knowledge-based recommender systems;open education;users' experience","Computer science;Context;Educational institutions;Knowledge based systems;Measurement;Ontologies;Standards","computer aided instruction;computer science education;educational courses;indexing;information retrieval;meta data;ontologies (artificial intelligence);recommender systems;ubiquitous computing","computer science OER repository;computer science major;computer science open educational resources;contextual information;courses;exercises;introductory sequence;knowledge-based strategy;lecture notes;metadata standards;ontology-based semantic indexing;open educational repositories;personalization level;personalized access;quiz questions;semantically enriched context-aware OER recommendation strategy;strategy performance;student dropout rates;student performance;user knowledge;worked problems","","4","","19","","20140317","Nov. 2014","","IEEE","IEEE Journals & Magazines"
"Exploring user expertise and descriptive ability in community question answering","B. Yang; S. Manandhar","Department of Computer Science, University of York, UK","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","320","327","The research on community question answering (CQA) has been paid increasing attention in recent years. In CQA, to reduce the number of unanswered questions and the time for askers to wait, it is very necessary to identify relevant experts or best answers for these questions. Generally, the experts' answers are more likely to be the best answers. Existing studies considered that user expertise is reflected by the voting scores of both answers and questions. However, voting scores of questions are not really related to user expertise. In this paper, we proposed a new probabilistic model to depict users' expertise based on answers and their descriptive ability based on questions. To exploit social information in CQA, the link analysis is also considered. Extensive experiments on the large Stack Overflow dataset demonstrate that our methods can achieve comparable or even better performance than the state-of-the-art models.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921604","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921604","","Communities;Conferences;Gaussian distribution;Knowledge discovery;Social network services;Training;Vocabulary","probability;question answering (information retrieval)","CQA;community question answering;descriptive ability;large Stack Overflow dataset;link analysis;probabilistic model;social information;user expertise","","4","","22","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Topic modeling on users's comments","D. Ramamonjisoa","Faculty of Software and Information Science, Iwate Prefectural University, IPU, Takizawa, Japan","2014 Third ICT International Student Project Conference (ICT-ISPC)","20141016","2014","","","177","180","User-contributed comments are increasing exponentially on the Social Web, they are found widely in the social media sites (internet discussion fora or news providers). This paper describes an experiment for topic modeling on users' comments in social media. The future application of the method is discussed.","","Electronic:978-1-4799-5573-2; POD:978-1-4799-5574-9","10.1109/ICT-ISPC.2014.6923245","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923245","topic modeling;users's comments","Xenon","information retrieval;social networking (online)","social Web;social media;topic modeling;user-contributed comments","","0","","11","","","26-27 March 2014","","IEEE","IEEE Conference Publications"
"Comparison of Similarity Coefficients for Chemical Database Retrieval","M. Syuib; S. M. Arif; N. Malim","Sch. of Inf. Technol., Univ. Kebangsaan Malaysia, Bangi, Malaysia","2013 1st International Conference on Artificial Intelligence, Modelling and Simulation","20141120","2013","","","129","133","Similarity-based virtual screening is used in drug discovery by using computational model for rapid evaluation of large number of chemical molecules. Similarity searches use 2D or 3D fingerprints and similarity coefficient to calculate the structural resemblance between each molecule in a chemical database and a target structure. The objective of this work is to determine the best coefficient to be used in similarity searching to get the optimal results. This paper will describe the experiment to perform the molecular similarity searching using different similarity coefficients, which focus on 2D UNITY or ECFP4 fingerprint on 5 activity classes. We will also highlight the different similarity values and the optimal results of similarity measures. All this could depend on what type of fingerprint. As a conclusion, we found that every combination measure has its own advantage. But to look for the best possible results, the nature of molecular activity class could also play an important role.","","Electronic:978-1-4799-3251-1; POD:978-1-4799-3252-8","10.1109/AIMS.2013.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6959906","2D fingerprints;Chemoinformatics;similarity measure;virtual screening","Chemicals;Compounds;Computers;Databases;Drugs;Fingerprint recognition","chemical engineering computing;database management systems;drugs;information retrieval","2D UNITY;2D fingerprints;3D fingerprints;ECFP4 fingerprint;chemical database retrieval;chemical molecules;computational model;drug discovery;molecular activity class;molecular similarity searching;similarity coefficients;similarity-based virtual screening;structural resemblance","","0","","11","","","3-5 Dec. 2013","","IEEE","IEEE Conference Publications"
"An Industrial Case Study on Provenance Awareness of Composite Services","P. Zerva; K. Hamadache; G. Angouras; S. Zschaler; S. Miles","Dept. of Inf., King's Coll., London, UK","2014 10th International Conference on Semantics, Knowledge and Grids","20141124","2014","","","92","99","Provenance awareness adds a new dimension to the engineering of service-based systems, enabling them to increase their accountability through answering questions about the provenance of any data produced. Provenance awareness can be achieved by recording provenance data during system execution. In our previous work we have proposed an overall research agenda towards a design and analysis framework for provenance awareness of composite services. A fundamental element of this framework is the provenance model, Service Provontology, capturing the structure of the provenance data collected which allows designers to query and reason over provenance data instances of composite services. With this paper we contribute an industrial case study exploring real-world provenance data from a service-based system (ORBI). In our study ServiceProv becomes the tool for enabling representation and reasoning over ORB provenance data instances in order to answer specific provenance questions formalized as SPARQL expressions.","","Electronic:978-1-4799-6715-5; POD:978-1-4799-6716-2","10.1109/SKG.2014.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964670","industrial case study;provenance awareness;provenance data;service based systems;service composition","Analytical models;Cognition;Context;Data models;History;Ontologies;Servers","ontologies (artificial intelligence);query processing;question answering (information retrieval)","ORB provenance data instances;ORBI;SPARQL expressions;ServiceProv ontology;composite services;provenance awareness;provenance data instance querying;provenance data recording;question answering;service-based system engineering;system execution","","0","","15","","","27-29 Aug. 2014","","IEEE","IEEE Conference Publications"
"Improving Biodiversity Data Retrieval through Semantic Search and Ontologies","F. K. Amanqui; K. J. Serique; S. D. Cardoso; J. L. D. Santos; A. Albuquerque; D. A. Moreira","ICMC, Univ. of Sao Paulo, Sao Carlos, Brazil","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","1","","274","281","Due to the increased amount of available biodiversity data, many biodiversity research institutions are now making their databases openly available on the web. Researchers in the field use this databases to extract new knowledge and also share their own discoveries. However, when these researchers need to find relevant information in the data, they still rely on the traditional search approach, based on text matching, that is not appropriate to be used in these large amounts of heterogeneous biodiversity's data, leading to search results with low precision and recall. We present a new architecture that tackle this problem using a semantic search system for biodiversity data. Semantic search aims to improve search accuracy by using ontologies to understand user objectives and the contextual meaning of terms used in the search to generate more relevant results. Biodiversity data is mapped to terms from relevant ontologies, such as Darwin Core, DBpedia, Ontobio and Catalogue of Life, stored using semantic web formats and queried using semantic web tools (such as triple stores). A prototype semantic search tool was successfully implemented and evaluated by users from the National Research Institute for the Amazon (INPA). Our results show that the semantic search approach has a better precision (28% improvement) and recall (25% improvement) when compared to keyword based search, when used in a big set of representative biodiversity data (206,000 records) from INPA and the Emilio Gueldi Museum in ParaÃÅ (MPEG). We also show that, because the biodiversity data is now in semantic web format and mapped to ontology terms, it is easy to enhance it with information from other sources, an example using deforestation data (from the National Institute of Space Research - INPE) to enrich collection data is shown.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927553","Biodiversity;Data Integration;Ontology;Semantic Search;Semantic Web","Biodiversity;Bioinformatics;OWL;Ontologies;Resource description framework;Search problems;Semantics","data integration;database management systems;information retrieval;ontologies (artificial intelligence);semantic Web","Catalogue of Life;DBpedia;Darwin Core;Emilio Gueldi Museum in ParaÃÅ;INPA;INPE;MPEG;National Institute of Space Research;National Research Institute for the Amazon;Ontobio;biodiversity data retrieval;databases;deforestation data;heterogeneous biodiversity data;keyword based search;ontologies;representative biodiversity data;semantic Web formats;semantic Web tools;semantic search system;text matching search approach","","3","","15","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Efficient Approximate Membership Localization using P-Prune algorithm in blogs","Kaladevi A. C.; Nivetha S. M.","Department of Computer Science and Engineering, Sona College of Technology, Salem, Tamil Nadu, India","2014 International Conference on Computer Communication and Informatics","20141016","2014","","","1","8","Approximate Membership Localization (AML) is concerned with locating non-overlapped substrings thus avoiding redundancies. This overcomes the drawback of Approximate Membership Extraction (AME) process which has low efficiency for real world application. An algorithm called P-Prune is used in Blog search. This prunes most of the overlapped redundant substrings before generating them. Here we use the opinion retrieval scheme which analyses the viewers' comments on Blog contents. Our experimental study on blogs reveals the efficiency of P-Prune over AME method. We also work AML in application to a proposed Blog search framework, a search-based approach joining two tables using dictionary-based entity recognition from blogs. Apart from the advantage of AML over AME, the experiment also proves the efficiency of the search-based approach. This algorithm can be extended to video blogs (vlog) also.","","Electronic:978-1-4799-2352-6; POD:978-1-4799-2354-0","10.1109/ICCCI.2014.6921771","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921771","Approximate membership Extraction;Approximate membership localization;Blog;P-Prune","Algorithm design and analysis;Approximation algorithms;Approximation methods;Blogs;Dictionaries;Redundancy;Search engines","Web sites;information retrieval;learning (artificial intelligence);pattern recognition","AME process;AML;P-prune algorithm;approximate membership extraction process;approximate membership localization;blog search;dictionary-based entity recognition;opinion retrieval scheme;search-based approach;video blogs","","0","","25","","","3-5 Jan. 2014","","IEEE","IEEE Conference Publications"
"Joint voting prediction for questions and answers in CQA","Y. Yao; H. Tong; T. Xie; L. Akoglu; F. Xu; J. Lu","State Key Laboratory for Novel Software Technology, Nanjing University, China","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","340","343","Community Question Answering (CQA) sites have become valuable repositories that host a massive volume of human knowledge. How can we detect a high-value answer which clears the doubts of many users? Can we tell the user if the question s/he is posting would attract a good answer? In this paper, we aim to answer these questions from the perspective of the voting outcome by the site users. Our key observation is that the voting score of an answer is strongly positively correlated with that of its question, and such correlation could be in turn used to boost the prediction performance. Armed with this observation, we propose a family of algorithms to jointly predict the voting scores of questions and answers soon after they are posted in the CQA sites. Experimental evaluations demonstrate the effectiveness of our approaches.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921607","","Conferences;Correlation;Educational institutions;Joints;Knowledge discovery;Logistics;Prediction algorithms","Web sites;question answering (information retrieval)","CQA;community question answering site;joint voting prediction;voting outcome","","1","","18","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Attribute based encryption for securing personal health record on cloud","D. A. Gondkar; V. S. Kadam","S.I.T LONAWALA PUNE, India","2014 2nd International Conference on Devices, Circuits and Systems (ICDCS)","20141016","2014","","","1","5","Personal health record (PHR) has emerged as a patient-centric model of health information exchange. A PHR service allows a patient to create, manage, and control her personal health data in one place through the web, which has made the storage, retrieval, and sharing of the medical information more efficient. Especially, each patient is promised the full control of her medical records and can share her health data with a wide range of users, including healthcare providers, family members or friends. The main aim of this research work is to propose a novel framework of secure sharing of personal health records in cloud computing. Considering partially trustworthy cloud servers, we argue that to fully realize the patient-centric concept, patients shall have complete control of their own privacy through encrypting their PHR files to allow fine-grained access.","","Electronic:978-1-4799-1356-5; POD:978-1-4799-1354-1","10.1109/ICDCSyst.2014.6926174","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926174","","Circuits and systems;Cloud computing;Databases;Electronic mail;Encryption;Medical services","cloud computing;cryptography;health care;information retrieval;information storage;medical information systems","PHR;attribute based encryption;cloud computing;health care;health information exchange;medical information retrieval;medical information sharing;medical information storage;patient-centric model;personal health record security","","2","","7","","","6-8 March 2014","","IEEE","IEEE Conference Publications"
"Performance analysis of different keyword extraction algorithms for emotion recognition from Uyghur text","S. Imam; R. Parhat; A. Hamdulla; Z. Li","Xinjiang Univ., Urumqi, China","The 9th International Symposium on Chinese Spoken Language Processing","20141027","2014","","","351","351","Summary form only given. This paper conducts the comparing research on Uyghur sentence sentiment classification using different keywords extraction methods. Firstly, the keywords expressing happiness and anger are extracted respectively by the methods of TextRank, SAD and SparseSVM, then used to train the sentiment models accordingly. The sentiment text database is built by excerpting two kinds of sentiments including anger and happiness from Uyghur movie transcriptions and novels. Several experiments are undertaken using different classification methods mentioned above. The experimental results show that the classification methods based on keyword extraction used in this paper are effective in Uyghur text sentence emotion recognition. Among them SparseSVM method gifts robustness and higher accuracy in recognition experiments.","","Electronic:978-1-4799-4219-0; POD:978-1-4799-4218-3; USB:978-1-4799-4220-6","10.1109/ISCSLP.2014.6936652","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936652","Emotion recognition;SDA;SparseSVM;TextRank;Uyghur","Abstracts;Algorithm design and analysis;Classification algorithms;Databases;Educational institutions;Emotion recognition;Performance analysis","emotion recognition;information retrieval;natural language processing;pattern classification;support vector machines;text analysis","SAD;SparseSVM;TextRank;Uyghur movie transcriptions;Uyghur sentence sentiment classification;Uyghur text sentence emotion recognition;anger;happiness;keyword extraction algorithms;performance analysis;sentiment models;sentiment text database","","0","","","","","12-14 Sept. 2014","","IEEE","IEEE Conference Publications"
"A research of dynamic theme crawler based on keywords and support vector machine","B. Song; J. m. Zhu; J. g. Zhang","School of Information, Central University of Finance and Economics, Beijing 100081, China","2014 International Conference on Management Science & Engineering 21th Annual Conference Proceedings","20141020","2014","","","21","26","With the rapid development of Internet technology, the limited ability of search engines is difficult to process the increasingly mass data. How to obtain the useful target information from the internet in a short time has become a difficult problem currently. The search results by using general search engines are inaccuracy and the relationship with the search them is not very strong. The theme crawler can focus on a single field, but cannot be applied to other fields. According to introducing the keywords and support vector machine model, this paper proposed a dynamic theme crawler system. The experimental result shows that, the dynamic theme crawler based on keywords and support vector machine can effectively acquire target information. This method can be flexibly applied in the fields of information security, the management of enterprise public relations crisis and so on.","2155-1847;21551847","CD-ROM:978-1-4799-5374-5; Electronic:978-1-4799-5376-9; POD:978-1-4799-5932-7","10.1109/ICMSE.2014.6930203","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930203","algorithm;crawler;dynamic theme;support vector machine","Algorithm design and analysis;Classification algorithms;Correlation;Crawlers;Heuristic algorithms;Support vector machine classification","Internet;information retrieval;search engines;support vector machines","Internet technology;dynamic theme crawler;enterprise public relations crisis management;information security;keywords;search engines;support vector machine;target information","","0","","21","","","17-19 Aug. 2014","","IEEE","IEEE Conference Publications"
"Phonetic matching and syntactic tree similarity based QA system for SMS queries","A. Mittal; P. Bhatt; P. Kumar","Department of Computer Science And Engineering, Graphic Era University, Dehradun, India","2014 International Conference on Green Computing Communication and Electrical Engineering (ICGCCEE)","20141016","2014","","","1","6","Currently there have been many QA systems for SMS queries with large archive but none is addressing the most commonly present phonetic noise in SMS queries. However, Finding similar questions in the QA archive is not trivial especially in the presence of phonetic noise. This paper proposes a solution to handle the noise including phonetic noise. We present a technique to handle semantic variation by developing new phonetic algorithm that uses Soundex and Metaphone algorithm; In addition we have modified Longest Common Subsequence problem to raise the level of similarity between noisy words of SMS and corresponding dictionary words. Following this approach it also handles syntactic variation in question formulation without any SMS normalization.","","Electronic:978-1-4799-4982-3; POD:978-1-4799-4981-6","10.1109/ICGCCEE.2014.6921412","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921412","Noise Handling;SMS queries;Similarity Score;noisy text","Accuracy;Dictionaries;Kernel;Noise;Noise measurement;Semantics;Syntactics","dictionaries;electronic messaging;query processing;question answering (information retrieval);speech processing;trees (mathematics)","Metaphone algorithm;QA archive;SMS queries;Soundex algorithm;dictionary words;longest common subsequence problem;phonetic matching;phonetic noise;question formulation;short message service;syntactic tree similarity based QA system","","0","","8","","","6-8 March 2014","","IEEE","IEEE Conference Publications"
"Wireless component-based health data acquisition and monitoring system","A. H. Kioumars; L. Tang","School of Engineering and Advanced Technology, Massey University, Palmerston North, New Zealand","2014 9th IEEE Conference on Industrial Electronics and Applications","20141023","2014","","","1567","1572","In the field of human health, collecting and analyzing real-time data is crucial. Information technology (IT) has the potential to improve the quality, safety and efficiency of healthcare by enabling the service providers and their patients to access the health information. IT allows healthcare providers to collect, store, retrieve, and transfer information electronically. Patients will be able to monitor their own vital signs from their home and communicate the results to their service providers wirelessly. This makes it possible to address a problem before a patient requires acute care and maintain the quality of their life by enabling them to be close to their families and relatives. This paper presents a novel component-based monitoring system (T-HBR). T-HBR supports a multi-user environment, has a high level of security and has the potential to communicate with a remote server or service robot wirelessly. The experimental results of the system demonstrate the ease of use and high degree of reliability.","2156-2318;21562318","CD-ROM:978-1-4799-4316-6; Electronic:978-1-4799-4315-9; POD:978-1-4799-4314-2","10.1109/ICIEA.2014.6931418","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6931418","Electronic Health Records;Emails and Messages;Lightweight Strings;MySQL Database;T-HBR Software;Wireless Programming;XBee Wireless Communication","Biomedical imaging;Databases;EPROM;Electronic mail;Hardware;Medical services;Software","biomedical communication;data acquisition;data analysis;electronic data interchange;electronic health records;health care;information retrieval;information storage;patient care;patient monitoring;reliability;security of data;wireless sensor networks","IT;T-HBR;acute care;component-based monitoring system;health information;healthcare efficiency;healthcare providers;healthcare quality;healthcare safety;human health;information collection;information retrieval;information storing;information technology;information transfer;multiuser environment;quality of life;real-time data analysis;real-time data collection;reliability degree;remote server;security level;service providers;service robot;vital signs;wireless component-based health data acquisition","","0","","10","","","9-11 June 2014","","IEEE","IEEE Conference Publications"
"Integrating Ontological Information about Genes","N. Dess√¨; E. Pascariello; B. Pes","Dipt. di Mat. e Inf., Univ. degli Studi di Cagliari, Cagliari, Italy","2014 IEEE 23rd International WETICE Conference","20141020","2014","","","417","422","With the advent of biological ontologies an increasing amount of methods are emerging for enriching gene information by means of their annotations. However, problems occur in assessing semantic similarity over genetic aspects that are represented independently in different schemas when, in reality, they are not. This paper presents a framework that integrates heterogeneous knowledge from different resources (i.e. ontologies, texts, expert classifications) for capturing information about how a set of genes work together in targeting a biological process. Our approach grounds on the ontological annotation of gene summaries. Given the analogy between these annotations and the representation of documents in information retrieval, we apply techniques used in text mining to evaluate the semantic similarity of summaries within a gene set. To determine if our framework makes sense in a biological context, we conducted experiments on popular gene sets and compared results with what asserted by domain experts. Our approach provides an empirical basis for capturing complementary information about how genes interact and could be used in conjunction with other similarity methods or bioinformatic tools.","1524-4547;15244547","Electronic:978-1-4799-4249-7; POD:978-1-4799-4248-0","10.1109/WETICE.2014.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927094","Bio-organizations;Gene summaries;Ontologies;Semantic annotation","Biomedical measurement;Ontologies;Organizations;Protein engineering;Proteins;Semantics","bioinformatics;data mining;genetics;information retrieval;ontologies (artificial intelligence);text analysis","bioinformatic tools;biological ontologies;biological process;documents representation;gene information;gene set;gene summaries;genetic aspects;heterogeneous knowledge;information retrieval;ontological annotation;ontological information;semantic similarity;similarity methods;text mining","","3","","16","","","23-25 June 2014","","IEEE","IEEE Conference Publications"
"SoK: Introspections on Trust and the Semantic Gap","B. Jain; M. B. Baig; D. Zhang; D. E. Porter; R. Sion","","2014 IEEE Symposium on Security and Privacy","20141120","2014","","","605","620","An essential goal of Virtual Machine Introspection (VMI) is assuring security policy enforcement and overall functionality in the presence of an untrustworthy OS. A fundamental obstacle to this goal is the difficulty in accurately extracting semantic meaning from the hypervisor's hardware level view of a guest OS, called the semantic gap. Over the twelve years since the semantic gap was identified, immense progress has been made in developing powerful VMI tools. Unfortunately, much of this progress has been made at the cost of reintroducing trust into the guest OS, often in direct contradiction to the underlying threat model motivating the introspection. Although this choice is reasonable in some contexts and has facilitated progress, the ultimate goal of reducing the trusted computing base of software systems is best served by a fresh look at the VMI design space. This paper organizes previous work based on the essential design considerations when building a VMI system, and then explains how these design choices dictate the trust model and security properties of the overall system. The paper then observes portions of the VMI design space which have been under-explored, as well as potential adaptations of existing techniques to bridge the semantic gap without trusting the guest OS. Overall, this paper aims to create an essential checkpoint in the broader quest for meaningful trust in virtualized environments through VM introspection.","1081-6011;10816011","Electronic:978-1-4799-4686-0; POD:978-1-4799-4685-3","10.1109/SP.2014.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6956590","VM Introspection;semantic gap;trust","Data structures;Kernel;Linux;Monitoring;Security;Semantics;Virtual machine monitors","information retrieval;security of data;trusted computing;virtual machines;virtualisation","VMI system;semantic gap;semantic meaning extraction;system security;trusted computing base;virtual machine introspection;virtualized environments","","12","","94","","","18-21 May 2014","","IEEE","IEEE Conference Publications"
"Capability Annotation of Actions Based on Their Textual Descriptions","F. Gao; S. Bhiri","INSIGHT, NUI Galway, Galway, Ireland","2014 IEEE 23rd International WETICE Conference","20141020","2014","","","257","262","Current approaches for semantic management of business processes and services assume and build on semantic models which don't exist in practice. A key and open research problem is how to lift semantic models from existing process and service descriptions. In this paper, we are interested in extracting action capabilities from their textual descriptions. We first introduce a semantic frame-based capability model that features domain-specific functional properties. We presentan approach for retrieving top-k capabilities that match best a natural language description of an action. Our approach has been evaluated using publicly available process and service descriptions.","1524-4547;15244547","Electronic:978-1-4799-4249-7; POD:978-1-4799-4248-0","10.1109/WETICE.2014.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927061","business process management;capability modelling;semantic annotation;semantic services","Business;Feature extraction;Instruments;Natural languages;Ontologies;Pragmatics;Semantics","business data processing;information retrieval;text analysis","action capability annotation;business services;domain-specific functional properties;natural language description;process description;semantic business process management;semantic frame-based capability model;semantic models;service description;textual description;top-k capabilities retrieval","","0","","17","","","23-25 June 2014","","IEEE","IEEE Conference Publications"
"A novel statistical and linguistic features based technique for keyword extraction","A. Gupta; A. Dixit; A. K. Sharma","Computer Engineering Department YMCA University of Science & Technology Faridabad, India","2014 International Conference on Information Systems and Computer Networks (ISCON)","20141124","2014","","","55","59","WWW is a decentralized, distributed and heterogeneous information resource. With increased availability of information through WWW, it is very difficult to read all documents to retrieve the desired results; therefore there is a need of summarization methods which can help in providing contents of a given document in a precise manner. Keywords of a document may provide a compact representation of a document's content. As a result various algorithms and systems intended to carry out automatic keywords extraction have been proposed in the recent past. However, the existing solutions require either training models or domain specific information for automatic keyword extraction. To cater to these shortcomings an innovative hybrid approach for automatic keyword extraction using statistical and linguistic features of a document has been proposed. This statistical and linguistic technique based keyword extraction works on an individual document without any prior parameter change and takes full advantage of all the features of the document to extract the keywords. The extracted keywords can than assist in domain specific indexing. The performance of the proposed method as compared to existing Keyword Extraction tools such as Dream web design etc. in terms of Precision and Recall are also presented in this paper.","","Electronic:978-1-4799-2981-8; POD:978-1-4799-2982-5","10.1109/ICISCON.2014.6965218","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965218","Domain ontology;Extractor;Key-phrase;Linguistic technique;Statistical technique;Unsupervised-approach","Data mining;Educational institutions;Feature extraction;Ontologies;Pragmatics;Web pages","Internet;computational linguistics;indexing;information resources;information retrieval;statistical analysis","World Wide Web;automatic keywords extraction;document retrieval;domain specific indexing;information resource;keyword extraction;linguistic features;statistical features","","1","","8","","","1-2 March 2014","","IEEE","IEEE Conference Publications"
"Android and Wireless data-extraction using Wi-Fi","B. Busstra; N. A. Le-Khac; M. T. Kechadi","Digital Forensic Investigator, Central Unit, Dutch Police, Zoetermeer, The Netherlands","Fourth edition of the International Conference on the Innovative Computing Technology (INTECH 2014)","20141020","2014","","","170","175","Today, mobile phones are very popular, fast growing technology. Mobile phones of the present day are more and more like small computers. The so-called ‚Äúsmartphones‚Äù contain a wealth of information each. This information has been proven to be very useful in crime investigations, because relevant evidence can be found in data retrieved from mobile phones used by criminals. In traditional methods, the data from mobile phones can be extracted using an USB-cable. However, for some reason this USB-cable connection cannot be made, the data should be extracted in an alternative way. In this paper, we study the possibility of extracting data from mobile devices using a Wi-Fi connection. We describe our approach on mobile devices containing the Android Operating System. Through our experiments, we also give recommendation on which application and protocol can be used best to retrieve data.","","Electronic:978-1-4799-4233-6; POD:978-1-4799-4232-9","10.1109/INTECH.2014.6927769","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927769","Android;SSHDroid;Wireless data extraction;network protocols;rsync","Androids;Data mining;Humanoid robots;IEEE 802.11 Standards;Protocols;Smart phones","Android (operating system);information retrieval;protocols;smart phones;wireless LAN","Android operating system;USB-cable connection;Wi-Fi connection;crime investigations;criminals;data retrieval;mobile devices;mobile phones;protocol;smartphones;wireless data-extraction","","1","","29","","","13-15 Aug. 2014","","IEEE","IEEE Conference Publications"
"Tools for External Plagiarism Detection in DOCODE","J. D. Vel√°squez; E. M. Taylor","Dept. of Ind. Eng., Univ. of Chile, Santiago, Chile","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","2","","296","303","In this paper we describe the algorithms and tools offered by DOCODE, a system for plagiarism detection in educational institutions, with a special focus on the task of external plagiarism detection using the Web as a source of information. In that context, although DOCODE is a full-featured system based on several algorithms, our main contribution is an algorithm that given a document is capable of retrieving similar or related documents from the Web, tackling the problem of external plagiarism detection. However, all our algorithms work together to provide high-level plagiarism detection functionalities to our users. Therefore, here we also give details about how these functionalities are bundled and presented in ad-hoc Web-based user interfaces for different kinds of clients, supporting the decision-making process regarding possible plagiarism cases.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927638","","Business;Context;Detectors;Educational institutions;Plagiarism;Search engines;Web services","Internet;computer aided instruction;information retrieval;text analysis","DOCODE;ad-hoc Web-based user interface;decision making process;document retrieval;educational institution;external plagiarism detection tools;full-featured system;high-level plagiarism detection functionalities;plagiarism cases","","0","","34","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Intelligent online customer recognition framework: Dealing with common personal names","M. Saberi; O. K. Hussain; E. Chang","School of Information Systems, Curtin University, Perth, Australia","2014 9th IEEE Conference on Industrial Electronics and Applications","20141023","2014","","","966","971","Nowadays, individuals (customers) are in contact with various organizations to meet their particular needs. As an integrated and unique database is not used in the different organizations, individuals have different identity numbers (IDs) and obviously do not memorize all of them. Also, from the psychological perspective, customers like to use their name as their identity, not a customer number or ID number. Unfortunately in current CRM systems, customers are not often recognized by their personal names only. The task of customer recognition is made more difficult by the common occurrence of many popular names. Also, as they have access to different communication channels, CRM systems keep in touch by various means. In the present paper, an intelligent online customer recognition (IOCR) framework is proposed which takes into consideration the presence of the more common or popular personal names of its customers. Moreover, to have a clean database, the framework includes a policy that uses the features of customers for which a standard format is available.","2156-2318;21562318","CD-ROM:978-1-4799-4316-6; Electronic:978-1-4799-4315-9; POD:978-1-4799-4314-2","10.1109/ICIEA.2014.6931303","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6931303","Common personal names;Customer recognition;Intelligent online CRM","Artificial intelligence;Customer relationship management;Educational institutions;Measurement;Organizations;Search engines;Standards","Internet;customer relationship management;database management systems;information retrieval","CRM systems;IOCR framework;clean database;common personal names;communication channels;identity numbers;information retrieval;intelligent online customer recognition framework;inverse document frequency concept;psychological perspective","","2","","12","","","9-11 June 2014","","IEEE","IEEE Conference Publications"
"Text summarization using enhanced MMR technique","R. Kurmi; P. Jain","Department of Information Technology, S.A.T.I, College, Vidisha, India","2014 International Conference on Computer Communication and Informatics","20141016","2014","","","1","5","Now a day when huge amount of documents and web contents are available, so reading of full content is somewhat difficult. Summarization is a way to give abstract form of large document so that the moral of the document can be communicated easily. Current research in automatic summarization is dominated by some effective, yet naive approaches: summarization through extraction, summarization through Abstraction and multi-document summarization. These techniques are used to building a summary of a document. Although there are a number of techniques implemented for the summarization of text for the single document or for the online web data or for any language. Here in this paper we are implemented an efficient technique for text summarization to reduce the computational cost and time and also the storage capacity.","","Electronic:978-1-4799-2352-6; POD:978-1-4799-2354-0","10.1109/ICCCI.2014.6921769","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921769","Extraction;MMR;NLP;Summarization;snippets","Accuracy;Computers;Data mining;Informatics;Semantics;Web pages","document handling;information retrieval;natural language processing","Web content;document content;document summary;enhanced MMR technique;multidocument summarization approach;summarization through abstraction approach;summarization through extraction approach;text summarization","","0","","21","","","3-5 Jan. 2014","","IEEE","IEEE Conference Publications"
"A Clique Based Web Page Classification Corrective Approach","B. Abdelbadie; B. Mohammed","Comput. Sci. Dept., Mohammed V-Agdal Univ., Rabat, Morocco","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","2","","467","473","Nowadays, the Web is the most relevant data source. Its size does not stop growing day by day. Web page classification becomes crucial due to this overwhelming amount of data. Web pages contain many noisy contents that bias textual classifiers and lead them to lose focus on their main subject. Web pages are related to each other either implicitly by users' intuitive judgments or explicitly by hyperlinks. Thus, the use of those links in order to correct a class assigned by textual classifier to a web page can be beneficial. In this paper, we propose a post classification corrective approach called Clique Based Correction (CBC) that uses the query-log to build an implicit neighborhood, and collectively corrects classes assigned by a textual classifier to web pages of that neighborhood. This correction helps improve text classifier's results by correcting wrongly assigned categories. When two web pages are linked to each other, they may share the same topic, but when more web pages (three for example) are all related to each other, the probability that those web pages share the same subject becomes stronger. The proposed method operates in four steps. In the first step, it builds a graph called implicit graph, whose vertices are web pages and edges are implicit links. In the second step, it uses a text classifier to determine classes of all web pages represented by vertices in the implicit graph. In the third step, it extracts cliques of web pages from the implicit graph. In the fourth step, it assigns a class to every clique using a voting process. Each web page will be labeled using the class of its clique. This adjustment leads to improvements of results provided by the text classifier. We conduct our experiments using three classifiers: SVM (Support Vector Machine), NB (NaiÃàve Bayes) and KNN (K Nearest Neighbors), on two subsets of ODP (Open Directory Project). Results show that: (1) when applied after SVM, NB or KNN, CBC helps bringing improvements on r- sults. (2) The number of unrelated web pages must be low in order to have significant improvement.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.135","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927662","correction;maximum clique;query-log;semantic web;web page classification","Art;Classification algorithms;Computers;Niobium;Support vector machines;Text categorization;Web pages","Bayes methods;Internet;classification;graph theory;information retrieval;support vector machines","K nearest neighbor;KNN;NaiÃàve Bayes;ODP;SVM;Web page classification corrective approach;World Wide Web;clique based correction;implicit graph;open directory project;query-log;support vector machine;textual classifier","","0","","24","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"Min(e)d your tags: Analysis of Question response time in StackOverflow","V. Bhat; A. Gokhale; R. Jadhav; J. Pudipeddi; L. Akoglu","Department of Computer Science, Stony Brook University, USA","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","328","335","Given a newly posted question on a Question and Answer (Q&A) site, how long will it take until an answer is received? Does response time relate to factors about how the question asker composes their question? If so, what are those factors? With advances in social media and the Web, Q&A sites have become a major source of information for Internet users. Response time of a question is an important aspect in these sites as it is associated with the users' satisfaction and engagement, and thus the lifespan of these online communities. In this paper we study and estimate response time for questions in StackOverflow, a popular online Q&A forum where software developers post and answer questions related to programming. We analyze a long list of factors in the data and identify those that have clear relation with response time. Our key finding is that tag-related factors, such as their ‚Äúpopularity‚Äù (how often the tag is used) and the number of their ‚Äúsubscribers‚Äù (how many users can answer questions containing the tag), provide much stronger evidence than factors not related to tags. Finally, we learn models using the identified evidential features for predicting the response time of questions, which also demonstrate the significance of tags chosen by the question asker.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921605","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921605","collective intelligence;evidential feature analysis;human behavior;online communities;question answering sites;question response time;user engagement","Conferences;Correlation;Internet;Predictive models;Programming;Social network services;Time factors","Web sites;question answering (information retrieval)","Internet users;StackOverflow;online communities;popular online Q&A forum;question and answer site;social media","","6","","18","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Increase the concurrency for multi-core systems through collision array based workload assignment","H. Zhou; L. S. Powers; J. Roveda","Department of Electrical and Computer Engineering, University of Arizona, Tucson USA","2014 International Conference on Information Science, Electronics and Electrical Engineering","20141106","2014","2","","1209","1215","Simply upgrading to multi-core systems has been proven to provide only minor speedup compared with single core systems. However, because multi-core systems use shared memory, the bottleneck for speedup lies in the memory I/O. The only way to fully utilize multiple cores is to increase parallelism and concurrency. This paper proposes a new collision array based workload assignment to increase data request cancellation. Through a task flow partitioning algorithm, we minimize sequential data access and then dynamically schedule tasks while increasing the data request cancellations. We have shown experimentally that this method can boost general system throughput by 3 times and reduce the execution time by an average of 27.6%. For a 3√ó3 multi-core system, if all nine cores are utilized, the speedup can reach 5.0 times. If only three cores are utilized to reduce communication and power, the proposed method can obtain 2.3 times speedup.","","CD-ROM:978-1-4799-3195-8; Electronic:978-1-4799-3197-2; POD:978-1-4799-3198-9","10.1109/InfoSEEE.2014.6947862","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6947862","collision array;concurrent;multi-core system;workload assignment","Arrays;Dynamic scheduling;Multicore processing;Parallel processing;Partitioning algorithms;Throughput","data structures;information retrieval;shared memory systems","collision array;data request cancellation;execution time;memory I-O;multicore systems;sequential data access;shared memory;single core systems;task flow partitioning algorithm;workload assignment","","0","","16","","","26-28 April 2014","","IEEE","IEEE Conference Publications"
"Automatic Glossing Services for E-learning Cloud Environments","R. Cortez; A. Vazhenin; J. Brine","Dept. of Comput. Sci., Univ. of Aizu, Aizu-wakamatsu, Japan","2014 IEEE 8th International Symposium on Embedded Multicore/Manycore SoCs","20141110","2014","","","128","131","In language learning scenarios, the use of glossing technique has a positive effect on incidental vocabulary acquisition as a by-product of reading. However, the preparation of materials that include glosses can be a time consuming task for the teacher. Automatic glossing tools have gained interest to help reduce such efforts, and to provide a better experience using electronic documents. Most glossing tools are still developed following a monolithic approach for a specific system or language due to its complexity. As e-Learning platforms are moving from monolithic applications to service based platforms suitable for Cloud environments, the tools as well should be designed following Service-Oriented principles. This work focuses on the design of automatic glossing services suitable for Cloud environments. The development follows an original Virtual-Model-View-Controller design pattern for the creation of loosely coupled components. The services are assembled in a web-based tool called Wiki Gloss. Wikipedia Miner is used to extract the content to feed the glosses from Wikipedia, taking advantage of the vast content and diversity of topics that are already available, as well as languages. This approach offers a potential reusability of services not only in diverse applications that can take advantage of glossing, but also provides the content in different languages.","","Electronic:978-1-4799-4305-0; POD:978-1-4799-4304-3","10.1109/MCSoC.2014.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949463","Cloud Computing;E-learning;Service Oriented Architecture (SOA);Web Services","Electronic learning;Electronic publishing;Encyclopedias;Service-oriented architecture;Vocabulary","Web services;cloud computing;computer aided instruction;document handling;information retrieval;natural language processing;service-oriented architecture;vocabulary","Web services;Web-based tool;Wiki Gloss;Wikipedia;Wikipedia Miner;automatic glossing services;content extraction;e-learning cloud environments;electronic documents;incidental vocabulary acquisition;language learning scenarios;monolithic approach;service based platforms;service oriented architecture;service-oriented principles;virtual-model-view-controller design pattern","","0","","12","","","23-25 Sept. 2014","","IEEE","IEEE Conference Publications"
"Evaluating the probabilistic side of proactive computing using cognitive modelling methodology and the statistical inference of user's cognitive states","D. Shirnin; D. Zampunieris","Computer Science and Communications Research Unit, University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, L-1359, Luxembourg","2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing","20141016","2014","","","449","454","In the following paper we present the work in progress, which aims to validate through theoretical and empirical study the probabilistic side of proactive computing. We choose the cognitive modelling methodology as an approach to implement the probabilistic inference of user's cognitive states during online activity. We create four cognitive models, which simulate various cognitive states of a user during the task of online search of medical terminology. Additionally, we elaborate several potential scenarios, which may take place during the given task, and we associate these scenarios with our models as the integral elements of the proactive system behaviour. Ultimately, by applying the rules of the Bayesian statistics, we envision to test the principles of proactive computing in the frame of probabilistic approach.","","Electronic:978-1-4799-6081-1; POD:978-1-4799-6082-8","10.1109/ICCI-CC.2014.6921497","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921497","Cognitive inference engines;Cognitive modelling;Model evaluation;Proactive Computing","Bayes methods;Computational modeling;Context;Data models;Probabilistic logic;Search problems;Terminology","cognition;inference mechanisms;information retrieval;statistical analysis","Bayesian statistics;cognitive modelling methodology;medical terminology;online search;proactive computing;probabilistic inference;probabilistic side;statistical inference;user cognitive states","","0","","20","","","18-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Using single source data to better understand User-generated Content (UGC) behavior","H. Lu; J. J. H. Zhu","Web Mining Lab, Department of Media and Communication, City University of Hong Kong, Hong Kong","2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)","20141016","2014","","","790","795","Single source refers to the unified measurement of different aspects of the same individual based on data from multiple sources. In the context of UGC, single source data can be used to study at least two important but as yet insufficiently investigated theoretical issues. First, single source data are ideal sources for studying inter-platform dynamics such as user migration across UGC platforms. Second, single source data can help to link individual self-reported cognitive factors with web crawled individual behavior logs, to achieve better understanding of individual behavior. In this paper, we select a random sample of Sina Blog users and collect their behavior information on both Sina Blog and Sina Weibo platforms; we also conduct an online survey to collect information about their cognitive factors. Merging all data together, we observe and quantify different behavior patterns of the same people across Blog and Weibo; we also identify alternative attractiveness and perceived popularity as significant drivers of one of the most important inter-platform dynamics - switching behavior.","","Electronic:978-1-4799-5877-1; POD:978-1-4799-5878-8; USB:978-1-4799-5876-4","10.1109/ASONAM.2014.6921676","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6921676","UGC behavior;inter-platform dynamics;single source data;user motivations","Blogs;Conferences;Encyclopedias;Media;Social network services;Switches","behavioural sciences computing;cognition;information retrieval;social networking (online)","Sina Blog users;Sina Weibo platforms;UGC platforms;Web crawled individual behavior logs;alternative attractiveness;cognitive factors;interplatform dynamics;interplatform dynamics-switching behavior;online survey;perceived popularity;self-reported cognitive factors;single source data;user-generated content behavior","","0","","19","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
"Extracting semantic prototypes and factual information from a large scale corpus using variable size window topic modelling","M. Korzycki; W. Korczy≈Ñski","AGH University of Science and Technology in Krak&#x00F3;w, ul. Mickiewicza 30, 30-962, Poland","2014 Federated Conference on Computer Science and Information Systems","20141023","2014","","","261","268","In this paper a model of textual events composed of a mixture of semantic stereotypes and factual information is proposed. A method is introduced that enables distinguishing automatically semantic prototypes of a general nature describing general categories of events from factual elements specific to a given event. Next, this paper presents the results of an experiment of unsupervised topic extraction performed on documents from a large-scale corpus with an additional temporal structure. This experiment was realized as a comparison of the nature of information provided by Latent Dirichlet Allocation and Vector Space modelling based on Log-Entropy weights. The impact of using different time windows of the corpus on the results of topic modelling is presented. Finally, a discussion is suggested on the issue if unsupervised topic modelling may reflect deeper semantic information, such as elements describing a given event or its causes and results, and discern it from pure factual data.","","Electronic:978-83-60810-58-3; POD:978-1-4799-2853-8; USB:978-8-3608-1057-6","10.15439/2014F253","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933023","","Accidents;Analytical models;Inductors;Prototypes;Semantics;Underwater vehicles","entropy;information retrieval;text analysis;vectors","factual elements;factual information extraction;large scale corpus;latent Dirichlet allocation;log-entropy weights;semantic prototype extraction;semantic prototypes;semantic stereotypes;temporal structure;textual events;time windows;unsupervised topic extraction;unsupervised topic modelling;variable size window topic modelling;vector space modelling","","0","","20","","","7-10 Sept. 2014","","IEEE","IEEE Conference Publications"
"A real time clustering method using document index graph","N. Akthar; M. V. Ahamad; A. U. S. Khan","Department of Computer Engineering, ZHCET, Aligarh Muslim University, 202002, India","2014 International Conference on Data Mining and Intelligent Computing (ICDMIC)","20141113","2014","","","1","7","From a previous survey, 45% of users did not get what they are actually looking for in the web using any search engine. Suppose, you have a million of text file in your server or in your computer, then there is a need to categorize them on the basis of their content in a very efficient way. As a result, IR (Information Retrieval) tool has been developed, it provides a more effective ways for users to categorize relevant data. Most of the clustering algorithm like Vector Space Model considers only single words but it is not incremental so it can't be applied on-line and another algorithm, STC, involves `trie' concept to identify shared phrases suitable to apply on-line, but the main problem is, it doesn't work for large number of data set. In this paper, we have introduced DIGE clustering algorithm which generates the clusters based on the common phrases and also on the single terms. DIGE clustering algorithm based on the DIG model for the representation of documents. The construction of DIG model is incremental, so DIGE is also capable to produce cluster using online document and also it doesn't occupy much memory, so also applicable for offline.","","Electronic:978-1-4799-4674-7; POD:978-1-4799-4673-0","10.1109/ICDMIC.2014.6954222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954222","Clustering;Document Index Graph;Incremental Algorithm;Phrase Cluster;Suffix Tree Clustering;Web-Snippets","Algorithm design and analysis;Clustering algorithms;Indexes;Merging;Rivers;Search engines;Vectors","Internet;document handling;graph theory;information retrieval;pattern clustering","DIG model;DIGE clustering algorithm;IR tool;document index graph;document representation;information retrieval tool;online document;real time clustering method;search engine","","0","","21","","","5-6 Sept. 2014","","IEEE","IEEE Conference Publications"
"The research of online shopping evaluation based on grey linguistic multiple criteria decision making system","Z. Li; L. Zhang","School of Information Management, Wuhan University, Wuhan, China","2013 IEEE International Conference on Industrial Engineering and Engineering Management","20141124","2013","","","581","585","Online shopping is making purchasing decisions based on the evaluations from previous buyers. As there are few evaluations based on the existing evaluation criterion and no requirements on the evaluators, the evaluation result may not reflect the real situation fairly. By using gray language instead of the traditional evaluation, this paper establishes a multi-criterion decision-making model, providing the online shoppers with decision-making support. Through relevant data experiment, it is confirmed that the evaluations by grey language can be much fairer. In the future, the researchers can embed this model into the online-shopping recommendation system to facilitate the shopping retrieval.","2157-3611;21573611","Electronic:978-1-4799-0986-5; POD:978-1-4799-0984-1; USB:978-1-4799-0985-8","10.1109/IEEM.2013.6962478","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6962478","evaluation;grey linguistic;multiple criteria;onlineshopping","Appraisal;Decision making;Internet;Logistics;Pragmatics;Standards","Internet;decision making;decision support systems;grey systems;information retrieval;recommender systems;retail data processing","decision-making support;grey language;grey linguistic multiple criteria decision making system;online shopping evaluation criterion;online-shopping recommendation system;purchasing decision making;shopping retrieval","","0","","12","","","10-13 Dec. 2013","","IEEE","IEEE Conference Publications"
"Music auto-tagging with variable feature sets and probabilistic annotation","J. Yin; Q. Yan; Y. Lv; Q. Tao","College of Computer and Information, Hohai University, China","2014 9th International Symposium on Communication Systems, Networks & Digital Sign (CSNDSP)","20141016","2014","","","156","160","This paper proposes a music auto-tagging system based on probabilistic annotation of semantically meaningful tags with variable feature sets. The perception-related long-term features are extracted. The original features are selected by a combination algorithm of ReliefF and principle component analysis (PCA) to form a variable unique feature subset for each tag. The Gaussian mixture models (GMMs) are then trained for each tag. The test tracks are tagged by the output probability of GMMs. To evaluate the quality of the proposed music auto-tagging system, the per-tag precision and recall rates and F-score are measured. Experiment results indicate that the performance of the models trained with the original feature sets is comparable with those trained with MFCC. The reduced variable feature sets demonstrates 2% and 5% up than the original system in precision and recall rates.","","Electronic:978-1-4799-2581-0; POD:978-1-4799-2582-7; USB:978-1-4799-7529-7","10.1109/CSNDSP.2014.6923816","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923816","GMMs;Music auto-tagging system;PCA;ReliefF;probabilistic annotation;quantization","Databases;Feature extraction;Principal component analysis;Probabilistic logic;Quantization (signal);Semantics;Vectors","Gaussian processes;information retrieval;music;principal component analysis","F-score;GMM;Gaussian mixture models;PCA;ReliefF;music auto-tagging system;output probability;perception-related long-term features;principle component analysis;probabilistic annotation;recall rates;variable feature sets","","2","","17","","","23-25 July 2014","","IEEE","IEEE Conference Publications"
"Cross-section retrieval from full-waveform LiDAR using sparse solutions","M. Azadbakht; C. S. Fraser; C. Zhang; J. Leach","Cooperative Research Centre for Spatial Information, VIC 3053, Australia","2014 IEEE Geoscience and Remote Sensing Symposium","20141106","2014","","","1959","1962","Accurate waveform restoration, from the received noisy waveform, is of great interest to the full-waveform LiDAR community. As a result of this, important attributes could be estimated precisely which are valuable in describing and differentiating LiDAR targets. Assumptions behind prominent methods like the Gaussian decomposition do not hold due to the complexity of the land surface. Deconvolution is a standard approach to retrieve the target cross-section. A regularization method is proposed based on sparsity constraints and it is compared to other well-known deconvolution methods. Numerical and visual results illustrate the robustness of the proposed method with regard to signal restoration and to suppression of noise and oscillation effects.","2153-6996;21536996","Electronic:978-1-4799-5775-0; POD:978-1-4799-5314-1; USB:978-1-4799-5774-3","10.1109/IGARSS.2014.6946844","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6946844","Deconvolution;Regularization;cross-section;full-waveform;ill-posed problem","Accuracy;Deconvolution;Indexes;Laser radar;Noise;Remote sensing;Wiener filters","deconvolution;information retrieval;numerical analysis;optical radar;remote sensing by radar;signal denoising;signal restoration","Gaussian decomposition;active remote sensing;cross-section retrieval;deconvolution;full-waveform lidar community;land surface;lidar target differentiation;noise effects;noisy waveform;numerical results;oscillation effects;signal restoration;sparse solutions;visual results;waveform restoration","","0","","27","","","13-18 July 2014","","IEEE","IEEE Conference Publications"
"Flexible end-to-end content security in CCN","C. A. Wood; E. Uzun","Palo Alto Research Center Palo Alto, CA 94304","2014 IEEE 11th Consumer Communications and Networking Conference (CCNC)","20141103","2014","","","858","865","Content-centric networking (CCN) project, a flavor of information-centric networking (ICN), decouples data from its source by shifting the emphasis from hosts and interfaces to information. As a result, content becomes directly accessible and routable within the network. In this data-centric paradigm, techniques for maintaining content confidentiality and privacy typically rely on cryptographic techniques similar to those used in modern digital rights management (DRM) applications, which often require multiple consumer-to-producer (end-to-end) messages to be transmitted to establish identities, acquire licenses, and access encrypted content. In this paper, we present a secure content distribution architecture for CCN that is based on proxy re-encryption. Our design provides strong end-to-end content security and reduces the number of protocol messages required for user authentication and key retrieval. Unlike widely-deployed solutions, our solution is also capable of utilizing the opportunistic in-network caches in CCN. We also experimentally compare two proxy re-encryption schemes that can be used to implement the architecture, and describe the proof of concept application we developed over CCNx.","2331-9852;23319852","Electronic:978-1-4799-2355-7; POD:978-1-4799-2357-1","10.1109/CCNC.2014.6940528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940528","","Cloud computing;Computer architecture;Encryption;Identity-based encryption","computer network security;cryptographic protocols;data privacy;digital rights management;information retrieval;internetworking","CCN;DRM applications;ICN;consumer-to-producer end-to-end messages;content confidentiality maintenance;content privacy maintenance;content-centric networking;cryptographic techniques;data decoupling;data source;data-centric paradigm;digital rights management applications;end-to-end content security;flexible end-to-end content security;information hosts;information interfaces;information-centric networking;key retrieval;opportunistic in-network caches;protocol messages;proxy re-encryption;secure content distribution architecture;user authentication","","2","16","21","","","10-13 Jan. 2014","","IEEE","IEEE Conference Publications"
"An ontology-based search engine for postgraduate students information at the ministry of higher education portal of Iraq","H. N. Abed; A. Y. C. Tang; Z. C. Cob","Department of Computer Science, College of Science, University of Diyala, Iraq","2013 13th International Conference on Intellient Systems Design and Applications","20141013","2013","","","69","73","In recent times, information integration has gained importance in the provision of focused and structured information for more comprehensive purposes in the educational domain. Although the Ministry of Higher Education in Iraq has a Web portal that provides information pertaining to Postgraduate studies and research, information retrieval supported by the Website still pose a major problem to students. In order to address this problem the researcher in this study propose an ontology based framework for performing the process of information integration in four well-known universities in Iraq. This paper applies Uschold and King ontology building methodology to develop the domain ontology to support information interoperability among the selected Iraqi universities. A query system is then developed for assisting postgraduate students in their search for information related to postgraduate research and development at the universities.","2164-7143;21647143","Electronic:978-1-4799-3516-1; POD:978-1-4799-3517-8","10.1109/ISDA.2013.6920710","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920710","Information integration;Ushold and King;framework;ontology","Educational institutions;Ontologies","Web sites;educational institutions;educational technology;further education;information retrieval;ontologies (artificial intelligence);portals;search engines","Iraq;Iraqi universities;Uschold;Web portal;Website;educational domain;information integration;information interoperability;information retrieval;king ontology building methodology;ministry of higher education portal;ontology based framework;ontology-based search engine;postgraduate research and development;postgraduate students information;postgraduate studies","","0","","15","","","8-10 Dec. 2013","","IEEE","IEEE Conference Publications"
"Extracting Content of Characters from Single Text","B. Xu; F. Wang; H. Zhuge","Nanjing Univ. of Posts & Telecommun., Nanjing, China","2014 10th International Conference on Semantics, Knowledge and Grids","20141124","2014","","","145","149","Reading texts through digital tools, like computer, ipad or kindle, is gradually replacing the traditional reading behaviour. When people read a long text containing many characters such as a novel, they sometimes only want the content on one or several specific characters instead of the full text. This paper proposes a method to extract the content related one or several characters based on associative memory (ECC). The degree of closeness between a paragraph and multiple characters is calculated. Experiments demonstrate the effectiveness of ECC.","","Electronic:978-1-4799-6715-5; POD:978-1-4799-6716-2","10.1109/SKG.2014.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964681","Content extraction;associative memory;characters","Associative memory;Computers;Data mining;Encyclopedias;Equations;Error correction codes;Navigation","information retrieval;learning (artificial intelligence);text analysis","ECC;associative memory;characters content extraction;closeness degree;reading behaviour;text reading","","0","","15","","","27-29 Aug. 2014","","IEEE","IEEE Conference Publications"
"Ontology Based Data Access and Integration for Improving the Effectiveness of Farming in Nepal","S. Pokharel; M. A. Sherif; J. Lehmann","","2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20141020","2014","2","","319","326","Is widely accepted that food supply and quality are major problems in the 21st century. Due to the growth of the world's population, there is a pressing need to improve the productivity of agricultural crops, which hinges on different factors such as geographical location, soil type, weather condition and particular attributes of the crops to plant. In many regions of the world, information about those factors is not readily accessible and dispersed across a multitude of different sources. One of those regions is Nepal, in which the lack of access to this knowledge poses a significant burden for agricultural planning and decision making. Making such knowledge more accessible can boot up a farmer's living standard and increase their competitiveness on national and global markets. In this article, we show how we converted several available, although not easily accessible, datasets to RDF, thereby lowering the barrier for data re-usage and integration. We describe the conversion, linking, and publication process as well as use cases, which can be implemented using the farming datasets in Nepal.","","Electronic:978-1-4799-4143-8; POD:978-1-4799-4142-1","10.1109/WI-IAT.2014.114","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927641","","Agriculture;Meteorology;Ontologies;Production;Resource description framework;Shape;Soil","data integration;farming;information retrieval;ontologies (artificial intelligence)","Nepal;RDF;data conversion;data integration;data linking;data publication process;data re-use;farming effectiveness;ontology based data access","","1","","19","","","11-14 Aug. 2014","","IEEE","IEEE Conference Publications"
"A method for the extraction of phonetically-rich triphone sentences","G. Mendon√ßa; S. Candeias; F. Perdig√£o; C. Shulby; R. Toniazzo; A. Klautau; S. Alu√≠sio","Instituto de Ci&#x00EA;ncias Matem&#x00E1;ticas e de Computa&#x00E7;&#x00E3;o, Universidade de S&#x00E3;o Paulo - S&#x00E3;o Carlos, Brazil","2014 International Telecommunications Symposium (ITS)","20141106","2014","","","1","5","A method is proposed for compiling a corpus of phonetically-rich triphone sentences; i.e., sentences with a high variety of triphones, distributed in a uniform fashion. Such a corpus is of interest for a wide range of contexts, from automatic speech recognition to speech therapy. We evaluated this method by building phonetically-rich corpora for Brazilian Portuguese. The data employed comes from Wikipedia's dumps, which were converted into plain text, segmented and phonetically transcribed. The method consists of comparing the distance between the triphone distribution of the available sentences to an ideal uniform distribution, with equiprobable triphones. A greedy algorithm was implemented to recognize and evaluate the distance among sentences. A heuristic metric is proposed for pre-selecting sentences for the algorithm, in order to quicken its execution. The results show that, by applying the proposed metric, one can build corpora with more uniform triphone distributions.","","Electronic:978-1-4799-3743-1; POD:978-1-4799-3744-8","10.1109/ITS.2014.6947957","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6947957","","Electronic publishing;Encyclopedias;Internet;Measurement;Speech;Speech recognition","greedy algorithms;information retrieval;natural language processing;speech recognition;speech synthesis;text analysis","Brazilian Portuguese;Wikipedia dumps;automatic speech recognition;corpus compiling;distance evaluate;equiprobable triphones;greedy algorithm;heuristic metric;phonetically-rich triphone sentence extraction;sentences preselection;speech technology;speech therapy;text-to-speech systems;triphone distribution","","1","","25","","","17-20 Aug. 2014","","IEEE","IEEE Conference Publications"
